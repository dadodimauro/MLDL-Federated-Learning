{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from models_CCVR import ResNet50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# parameters\n",
    "iid = 0 # if the data is i.i.d or not\n",
    "unbalanced = 1 # in non i.i.d. setting split the data between clients equally or not\n",
    "num_users = 100 # number of client\n",
    "frac = 0.1 # fraction of the clients to be used for federated updates\n",
    "n_epochs = 100\n",
    "gpu = 1\n",
    "optimizer = \"sgd\" #sgd or adam\n",
    "local_batch_size = 10 # batch size of local updates in each user\n",
    "lr = 0.00001 # learning rate\n",
    "loss_function = \"CrossEntropyLoss\"\n",
    "\n",
    "n_virtual_samples = 1000\n",
    "\n",
    "fairAVG = 1\n",
    "\n",
    "num_groups = 0  # 0 for BatchNorm, > 0 for GroupNorm\n",
    "if num_groups == 0:\n",
    "    normalization_type = \"BatchNorm\"\n",
    "else:\n",
    "    normalization_type = \"GroupNorm\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if iid:\n",
    "    from utils_v2 import get_dataset\n",
    "else:\n",
    "    from utils import get_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# for REPRODUCIBILITY https://pytorch.org/docs/stable/notes/randomness.html\n",
    "torch.manual_seed(0)\n",
    "\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(0)\n",
    "\n",
    "np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"\n",
    "    An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, idxs):\n",
    "    trainloader = DataLoader(DatasetSplit(dataset, idxs),\n",
    "                             batch_size=None, shuffle=True, generator=generator,\n",
    "                             worker_init_fn=seed_worker)\n",
    "\n",
    "    return trainloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, user_groups = get_dataset(iid=iid, unbalanced=unbalanced,\n",
    "                                                       num_users=num_users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    },
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (linear): Linear(in_features=2048, out_features=10, bias=True)\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50(n_type=normalization_type)\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "    device = torch.device(\"cpu\")\n",
    "    gpu = 0\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpu = 1\n",
    "\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if iid:\n",
    "    filename_pt = \"fedAVG_results/ResNet50_100_sgd_lr_[0.001]_C[0.1]_iid[1]_unbalanced[0]_E[1]_B[10]_BatchNorm_numGroups[0].pt\"\n",
    "else:\n",
    "    if unbalanced:\n",
    "        filename_pt = \"fedAVG_results/weighted_average/ResNet50_100_sgd_lr_[0.001]_C[0.1]_iid[0]_unbalanced[1]_E[1]_B[10]_BatchNorm_numGroups[0].pt\"\n",
    "    else:\n",
    "        filename_pt = \"fedAVG_results/ResNet50_100_sgd_lr_[0.001]_C[0.1]_iid[0]_unbalanced[0]_E[1]_B[10]_BatchNorm_numGroups[0].pt\"\n",
    "\n",
    "if fairAVG:\n",
    "    filename_pt = \"fedAVG_results/ResNet50_100_sgd_lr_[0.001]_C[0.1]_iid[0]_unbalanced[1]_E[1]_B[10]_BatchNorm_numGroups[0].pt\"\n",
    "\n",
    "# load saved model (i.e. the one with the smallest validation loss)\n",
    "model.load_state_dict(torch.load(filename_pt))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 2048, 1, 1])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_extractor(torch.rand(1,3,32,32).cuda()).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if not name.startswith('linear'):\n",
    "        param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def clientUpdate(f, dataset, idxs, device):\n",
    "\n",
    "    trainloader = get_dataloader(dataset, idxs)\n",
    "\n",
    "    d = {}\n",
    "    sum_ = 0\n",
    "\n",
    "    # extract features by category\n",
    "    for batch_idx, (image, label) in enumerate(trainloader):\n",
    "        sum_ += 1\n",
    "\n",
    "        image = image.to(device)\n",
    "        label = int(label.cpu())\n",
    "\n",
    "        # feature = (f(image.reshape(1, 3, 32, 32))[\"avg_pool2d\"].reshape(-1)).cpu().detach()\n",
    "        # feature = (f(image.reshape(1, 3, 32, 32))[\"layer4.2.relu_2\"].reshape(-1)).cpu().detach()\n",
    "        # feature = (f(image.reshape(1, 3, 32, 32)).reshape(-1)).cpu().detach()\n",
    "\n",
    "        # feature = (f(image.reshape(1, 3, 32, 32)).reshape(-1)).detach().cpu()\n",
    "        feature = (f.feature_extractor(image.reshape(1, 3, 32, 32)).reshape(-1)).detach().cpu()\n",
    "\n",
    "        if label in d.keys():\n",
    "            d[label].append(feature)\n",
    "        else:\n",
    "            d[label] = [feature]\n",
    "\n",
    "    # mu, sigma\n",
    "    upload_d = {}\n",
    "\n",
    "    # for k, v in tqdm.tqdm(d.items()):\n",
    "    #     v_item = torch.stack(v).detach().cpu()\n",
    "    #\n",
    "    #     # consider the case where the sample size is too small to upload\n",
    "    #     if len(v_item) < 10:\n",
    "    #         continue\n",
    "    #\n",
    "    #     mu, sigma = v_item.mean(dim=0), v_item.var(dim=0)\n",
    "    #     upload_d[k] = {\"mu\": mu, \"sigma\": sigma, \"N\": len(v)}\n",
    "\n",
    "    for k, v in tqdm.tqdm(d.items()):\n",
    "        v_item = torch.stack(v).detach().cpu()\n",
    "        # consider the case where the sample size is too small to upload\n",
    "        if len(v_item) < 10:\n",
    "            continue\n",
    "\n",
    "        N = len(v)\n",
    "\n",
    "        mu = v_item.mean(dim=0)\n",
    "\n",
    "        # sigma = torch.zeros((2048, 2048))\n",
    "        # sigma = v_item.reshape(50, 8192).transpose(1, 0).cov()\n",
    "        sigma = v_item.reshape(len(v), 2048).transpose(1, 0).cov()\n",
    "\n",
    "        # sigma = torch.zeros((8192, 8192))\n",
    "        # for t in v_item:\n",
    "        #     # x = (t - mu).reshape(1, 2048)\n",
    "        #     x = (t - mu).reshape(1, 8192)\n",
    "        #     sigma = torch.add(torch.mul(x, torch.transpose(x, 1, 0)), sigma)\n",
    "        # sigma *= 1/(N-1)\n",
    "        #\n",
    "        upload_d[k] = {\"mu\": mu, \"sigma\": sigma, \"N\": N}\n",
    "\n",
    "\n",
    "    return upload_d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_14552\\3803027819.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image), torch.tensor(label)\n",
      "100%|██████████| 6/6 [00:00<00:00, 77.06it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 113.25it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 118.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 100.28it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 125.34it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 130.22it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 143.24it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 150.39it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 129.98it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 137.62it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 139.26it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 129.98it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 129.38it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 116.98it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 120.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 123.83it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 128.92it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 135.05it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 127.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 129.38it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 129.37it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 125.33it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 107.43it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 115.69it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 132.43it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 111.44it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 130.78it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 115.11it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 113.94it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 136.73it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 134.69it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 122.28it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 107.98it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 117.96it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 114.59it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 125.41it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 129.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.30it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 136.02it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 115.70it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 127.32it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 134.69it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 125.34it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 123.62it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 127.61it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 121.01it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 117.96it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 143.19it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 133.77it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 115.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.80it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 125.34it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 123.41it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 136.74it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 136.73it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 128.00it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 108.44it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 101.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 133.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 143.24it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 130.78it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 154.26it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 167.10it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 154.26it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 104.76it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 104.76it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 129.98it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 143.23it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 125.33it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 130.78it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 88.47it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 118.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 93.27it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 123.13it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 79.16it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 125.48it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 86.68it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 129.98it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 113.51it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 104.17it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 89.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 137.63it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 136.73it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 69.49it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 128.54it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 47.50it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 139.90it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 130.78it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 135.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.78it/s]\n"
     ]
    }
   ],
   "source": [
    "m = max(int(frac * num_users), 1) # number of users to be used for federated updates, at least 1\n",
    "# idxs_users = np.random.choice(range(num_users), m, replace=False) # choose randomly m users\n",
    "\n",
    "idxs_users = range(num_users)\n",
    "\n",
    "upload_d_list = [ clientUpdate(model, train_dataset, user_groups[idx], device) for idx in idxs_users ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def server_aggregate_stat(upload_d_list, n_virtual_samples):\n",
    "    # statistical feature distribution for each label\n",
    "    fd_d = {}\n",
    "\n",
    "    for l in range(10):  # for each label\n",
    "\n",
    "        print(\"label : \", l)\n",
    "\n",
    "        # clients do not necessarily have all tags, heterogeneous\n",
    "        labeled_fd_lst = [ x for x in upload_d_list if l in x.keys() ]\n",
    "        sum_n = sum(x[l][\"N\"] for x in labeled_fd_lst)\n",
    "\n",
    "        mu_lst = [fd[l][\"mu\"] * fd[l][\"N\"] / sum_n for fd in labeled_fd_lst]\n",
    "        mu = torch.stack(mu_lst).sum(dim=0)\n",
    "        # print(mu.shape)\n",
    "        #\n",
    "        # sigma1 = torch.stack(\n",
    "        #     [fd[l][\"mu\"] * (fd[l][\"N\"] - 1) / (sum_n - 1) for fd in labeled_fd_lst]\n",
    "        # ).sum(dim=0)\n",
    "        #\n",
    "        # sigma2 = torch.stack(\n",
    "        #     [\n",
    "        #         fd[l][\"mu\"] * fd[l][\"mu\"].T * fd[l][\"N\"] / (sum_n - 1)\n",
    "        #         for fd in labeled_fd_lst\n",
    "        #     ]\n",
    "        # ).sum(dim=0)\n",
    "        #\n",
    "        # sigma = sigma1 + sigma2 - sum_n / (sum_n - 1) * mu * mu.T\n",
    "\n",
    "        sigma1 = torch.stack(\n",
    "            [ (fd[l][\"N\"] - 1) / (sum_n - 1) * fd[l][\"sigma\"] for fd in labeled_fd_lst ]\n",
    "        ).sum(dim=0)\n",
    "        # print(sigma1.shape)\n",
    "\n",
    "        sigma2 = torch.stack(\n",
    "            [\n",
    "                fd[l][\"mu\"].reshape(1, 2048) * torch.transpose(fd[l][\"mu\"].reshape(1, 2048), 1, 0) * fd[l][\"N\"] / (sum_n - 1)\n",
    "                for fd in labeled_fd_lst\n",
    "            ]\n",
    "        ).sum(dim=0)\n",
    "        # sigma2 = torch.stack(\n",
    "        #     [\n",
    "        #         fd[l][\"mu\"].reshape(1, 8192) * torch.transpose(fd[l][\"mu\"].reshape(1, 8192), 1, 0) * fd[l][\"N\"] / (sum_n - 1)\n",
    "        #         for fd in labeled_fd_lst\n",
    "        #     ]\n",
    "        # ).sum(dim=0)\n",
    "        # print(sigma2.shape)\n",
    "\n",
    "        sigma3 = sum_n / (sum_n - 1) * mu.reshape(1, 2048) * torch.transpose(mu.reshape(1, 2048), 1, 0)\n",
    "        # sigma3 = sum_n / (sum_n - 1) * mu.reshape(1, 8192) * torch.transpose(mu.reshape(1, 8192), 1, 0)\n",
    "        # print(sigma3.shape)\n",
    "\n",
    "        sigma = sigma1 + sigma2 - sigma3\n",
    "        # print(sigma.shape)\n",
    "\n",
    "\n",
    "        virtual_samples = np.random.default_rng().multivariate_normal(mu, sigma, check_valid='ignore', size=n_virtual_samples, tol=1e-6, method='eigh')\n",
    "        fd_d[l] = torch.tensor(virtual_samples)\n",
    "\n",
    "\n",
    "        # generate data samples with batchsize of 1k, there are 10 categories in total, so it is 10k samples\n",
    "        # dist_c = np.random.normal(mu, sigma, size=(1000, mu.size()[0]))\n",
    "        # fd_d[l] = torch.tensor(dist_c)\n",
    "\n",
    "    return fd_d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label :  0\n",
      "label :  1\n",
      "label :  2\n",
      "label :  3\n",
      "label :  4\n",
      "label :  5\n",
      "label :  6\n",
      "label :  7\n",
      "label :  8\n",
      "label :  9\n"
     ]
    }
   ],
   "source": [
    "fd_d = server_aggregate_stat(upload_d_list, n_virtual_samples=n_virtual_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "filename_pikle = \"CCVR_results/FairAVG/extracted_features_iid[{}]_unbalanced[{}]_N[{}]_new.pickle\".format(iid, unbalanced, 1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "with open(filename_pikle, 'wb') as handle:\n",
    "    pickle.dump(fd_d, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "with open(filename_pikle, 'rb') as handle:\n",
    "    fd_d = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class DictDataset(Dataset):\n",
    "    def __init__(self, label_data_d):\n",
    "        \"Initialization\"\n",
    "        self.data, self.labels = [], []\n",
    "        for label, data in label_data_d.items():\n",
    "            self.data.append(data)\n",
    "            self.labels.append(torch.tensor([label] * len(data)))\n",
    "\n",
    "        self.data = torch.cat(self.data).type(torch.float32)\n",
    "        self.labels = torch.cat(self.labels).type(torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        return self.data[index], self.labels[index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def get_dataloader(trainset, testset, batch_size, num_workers=0, pin_memory=False):\n",
    "    trainloader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    testloader = DataLoader(\n",
    "        testset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "relu = torch.nn.ReLU()\n",
    "beta = 0.5\n",
    "for k, v in fd_d.items():\n",
    "    x = relu(v)\n",
    "    fd_d[k] = torch.tensor(np.power(x.cpu().numpy(), beta)).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def selectMc(fd_d, n_virtual_samples):\n",
    "    d = {}\n",
    "    for k, v in fd_d.items():\n",
    "        d[k] = v[:n_virtual_samples]\n",
    "\n",
    "    return d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "n_virtual_samples = 100  # change here\n",
    "\n",
    "d = selectMc(fd_d, n_virtual_samples=n_virtual_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# d = fd_d\n",
    "# n_virtual_samples = 500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(filename_pt))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every step_size epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "# prepare datasets, models, optimizers, and more\n",
    "trainset = DictDataset(fd_d)\n",
    "train_loader, _ = get_dataloader(trainset, trainset, batch_size=local_batch_size)\n",
    "\n",
    "n_epochs = 1000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.229 \tTraining Accuracy: 0.015\n",
      "Epoch: 2 \tTraining Loss: 0.227 \tTraining Accuracy: 0.016\n",
      "Epoch: 3 \tTraining Loss: 0.226 \tTraining Accuracy: 0.016\n",
      "Epoch: 4 \tTraining Loss: 0.226 \tTraining Accuracy: 0.017\n",
      "Epoch: 5 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 6 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 7 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 8 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 9 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 10 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 11 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 12 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 13 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 14 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 15 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 16 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 17 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 18 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 19 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 20 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 21 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 22 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 23 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 24 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 25 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 26 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 27 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 28 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 29 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 30 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 31 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 32 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 33 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 34 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 35 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 36 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 37 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 38 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 39 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 40 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 41 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 42 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 43 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 44 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 45 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 46 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 47 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 48 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 49 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 50 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 51 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 52 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 53 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 54 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 55 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 56 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 57 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 58 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 59 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 60 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 61 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 62 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 63 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 64 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 65 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 66 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 67 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 68 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 69 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 70 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 71 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 72 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 73 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 74 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 75 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 76 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 77 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 78 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 79 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 80 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 81 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 82 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 83 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 84 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 85 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 86 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 87 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 88 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 89 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 90 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 91 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 92 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 93 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 94 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 95 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 96 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 97 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 98 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 99 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 100 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 101 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 102 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 103 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 104 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 105 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 106 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 107 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 108 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 109 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 110 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 111 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 112 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 113 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 114 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 115 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 116 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 117 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 118 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 119 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 120 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 121 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 122 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 123 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 124 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 125 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 126 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 127 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 128 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 129 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 130 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 131 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 132 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 133 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 134 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 135 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 136 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 137 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 138 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 139 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 140 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 141 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 142 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 143 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 144 \tTraining Loss: 0.226 \tTraining Accuracy: 0.018\n",
      "Epoch: 145 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 146 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 147 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 148 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 149 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 150 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 151 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 152 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 153 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 154 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 155 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 156 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 157 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 158 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 159 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 160 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 161 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 162 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 163 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 164 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 165 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 166 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 167 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 168 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 169 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 170 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 171 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 172 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 173 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 174 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 175 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 176 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 177 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 178 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 179 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 180 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 181 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 182 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 183 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 184 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 185 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 186 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 187 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 188 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 189 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 190 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 191 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 192 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 193 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 194 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 195 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 196 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 197 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 198 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 199 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 200 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 201 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 202 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 203 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 204 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 205 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 206 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 207 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 208 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 209 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 210 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 211 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 212 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 213 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 214 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 215 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 216 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 217 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 218 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 219 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 220 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 221 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 222 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 223 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 224 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 225 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 226 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 227 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 228 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 229 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 230 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 231 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 232 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 233 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 234 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 235 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 236 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 237 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 238 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 239 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 240 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 241 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 242 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 243 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 244 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 245 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 246 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 247 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 248 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 249 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 250 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 251 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 252 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 253 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 254 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 255 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 256 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 257 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 258 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 259 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 260 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 261 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 262 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 263 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 264 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 265 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 266 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 267 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 268 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 269 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 270 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 271 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 272 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 273 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 274 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 275 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 276 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 277 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 278 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 279 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 280 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 281 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 282 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 283 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 284 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 285 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 286 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 287 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 288 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 289 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 290 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 291 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 292 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 293 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 294 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 295 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 296 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 297 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 298 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 299 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 300 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 301 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 302 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 303 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 304 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 305 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 306 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 307 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 308 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 309 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 310 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 311 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 312 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 313 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 314 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 315 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 316 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 317 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 318 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 319 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 320 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 321 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 322 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 323 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 324 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 325 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 326 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 327 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 328 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 329 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 330 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 331 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 332 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 333 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 334 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 335 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 336 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 337 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 338 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 339 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 340 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 341 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 342 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 343 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 344 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 345 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 346 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 347 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 348 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 349 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 350 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 351 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 352 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 353 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 354 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 355 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 356 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 357 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 358 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 359 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 360 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 361 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 362 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 363 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 364 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 365 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 366 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 367 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 368 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 369 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 370 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 371 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 372 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 373 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 374 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 375 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 376 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 377 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 378 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 379 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 380 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 381 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 382 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 383 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 384 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 385 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 386 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 387 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 388 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 389 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 390 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 391 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 392 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 393 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 394 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 395 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 396 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 397 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 398 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 399 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 400 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 401 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 402 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 403 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 404 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 405 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 406 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 407 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 408 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 409 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 410 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 411 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 412 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 413 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 414 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 415 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 416 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 417 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 418 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 419 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 420 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 421 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 422 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 423 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 424 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 425 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 426 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 427 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 428 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 429 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 430 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 431 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 432 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 433 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 434 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 435 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 436 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 437 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 438 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 439 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 440 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 441 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 442 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 443 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 444 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 445 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 446 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 447 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 448 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 449 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 450 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 451 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 452 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 453 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 454 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 455 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 456 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 457 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 458 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 459 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 460 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 461 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 462 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 463 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 464 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 465 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 466 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 467 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 468 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 469 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 470 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 471 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 472 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 473 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 474 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 475 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 476 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 477 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 478 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 479 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 480 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 481 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 482 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 483 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 484 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 485 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 486 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 487 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 488 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 489 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 490 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 491 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 492 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 493 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 494 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 495 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 496 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 497 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 498 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 499 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 500 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 501 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 502 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 503 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 504 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 505 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 506 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 507 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 508 \tTraining Loss: 0.226 \tTraining Accuracy: 0.019\n",
      "Epoch: 509 \tTraining Loss: 0.226 \tTraining Accuracy: 0.020\n",
      "Epoch: 510 \tTraining Loss: 0.226 \tTraining Accuracy: 0.020\n",
      "Epoch: 511 \tTraining Loss: 0.226 \tTraining Accuracy: 0.020\n",
      "Epoch: 512 \tTraining Loss: 0.226 \tTraining Accuracy: 0.020\n",
      "Epoch: 513 \tTraining Loss: 0.226 \tTraining Accuracy: 0.020\n",
      "Epoch: 514 \tTraining Loss: 0.226 \tTraining Accuracy: 0.020\n",
      "Epoch: 515 \tTraining Loss: 0.226 \tTraining Accuracy: 0.020\n",
      "Epoch: 516 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 517 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 518 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 519 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 520 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 521 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 522 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 523 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 524 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 525 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 526 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 527 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 528 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 529 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 530 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 531 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 532 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 533 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 534 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 535 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 536 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 537 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 538 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 539 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 540 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 541 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 542 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 543 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 544 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 545 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 546 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 547 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 548 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 549 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 550 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 551 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 552 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 553 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 554 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 555 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 556 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 557 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 558 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 559 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 560 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 561 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 562 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 563 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 564 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 565 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 566 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 567 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 568 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 569 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 570 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 571 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 572 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 573 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 574 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 575 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 576 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 577 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 578 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 579 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 580 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 581 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 582 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 583 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 584 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 585 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 586 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 587 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 588 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 589 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 590 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 591 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 592 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 593 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 594 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 595 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 596 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 597 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 598 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 599 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 600 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 601 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 602 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 603 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 604 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 605 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 606 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 607 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 608 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 609 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 610 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 611 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 612 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 613 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 614 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 615 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 616 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 617 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 618 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 619 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 620 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 621 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 622 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 623 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 624 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 625 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 626 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 627 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 628 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 629 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 630 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 631 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 632 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 633 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 634 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 635 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 636 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 637 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 638 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 639 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 640 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 641 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 642 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 643 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 644 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 645 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 646 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 647 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 648 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 649 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 650 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 651 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 652 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 653 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 654 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 655 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 656 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 657 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 658 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 659 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 660 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 661 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 662 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 663 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 664 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 665 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 666 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 667 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 668 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 669 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 670 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 671 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 672 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 673 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 674 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 675 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 676 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 677 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 678 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 679 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 680 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 681 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 682 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 683 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 684 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 685 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 686 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 687 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 688 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 689 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 690 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 691 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 692 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 693 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 694 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 695 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 696 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 697 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 698 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 699 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 700 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 701 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 702 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 703 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 704 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 705 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 706 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 707 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 708 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 709 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 710 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 711 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 712 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 713 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 714 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 715 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 716 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 717 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 718 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 719 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 720 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 721 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 722 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 723 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 724 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 725 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 726 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 727 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 728 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 729 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 730 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 731 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 732 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 733 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 734 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 735 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 736 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 737 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 738 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 739 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 740 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 741 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 742 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 743 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 744 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 745 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 746 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 747 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 748 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 749 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 750 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 751 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 752 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 753 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 754 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 755 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 756 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 757 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 758 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 759 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 760 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 761 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 762 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 763 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 764 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 765 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 766 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 767 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 768 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 769 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 770 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 771 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 772 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 773 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 774 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 775 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 776 \tTraining Loss: 0.225 \tTraining Accuracy: 0.020\n",
      "Epoch: 777 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 778 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 779 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 780 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 781 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 782 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 783 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 784 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 785 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 786 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 787 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 788 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 789 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 790 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 791 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 792 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 793 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 794 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 795 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 796 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 797 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 798 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 799 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 800 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 801 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 802 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 803 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 804 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 805 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 806 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 807 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 808 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 809 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 810 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 811 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 812 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 813 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 814 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 815 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 816 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 817 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 818 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 819 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 820 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 821 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 822 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 823 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 824 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 825 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 826 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 827 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 828 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 829 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 830 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 831 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 832 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 833 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 834 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 835 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 836 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 837 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 838 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 839 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 840 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 841 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 842 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 843 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 844 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 845 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 846 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 847 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 848 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 849 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 850 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 851 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 852 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 853 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 854 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 855 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 856 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 857 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 858 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 859 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 860 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 861 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 862 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 863 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 864 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 865 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 866 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 867 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 868 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 869 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 870 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 871 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 872 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 873 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 874 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 875 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 876 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 877 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 878 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 879 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 880 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 881 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 882 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 883 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 884 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 885 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 886 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 887 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 888 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 889 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 890 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 891 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 892 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 893 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 894 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 895 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 896 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 897 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 898 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 899 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 900 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 901 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 902 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 903 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 904 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 905 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 906 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 907 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 908 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 909 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 910 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 911 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 912 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 913 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 914 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 915 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 916 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 917 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 918 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 919 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 920 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 921 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 922 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 923 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 924 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 925 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 926 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 927 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 928 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 929 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 930 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 931 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 932 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 933 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 934 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 935 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 936 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 937 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 938 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 939 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 940 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 941 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 942 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 943 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 944 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 945 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 946 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 947 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 948 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 949 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 950 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 951 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 952 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 953 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 954 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 955 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 956 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 957 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 958 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 959 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 960 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 961 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 962 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 963 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 964 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 965 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 966 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 967 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 968 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 969 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 970 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 971 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 972 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 973 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 974 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 975 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 976 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 977 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 978 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 979 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 980 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 981 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 982 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 983 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 984 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 985 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 986 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 987 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 988 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 989 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 990 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 991 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 992 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 993 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 994 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 995 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 996 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 997 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 998 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n",
      "Epoch: 999 \tTraining Loss: 0.225 \tTraining Accuracy: 0.021\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(1, n_epochs+1)\n",
    "for epoch in range(1, n_epochs):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    correct_train = 0.0\n",
    "    correct_valid = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    # g.train()\n",
    "    model.train()\n",
    "\n",
    "    # for k, v in fd_d.items():\n",
    "    for k, v in d.items():\n",
    "\n",
    "        for i in range(0, len(v), 50):\n",
    "            data, target = v[i:i+50].type(torch.float).cuda(), torch.full((50,), k).cuda()\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            # output = g(data.reshape((1000, 2048, 1, 1)))\n",
    "            output = model.classifier(data.reshape((50, 2048, 1, 1)))\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += (loss.data.item() * data.shape[0])\n",
    "            # print('outputs on which to apply torch.max ', prediction)\n",
    "            # find the maximum along the rows, use dim=1 to torch.max()\n",
    "            _, predicted_outputs = torch.max(output.data, 1)\n",
    "            # Update the running corrects\n",
    "            correct_train += (predicted_outputs == target).float().sum().item()\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    # calculate accuracies\n",
    "    train_acc =  correct_train / len(train_loader.sampler)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.3f} \\tTraining Accuracy: {:.3f}'.format(\n",
    "        epoch, train_loss, train_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def test_inference(model, test_dataset, gpu=1, local_batch_size=10, loss_function=\"CrossEntropyLoss\"):\n",
    "    \"\"\"\n",
    "    Returns the test accuracy and loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # f.eval()\n",
    "    # g.eval()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "\n",
    "    device = 'cuda' if gpu else 'cpu'\n",
    "    if loss_function == \"NLLLoss\":\n",
    "        criterion = nn.NLLLoss()\n",
    "    if loss_function == \"CrossEntropyLoss\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    testloader = DataLoader(test_dataset, batch_size=10,\n",
    "                            shuffle=False, generator=generator)\n",
    "\n",
    "    for images, labels in testloader:\n",
    "    # for k, v in fd_d.items():\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # images, labels = v.type(torch.float).cuda().reshape(1000, 2048, 1, 1), torch.full((1000,), k).cuda()\n",
    "\n",
    "        # Inference\n",
    "        # output_f = f(images)[\"avg_pool2d\"]\n",
    "        # output_f = f(images.cuda())\n",
    "        # output_g = g(output_f)\n",
    "        output_g = model(images.cuda())\n",
    "\n",
    "        loss = criterion(output_g, labels)\n",
    "        test_loss += (loss.data.item() * images.shape[0])\n",
    "\n",
    "        # Prediction\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output_g, 1)\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            label = labels.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    # average test loss\n",
    "    test_loss = test_loss / len(testloader.dataset)\n",
    "\n",
    "    accuracy = np.sum(class_correct) / np.sum(class_total)\n",
    "\n",
    "    return accuracy, test_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results after 1000 global rounds of training:\n",
      "|---- Test Accuracy: 61.54%\n"
     ]
    }
   ],
   "source": [
    "# test the trained model\n",
    "\n",
    "test_acc, test_loss = test_inference(model, test_dataset=test_dataset, gpu=gpu,\n",
    "                                     loss_function=loss_function)\n",
    "\n",
    "print(f'\\nResults after {n_epochs} global rounds of training:')\n",
    "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "filename_pt = \"CCVR_results/FairAVG//iid[{}]_unbalanced[{}]_Mc[{}].pt\".format(\n",
    "    iid, unbalanced, n_virtual_samples)\n",
    "torch.save(model.state_dict(), filename_pt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}