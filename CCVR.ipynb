{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from models_CCVR import ResNet50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# parameters\n",
    "iid = 1 # if the data is i.i.d or not\n",
    "unbalanced = 0 # in non i.i.d. setting split the data between clients equally or not\n",
    "num_users = 100 # number of client\n",
    "frac = 0.1 # fraction of the clients to be used for federated updates\n",
    "n_epochs = 100\n",
    "gpu = 0\n",
    "optimizer = \"sgd\" #sgd or adam\n",
    "local_batch_size = 10 # batch size of local updates in each user\n",
    "lr = 0.0001 # learning rate\n",
    "loss_function = \"CrossEntropyLoss\"\n",
    "\n",
    "n_virtual_samples = 1000\n",
    "\n",
    "num_groups = 0  # 0 for BatchNorm, > 0 for GroupNorm\n",
    "if num_groups == 0:\n",
    "    normalization_type = \"BatchNorm\"\n",
    "else:\n",
    "    normalization_type = \"GroupNorm\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "if iid:\n",
    "    from utils_v2 import get_dataset\n",
    "else:\n",
    "    from utils import get_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# for REPRODUCIBILITY https://pytorch.org/docs/stable/notes/randomness.html\n",
    "torch.manual_seed(0)\n",
    "\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(0)\n",
    "\n",
    "np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"\n",
    "    An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, idxs):\n",
    "    trainloader = DataLoader(DatasetSplit(dataset, idxs),\n",
    "                             batch_size=None, shuffle=True, generator=generator,\n",
    "                             worker_init_fn=seed_worker)\n",
    "\n",
    "    return trainloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, user_groups = get_dataset(iid=iid, unbalanced=unbalanced,\n",
    "                                                       num_users=num_users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    },
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (linear): Linear(in_features=2048, out_features=10, bias=True)\n)"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50(n_type=normalization_type)\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "    device = torch.device(\"cpu\")\n",
    "    gpu = 0\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpu = 1\n",
    "\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if iid:\n",
    "    filename_pt = \"fedAVG_results/ResNet50_100_sgd_lr_[0.001]_C[0.1]_iid[1]_unbalanced[0]_E[1]_B[10]_BatchNorm_numGroups[0].pt\"\n",
    "else:\n",
    "    if unbalanced:\n",
    "        filename_pt = \"fedAVG_results/weighted_average/ResNet50_100_sgd_lr_[0.001]_C[0.1]_iid[0]_unbalanced[1]_E[1]_B[10]_BatchNorm_numGroups[0].pt\"\n",
    "    else:\n",
    "        filename_pt = \"fedAVG_results/ResNet50_100_sgd_lr_[0.001]_C[0.1]_iid[0]_unbalanced[0]_E[1]_B[10]_BatchNorm_numGroups[0].pt\"\n",
    "\n",
    "# load saved model (i.e. the one with the smallest validation loss)\n",
    "model.load_state_dict(torch.load(filename_pt))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 2048, 1, 1])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_extractor(torch.rand(1,3,32,32).cuda()).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if not name.startswith('linear'):\n",
    "        param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def clientUpdate(f, dataset, idxs, device):\n",
    "\n",
    "    trainloader = get_dataloader(dataset, idxs)\n",
    "\n",
    "    d = {}\n",
    "    sum_ = 0\n",
    "\n",
    "    # extract features by category\n",
    "    for batch_idx, (image, label) in enumerate(trainloader):\n",
    "        sum_ += 1\n",
    "\n",
    "        image = image.to(device)\n",
    "        label = int(label.cpu())\n",
    "\n",
    "        # feature = (f(image.reshape(1, 3, 32, 32))[\"avg_pool2d\"].reshape(-1)).cpu().detach()\n",
    "        # feature = (f(image.reshape(1, 3, 32, 32))[\"layer4.2.relu_2\"].reshape(-1)).cpu().detach()\n",
    "        # feature = (f(image.reshape(1, 3, 32, 32)).reshape(-1)).cpu().detach()\n",
    "\n",
    "        # feature = (f(image.reshape(1, 3, 32, 32)).reshape(-1)).detach().cpu()\n",
    "        feature = (f.feature_extractor(image.reshape(1, 3, 32, 32)).reshape(-1)).detach().cpu()\n",
    "\n",
    "        if label in d.keys():\n",
    "            d[label].append(feature)\n",
    "        else:\n",
    "            d[label] = [feature]\n",
    "\n",
    "    # mu, sigma\n",
    "    upload_d = {}\n",
    "\n",
    "    # for k, v in tqdm.tqdm(d.items()):\n",
    "    #     v_item = torch.stack(v).detach().cpu()\n",
    "    #\n",
    "    #     # consider the case where the sample size is too small to upload\n",
    "    #     if len(v_item) < 10:\n",
    "    #         continue\n",
    "    #\n",
    "    #     mu, sigma = v_item.mean(dim=0), v_item.var(dim=0)\n",
    "    #     upload_d[k] = {\"mu\": mu, \"sigma\": sigma, \"N\": len(v)}\n",
    "\n",
    "    for k, v in tqdm.tqdm(d.items()):\n",
    "        v_item = torch.stack(v).detach().cpu()\n",
    "        # consider the case where the sample size is too small to upload\n",
    "        if len(v_item) < 10:\n",
    "            continue\n",
    "\n",
    "        N = len(v)\n",
    "\n",
    "        mu = v_item.mean(dim=0)\n",
    "\n",
    "        # sigma = torch.zeros((2048, 2048))\n",
    "        # sigma = v_item.reshape(50, 8192).transpose(1, 0).cov()\n",
    "        sigma = v_item.reshape(len(v), 2048).transpose(1, 0).cov()\n",
    "\n",
    "        # sigma = torch.zeros((8192, 8192))\n",
    "        # for t in v_item:\n",
    "        #     # x = (t - mu).reshape(1, 2048)\n",
    "        #     x = (t - mu).reshape(1, 8192)\n",
    "        #     sigma = torch.add(torch.mul(x, torch.transpose(x, 1, 0)), sigma)\n",
    "        # sigma *= 1/(N-1)\n",
    "        #\n",
    "        upload_d[k] = {\"mu\": mu, \"sigma\": sigma, \"N\": N}\n",
    "\n",
    "\n",
    "    return upload_d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_28532\\3803027819.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image), torch.tensor(label)\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 91.14it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 62.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 77.13it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 91.15it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 71.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 77.13it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 71.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.51it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 77.13it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 95.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 91.15it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 91.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 77.13it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 80.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 71.69it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 87.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 62.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 71.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 80.22it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 80.21it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.38it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 95.64it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 71.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 71.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 71.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 66.85it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.38it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 95.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 62.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 80.22it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 95.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.42it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 95.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 91.16it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 83.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 74.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.39it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 83.58it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 71.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 95.50it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 69.15it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 83.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 95.50it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 66.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 45.58it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 83.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 111.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 95.50it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 95.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 74.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 80.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 62.67it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 87.19it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 95.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 69.15it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 71.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 100.27it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 105.55it/s]\n"
     ]
    }
   ],
   "source": [
    "m = max(int(frac * num_users), 1) # number of users to be used for federated updates, at least 1\n",
    "# idxs_users = np.random.choice(range(num_users), m, replace=False) # choose randomly m users\n",
    "\n",
    "idxs_users = range(num_users)\n",
    "\n",
    "upload_d_list = [ clientUpdate(model, train_dataset, user_groups[idx], device) for idx in idxs_users ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def server_aggregate_stat(upload_d_list, n_virtual_samples):\n",
    "    # statistical feature distribution for each label\n",
    "    fd_d = {}\n",
    "\n",
    "    for l in range(10):  # for each label\n",
    "\n",
    "        print(\"label : \", l)\n",
    "\n",
    "        # clients do not necessarily have all tags, heterogeneous\n",
    "        labeled_fd_lst = [ x for x in upload_d_list if l in x.keys() ]\n",
    "        sum_n = sum(x[l][\"N\"] for x in labeled_fd_lst)\n",
    "\n",
    "        mu_lst = [fd[l][\"mu\"] * fd[l][\"N\"] / sum_n for fd in labeled_fd_lst]\n",
    "        mu = torch.stack(mu_lst).sum(dim=0)\n",
    "        # print(mu.shape)\n",
    "        #\n",
    "        # sigma1 = torch.stack(\n",
    "        #     [fd[l][\"mu\"] * (fd[l][\"N\"] - 1) / (sum_n - 1) for fd in labeled_fd_lst]\n",
    "        # ).sum(dim=0)\n",
    "        #\n",
    "        # sigma2 = torch.stack(\n",
    "        #     [\n",
    "        #         fd[l][\"mu\"] * fd[l][\"mu\"].T * fd[l][\"N\"] / (sum_n - 1)\n",
    "        #         for fd in labeled_fd_lst\n",
    "        #     ]\n",
    "        # ).sum(dim=0)\n",
    "        #\n",
    "        # sigma = sigma1 + sigma2 - sum_n / (sum_n - 1) * mu * mu.T\n",
    "\n",
    "        sigma1 = torch.stack(\n",
    "            [ (fd[l][\"N\"] - 1) / (sum_n - 1) * fd[l][\"sigma\"] for fd in labeled_fd_lst ]\n",
    "        ).sum(dim=0)\n",
    "        # print(sigma1.shape)\n",
    "\n",
    "        sigma2 = torch.stack(\n",
    "            [\n",
    "                fd[l][\"mu\"].reshape(1, 2048) * torch.transpose(fd[l][\"mu\"].reshape(1, 2048), 1, 0) * fd[l][\"N\"] / (sum_n - 1)\n",
    "                for fd in labeled_fd_lst\n",
    "            ]\n",
    "        ).sum(dim=0)\n",
    "        # sigma2 = torch.stack(\n",
    "        #     [\n",
    "        #         fd[l][\"mu\"].reshape(1, 8192) * torch.transpose(fd[l][\"mu\"].reshape(1, 8192), 1, 0) * fd[l][\"N\"] / (sum_n - 1)\n",
    "        #         for fd in labeled_fd_lst\n",
    "        #     ]\n",
    "        # ).sum(dim=0)\n",
    "        # print(sigma2.shape)\n",
    "\n",
    "        sigma3 = sum_n / (sum_n - 1) * mu.reshape(1, 2048) * torch.transpose(mu.reshape(1, 2048), 1, 0)\n",
    "        # sigma3 = sum_n / (sum_n - 1) * mu.reshape(1, 8192) * torch.transpose(mu.reshape(1, 8192), 1, 0)\n",
    "        # print(sigma3.shape)\n",
    "\n",
    "        sigma = sigma1 + sigma2 - sigma3\n",
    "        # print(sigma.shape)\n",
    "\n",
    "\n",
    "        virtual_samples = np.random.default_rng().multivariate_normal(mu, sigma, check_valid='ignore', size=n_virtual_samples, tol=1e-6, method='eigh')\n",
    "        fd_d[l] = torch.tensor(virtual_samples)\n",
    "\n",
    "\n",
    "        # generate data samples with batchsize of 1k, there are 10 categories in total, so it is 10k samples\n",
    "        # dist_c = np.random.normal(mu, sigma, size=(1000, mu.size()[0]))\n",
    "        # fd_d[l] = torch.tensor(dist_c)\n",
    "\n",
    "    return fd_d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label :  0\n",
      "label :  1\n",
      "label :  2\n",
      "label :  3\n",
      "label :  4\n",
      "label :  5\n",
      "label :  6\n",
      "label :  7\n",
      "label :  8\n",
      "label :  9\n"
     ]
    }
   ],
   "source": [
    "fd_d = server_aggregate_stat(upload_d_list, n_virtual_samples=n_virtual_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "filename_pikle = \"CCVR_results/extracted_features_iid[{}]_unbalanced[{}]_N[{}]_new.pickle\".format(iid, unbalanced, n_virtual_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "with open(filename_pikle, 'wb') as handle:\n",
    "    pickle.dump(fd_d, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "with open(filename_pikle, 'rb') as handle:\n",
    "    fd_d = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "class DictDataset(Dataset):\n",
    "    def __init__(self, label_data_d):\n",
    "        \"Initialization\"\n",
    "        self.data, self.labels = [], []\n",
    "        for label, data in label_data_d.items():\n",
    "            self.data.append(data)\n",
    "            self.labels.append(torch.tensor([label] * len(data)))\n",
    "\n",
    "        self.data = torch.cat(self.data).type(torch.float32)\n",
    "        self.labels = torch.cat(self.labels).type(torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        return self.data[index], self.labels[index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def get_dataloader(trainset, testset, batch_size, num_workers=0, pin_memory=False):\n",
    "    trainloader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    testloader = DataLoader(\n",
    "        testset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "relu = torch.nn.ReLU()\n",
    "beta = 0.5\n",
    "for k, v in fd_d.items():\n",
    "    x = relu(v)\n",
    "    fd_d[k] = torch.tensor(np.power(x.cpu().numpy(), beta)).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(filename_pt))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every step_size epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.1)\n",
    "\n",
    "# prepare datasets, models, optimizers, and more\n",
    "trainset = DictDataset(fd_d)\n",
    "train_loader, _ = get_dataloader(trainset, trainset, batch_size=local_batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.648 \tTraining Accuracy: 0.100\n",
      "Epoch: 2 \tTraining Loss: 2.362 \tTraining Accuracy: 0.031\n",
      "Epoch: 3 \tTraining Loss: 2.349 \tTraining Accuracy: 0.060\n",
      "Epoch: 4 \tTraining Loss: 2.398 \tTraining Accuracy: 0.092\n",
      "Epoch: 5 \tTraining Loss: 2.401 \tTraining Accuracy: 0.023\n",
      "Epoch: 6 \tTraining Loss: 2.385 \tTraining Accuracy: 0.009\n",
      "Epoch: 7 \tTraining Loss: 2.375 \tTraining Accuracy: 0.016\n",
      "Epoch: 8 \tTraining Loss: 2.376 \tTraining Accuracy: 0.022\n",
      "Epoch: 9 \tTraining Loss: 2.380 \tTraining Accuracy: 0.023\n",
      "Epoch: 10 \tTraining Loss: 2.381 \tTraining Accuracy: 0.019\n",
      "Epoch: 11 \tTraining Loss: 2.380 \tTraining Accuracy: 0.019\n",
      "Epoch: 12 \tTraining Loss: 2.379 \tTraining Accuracy: 0.020\n",
      "Epoch: 13 \tTraining Loss: 2.379 \tTraining Accuracy: 0.021\n",
      "Epoch: 14 \tTraining Loss: 2.379 \tTraining Accuracy: 0.021\n",
      "Epoch: 15 \tTraining Loss: 2.379 \tTraining Accuracy: 0.021\n",
      "Epoch: 16 \tTraining Loss: 2.379 \tTraining Accuracy: 0.020\n",
      "Epoch: 17 \tTraining Loss: 2.379 \tTraining Accuracy: 0.021\n",
      "Epoch: 18 \tTraining Loss: 2.379 \tTraining Accuracy: 0.021\n",
      "Epoch: 19 \tTraining Loss: 2.379 \tTraining Accuracy: 0.021\n",
      "Epoch: 20 \tTraining Loss: 2.378 \tTraining Accuracy: 0.021\n",
      "Epoch: 21 \tTraining Loss: 2.378 \tTraining Accuracy: 0.021\n",
      "Epoch: 22 \tTraining Loss: 2.378 \tTraining Accuracy: 0.021\n",
      "Epoch: 23 \tTraining Loss: 2.378 \tTraining Accuracy: 0.021\n",
      "Epoch: 24 \tTraining Loss: 2.378 \tTraining Accuracy: 0.021\n",
      "Epoch: 25 \tTraining Loss: 2.378 \tTraining Accuracy: 0.021\n",
      "Epoch: 26 \tTraining Loss: 2.378 \tTraining Accuracy: 0.021\n",
      "Epoch: 27 \tTraining Loss: 2.378 \tTraining Accuracy: 0.021\n",
      "Epoch: 28 \tTraining Loss: 2.378 \tTraining Accuracy: 0.021\n",
      "Epoch: 29 \tTraining Loss: 2.378 \tTraining Accuracy: 0.021\n",
      "Epoch: 30 \tTraining Loss: 2.378 \tTraining Accuracy: 0.021\n",
      "Epoch: 31 \tTraining Loss: 2.378 \tTraining Accuracy: 0.021\n",
      "Epoch: 32 \tTraining Loss: 2.377 \tTraining Accuracy: 0.022\n",
      "Epoch: 33 \tTraining Loss: 2.377 \tTraining Accuracy: 0.022\n",
      "Epoch: 34 \tTraining Loss: 2.377 \tTraining Accuracy: 0.022\n",
      "Epoch: 35 \tTraining Loss: 2.377 \tTraining Accuracy: 0.022\n",
      "Epoch: 36 \tTraining Loss: 2.377 \tTraining Accuracy: 0.022\n",
      "Epoch: 37 \tTraining Loss: 2.377 \tTraining Accuracy: 0.022\n",
      "Epoch: 38 \tTraining Loss: 2.377 \tTraining Accuracy: 0.022\n",
      "Epoch: 39 \tTraining Loss: 2.377 \tTraining Accuracy: 0.023\n",
      "Epoch: 40 \tTraining Loss: 2.377 \tTraining Accuracy: 0.023\n",
      "Epoch: 41 \tTraining Loss: 2.377 \tTraining Accuracy: 0.023\n",
      "Epoch: 42 \tTraining Loss: 2.377 \tTraining Accuracy: 0.023\n",
      "Epoch: 43 \tTraining Loss: 2.377 \tTraining Accuracy: 0.023\n",
      "Epoch: 44 \tTraining Loss: 2.376 \tTraining Accuracy: 0.023\n",
      "Epoch: 45 \tTraining Loss: 2.376 \tTraining Accuracy: 0.023\n",
      "Epoch: 46 \tTraining Loss: 2.376 \tTraining Accuracy: 0.023\n",
      "Epoch: 47 \tTraining Loss: 2.376 \tTraining Accuracy: 0.023\n",
      "Epoch: 48 \tTraining Loss: 2.376 \tTraining Accuracy: 0.023\n",
      "Epoch: 49 \tTraining Loss: 2.376 \tTraining Accuracy: 0.023\n",
      "Epoch: 50 \tTraining Loss: 2.376 \tTraining Accuracy: 0.024\n",
      "Epoch: 51 \tTraining Loss: 2.376 \tTraining Accuracy: 0.024\n",
      "Epoch: 52 \tTraining Loss: 2.376 \tTraining Accuracy: 0.024\n",
      "Epoch: 53 \tTraining Loss: 2.376 \tTraining Accuracy: 0.024\n",
      "Epoch: 54 \tTraining Loss: 2.376 \tTraining Accuracy: 0.024\n",
      "Epoch: 55 \tTraining Loss: 2.376 \tTraining Accuracy: 0.024\n",
      "Epoch: 56 \tTraining Loss: 2.376 \tTraining Accuracy: 0.024\n",
      "Epoch: 57 \tTraining Loss: 2.375 \tTraining Accuracy: 0.024\n",
      "Epoch: 58 \tTraining Loss: 2.375 \tTraining Accuracy: 0.024\n",
      "Epoch: 59 \tTraining Loss: 2.375 \tTraining Accuracy: 0.024\n",
      "Epoch: 60 \tTraining Loss: 2.375 \tTraining Accuracy: 0.024\n",
      "Epoch: 61 \tTraining Loss: 2.375 \tTraining Accuracy: 0.024\n",
      "Epoch: 62 \tTraining Loss: 2.375 \tTraining Accuracy: 0.024\n",
      "Epoch: 63 \tTraining Loss: 2.375 \tTraining Accuracy: 0.025\n",
      "Epoch: 64 \tTraining Loss: 2.375 \tTraining Accuracy: 0.025\n",
      "Epoch: 65 \tTraining Loss: 2.375 \tTraining Accuracy: 0.025\n",
      "Epoch: 66 \tTraining Loss: 2.375 \tTraining Accuracy: 0.025\n",
      "Epoch: 67 \tTraining Loss: 2.375 \tTraining Accuracy: 0.025\n",
      "Epoch: 68 \tTraining Loss: 2.375 \tTraining Accuracy: 0.025\n",
      "Epoch: 69 \tTraining Loss: 2.374 \tTraining Accuracy: 0.025\n",
      "Epoch: 70 \tTraining Loss: 2.374 \tTraining Accuracy: 0.025\n",
      "Epoch: 71 \tTraining Loss: 2.374 \tTraining Accuracy: 0.025\n",
      "Epoch: 72 \tTraining Loss: 2.374 \tTraining Accuracy: 0.025\n",
      "Epoch: 73 \tTraining Loss: 2.374 \tTraining Accuracy: 0.025\n",
      "Epoch: 74 \tTraining Loss: 2.374 \tTraining Accuracy: 0.025\n",
      "Epoch: 75 \tTraining Loss: 2.374 \tTraining Accuracy: 0.026\n",
      "Epoch: 76 \tTraining Loss: 2.374 \tTraining Accuracy: 0.026\n",
      "Epoch: 77 \tTraining Loss: 2.374 \tTraining Accuracy: 0.026\n",
      "Epoch: 78 \tTraining Loss: 2.374 \tTraining Accuracy: 0.026\n",
      "Epoch: 79 \tTraining Loss: 2.374 \tTraining Accuracy: 0.026\n",
      "Epoch: 80 \tTraining Loss: 2.374 \tTraining Accuracy: 0.027\n",
      "Epoch: 81 \tTraining Loss: 2.373 \tTraining Accuracy: 0.027\n",
      "Epoch: 82 \tTraining Loss: 2.373 \tTraining Accuracy: 0.027\n",
      "Epoch: 83 \tTraining Loss: 2.373 \tTraining Accuracy: 0.027\n",
      "Epoch: 84 \tTraining Loss: 2.373 \tTraining Accuracy: 0.027\n",
      "Epoch: 85 \tTraining Loss: 2.373 \tTraining Accuracy: 0.028\n",
      "Epoch: 86 \tTraining Loss: 2.373 \tTraining Accuracy: 0.028\n",
      "Epoch: 87 \tTraining Loss: 2.373 \tTraining Accuracy: 0.028\n",
      "Epoch: 88 \tTraining Loss: 2.373 \tTraining Accuracy: 0.028\n",
      "Epoch: 89 \tTraining Loss: 2.373 \tTraining Accuracy: 0.028\n",
      "Epoch: 90 \tTraining Loss: 2.373 \tTraining Accuracy: 0.028\n",
      "Epoch: 91 \tTraining Loss: 2.373 \tTraining Accuracy: 0.028\n",
      "Epoch: 92 \tTraining Loss: 2.373 \tTraining Accuracy: 0.028\n",
      "Epoch: 93 \tTraining Loss: 2.373 \tTraining Accuracy: 0.028\n",
      "Epoch: 94 \tTraining Loss: 2.372 \tTraining Accuracy: 0.029\n",
      "Epoch: 95 \tTraining Loss: 2.372 \tTraining Accuracy: 0.029\n",
      "Epoch: 96 \tTraining Loss: 2.372 \tTraining Accuracy: 0.029\n",
      "Epoch: 97 \tTraining Loss: 2.372 \tTraining Accuracy: 0.029\n",
      "Epoch: 98 \tTraining Loss: 2.372 \tTraining Accuracy: 0.029\n",
      "Epoch: 99 \tTraining Loss: 2.372 \tTraining Accuracy: 0.029\n",
      "Epoch: 100 \tTraining Loss: 2.372 \tTraining Accuracy: 0.029\n",
      "Epoch: 101 \tTraining Loss: 2.372 \tTraining Accuracy: 0.030\n",
      "Epoch: 102 \tTraining Loss: 2.372 \tTraining Accuracy: 0.030\n",
      "Epoch: 103 \tTraining Loss: 2.372 \tTraining Accuracy: 0.030\n",
      "Epoch: 104 \tTraining Loss: 2.372 \tTraining Accuracy: 0.030\n",
      "Epoch: 105 \tTraining Loss: 2.372 \tTraining Accuracy: 0.030\n",
      "Epoch: 106 \tTraining Loss: 2.371 \tTraining Accuracy: 0.030\n",
      "Epoch: 107 \tTraining Loss: 2.371 \tTraining Accuracy: 0.030\n",
      "Epoch: 108 \tTraining Loss: 2.371 \tTraining Accuracy: 0.031\n",
      "Epoch: 109 \tTraining Loss: 2.371 \tTraining Accuracy: 0.031\n",
      "Epoch: 110 \tTraining Loss: 2.371 \tTraining Accuracy: 0.031\n",
      "Epoch: 111 \tTraining Loss: 2.371 \tTraining Accuracy: 0.031\n",
      "Epoch: 112 \tTraining Loss: 2.371 \tTraining Accuracy: 0.031\n",
      "Epoch: 113 \tTraining Loss: 2.371 \tTraining Accuracy: 0.031\n",
      "Epoch: 114 \tTraining Loss: 2.371 \tTraining Accuracy: 0.031\n",
      "Epoch: 115 \tTraining Loss: 2.371 \tTraining Accuracy: 0.032\n",
      "Epoch: 116 \tTraining Loss: 2.371 \tTraining Accuracy: 0.032\n",
      "Epoch: 117 \tTraining Loss: 2.371 \tTraining Accuracy: 0.032\n",
      "Epoch: 118 \tTraining Loss: 2.371 \tTraining Accuracy: 0.032\n",
      "Epoch: 119 \tTraining Loss: 2.370 \tTraining Accuracy: 0.032\n",
      "Epoch: 120 \tTraining Loss: 2.370 \tTraining Accuracy: 0.032\n",
      "Epoch: 121 \tTraining Loss: 2.370 \tTraining Accuracy: 0.032\n",
      "Epoch: 122 \tTraining Loss: 2.370 \tTraining Accuracy: 0.033\n",
      "Epoch: 123 \tTraining Loss: 2.370 \tTraining Accuracy: 0.033\n",
      "Epoch: 124 \tTraining Loss: 2.370 \tTraining Accuracy: 0.033\n",
      "Epoch: 125 \tTraining Loss: 2.370 \tTraining Accuracy: 0.033\n",
      "Epoch: 126 \tTraining Loss: 2.370 \tTraining Accuracy: 0.033\n",
      "Epoch: 127 \tTraining Loss: 2.370 \tTraining Accuracy: 0.034\n",
      "Epoch: 128 \tTraining Loss: 2.370 \tTraining Accuracy: 0.034\n",
      "Epoch: 129 \tTraining Loss: 2.370 \tTraining Accuracy: 0.034\n",
      "Epoch: 130 \tTraining Loss: 2.370 \tTraining Accuracy: 0.034\n",
      "Epoch: 131 \tTraining Loss: 2.369 \tTraining Accuracy: 0.034\n",
      "Epoch: 132 \tTraining Loss: 2.369 \tTraining Accuracy: 0.034\n",
      "Epoch: 133 \tTraining Loss: 2.369 \tTraining Accuracy: 0.034\n",
      "Epoch: 134 \tTraining Loss: 2.369 \tTraining Accuracy: 0.035\n",
      "Epoch: 135 \tTraining Loss: 2.369 \tTraining Accuracy: 0.035\n",
      "Epoch: 136 \tTraining Loss: 2.369 \tTraining Accuracy: 0.035\n",
      "Epoch: 137 \tTraining Loss: 2.369 \tTraining Accuracy: 0.035\n",
      "Epoch: 138 \tTraining Loss: 2.369 \tTraining Accuracy: 0.035\n",
      "Epoch: 139 \tTraining Loss: 2.369 \tTraining Accuracy: 0.035\n",
      "Epoch: 140 \tTraining Loss: 2.369 \tTraining Accuracy: 0.035\n",
      "Epoch: 141 \tTraining Loss: 2.369 \tTraining Accuracy: 0.035\n",
      "Epoch: 142 \tTraining Loss: 2.369 \tTraining Accuracy: 0.035\n",
      "Epoch: 143 \tTraining Loss: 2.369 \tTraining Accuracy: 0.035\n",
      "Epoch: 144 \tTraining Loss: 2.368 \tTraining Accuracy: 0.036\n",
      "Epoch: 145 \tTraining Loss: 2.368 \tTraining Accuracy: 0.036\n",
      "Epoch: 146 \tTraining Loss: 2.368 \tTraining Accuracy: 0.036\n",
      "Epoch: 147 \tTraining Loss: 2.368 \tTraining Accuracy: 0.036\n",
      "Epoch: 148 \tTraining Loss: 2.368 \tTraining Accuracy: 0.036\n",
      "Epoch: 149 \tTraining Loss: 2.368 \tTraining Accuracy: 0.036\n",
      "Epoch: 150 \tTraining Loss: 2.368 \tTraining Accuracy: 0.036\n",
      "Epoch: 151 \tTraining Loss: 2.368 \tTraining Accuracy: 0.036\n",
      "Epoch: 152 \tTraining Loss: 2.368 \tTraining Accuracy: 0.036\n",
      "Epoch: 153 \tTraining Loss: 2.368 \tTraining Accuracy: 0.036\n",
      "Epoch: 154 \tTraining Loss: 2.368 \tTraining Accuracy: 0.036\n",
      "Epoch: 155 \tTraining Loss: 2.368 \tTraining Accuracy: 0.036\n",
      "Epoch: 156 \tTraining Loss: 2.368 \tTraining Accuracy: 0.037\n",
      "Epoch: 157 \tTraining Loss: 2.367 \tTraining Accuracy: 0.037\n",
      "Epoch: 158 \tTraining Loss: 2.367 \tTraining Accuracy: 0.037\n",
      "Epoch: 159 \tTraining Loss: 2.367 \tTraining Accuracy: 0.037\n",
      "Epoch: 160 \tTraining Loss: 2.367 \tTraining Accuracy: 0.037\n",
      "Epoch: 161 \tTraining Loss: 2.367 \tTraining Accuracy: 0.037\n",
      "Epoch: 162 \tTraining Loss: 2.367 \tTraining Accuracy: 0.037\n",
      "Epoch: 163 \tTraining Loss: 2.367 \tTraining Accuracy: 0.037\n",
      "Epoch: 164 \tTraining Loss: 2.367 \tTraining Accuracy: 0.037\n",
      "Epoch: 165 \tTraining Loss: 2.367 \tTraining Accuracy: 0.037\n",
      "Epoch: 166 \tTraining Loss: 2.367 \tTraining Accuracy: 0.038\n",
      "Epoch: 167 \tTraining Loss: 2.367 \tTraining Accuracy: 0.038\n",
      "Epoch: 168 \tTraining Loss: 2.367 \tTraining Accuracy: 0.038\n",
      "Epoch: 169 \tTraining Loss: 2.367 \tTraining Accuracy: 0.038\n",
      "Epoch: 170 \tTraining Loss: 2.366 \tTraining Accuracy: 0.038\n",
      "Epoch: 171 \tTraining Loss: 2.366 \tTraining Accuracy: 0.038\n",
      "Epoch: 172 \tTraining Loss: 2.366 \tTraining Accuracy: 0.038\n",
      "Epoch: 173 \tTraining Loss: 2.366 \tTraining Accuracy: 0.038\n",
      "Epoch: 174 \tTraining Loss: 2.366 \tTraining Accuracy: 0.038\n",
      "Epoch: 175 \tTraining Loss: 2.366 \tTraining Accuracy: 0.039\n",
      "Epoch: 176 \tTraining Loss: 2.366 \tTraining Accuracy: 0.039\n",
      "Epoch: 177 \tTraining Loss: 2.366 \tTraining Accuracy: 0.039\n",
      "Epoch: 178 \tTraining Loss: 2.366 \tTraining Accuracy: 0.039\n",
      "Epoch: 179 \tTraining Loss: 2.366 \tTraining Accuracy: 0.039\n",
      "Epoch: 180 \tTraining Loss: 2.366 \tTraining Accuracy: 0.039\n",
      "Epoch: 181 \tTraining Loss: 2.366 \tTraining Accuracy: 0.040\n",
      "Epoch: 182 \tTraining Loss: 2.365 \tTraining Accuracy: 0.040\n",
      "Epoch: 183 \tTraining Loss: 2.365 \tTraining Accuracy: 0.040\n",
      "Epoch: 184 \tTraining Loss: 2.365 \tTraining Accuracy: 0.040\n",
      "Epoch: 185 \tTraining Loss: 2.365 \tTraining Accuracy: 0.040\n",
      "Epoch: 186 \tTraining Loss: 2.365 \tTraining Accuracy: 0.040\n",
      "Epoch: 187 \tTraining Loss: 2.365 \tTraining Accuracy: 0.040\n",
      "Epoch: 188 \tTraining Loss: 2.365 \tTraining Accuracy: 0.040\n",
      "Epoch: 189 \tTraining Loss: 2.365 \tTraining Accuracy: 0.041\n",
      "Epoch: 190 \tTraining Loss: 2.365 \tTraining Accuracy: 0.041\n",
      "Epoch: 191 \tTraining Loss: 2.365 \tTraining Accuracy: 0.041\n",
      "Epoch: 192 \tTraining Loss: 2.365 \tTraining Accuracy: 0.041\n",
      "Epoch: 193 \tTraining Loss: 2.365 \tTraining Accuracy: 0.041\n",
      "Epoch: 194 \tTraining Loss: 2.365 \tTraining Accuracy: 0.041\n",
      "Epoch: 195 \tTraining Loss: 2.364 \tTraining Accuracy: 0.041\n",
      "Epoch: 196 \tTraining Loss: 2.364 \tTraining Accuracy: 0.041\n",
      "Epoch: 197 \tTraining Loss: 2.364 \tTraining Accuracy: 0.041\n",
      "Epoch: 198 \tTraining Loss: 2.364 \tTraining Accuracy: 0.041\n",
      "Epoch: 199 \tTraining Loss: 2.364 \tTraining Accuracy: 0.042\n",
      "Epoch: 200 \tTraining Loss: 2.364 \tTraining Accuracy: 0.042\n",
      "Epoch: 201 \tTraining Loss: 2.364 \tTraining Accuracy: 0.042\n",
      "Epoch: 202 \tTraining Loss: 2.364 \tTraining Accuracy: 0.042\n",
      "Epoch: 203 \tTraining Loss: 2.364 \tTraining Accuracy: 0.042\n",
      "Epoch: 204 \tTraining Loss: 2.364 \tTraining Accuracy: 0.043\n",
      "Epoch: 205 \tTraining Loss: 2.364 \tTraining Accuracy: 0.043\n",
      "Epoch: 206 \tTraining Loss: 2.364 \tTraining Accuracy: 0.043\n",
      "Epoch: 207 \tTraining Loss: 2.364 \tTraining Accuracy: 0.043\n",
      "Epoch: 208 \tTraining Loss: 2.363 \tTraining Accuracy: 0.043\n",
      "Epoch: 209 \tTraining Loss: 2.363 \tTraining Accuracy: 0.043\n",
      "Epoch: 210 \tTraining Loss: 2.363 \tTraining Accuracy: 0.043\n",
      "Epoch: 211 \tTraining Loss: 2.363 \tTraining Accuracy: 0.043\n",
      "Epoch: 212 \tTraining Loss: 2.363 \tTraining Accuracy: 0.044\n",
      "Epoch: 213 \tTraining Loss: 2.363 \tTraining Accuracy: 0.044\n",
      "Epoch: 214 \tTraining Loss: 2.363 \tTraining Accuracy: 0.044\n",
      "Epoch: 215 \tTraining Loss: 2.363 \tTraining Accuracy: 0.044\n",
      "Epoch: 216 \tTraining Loss: 2.363 \tTraining Accuracy: 0.044\n",
      "Epoch: 217 \tTraining Loss: 2.363 \tTraining Accuracy: 0.045\n",
      "Epoch: 218 \tTraining Loss: 2.363 \tTraining Accuracy: 0.045\n",
      "Epoch: 219 \tTraining Loss: 2.363 \tTraining Accuracy: 0.045\n",
      "Epoch: 220 \tTraining Loss: 2.363 \tTraining Accuracy: 0.045\n",
      "Epoch: 221 \tTraining Loss: 2.362 \tTraining Accuracy: 0.046\n",
      "Epoch: 222 \tTraining Loss: 2.362 \tTraining Accuracy: 0.046\n",
      "Epoch: 223 \tTraining Loss: 2.362 \tTraining Accuracy: 0.046\n",
      "Epoch: 224 \tTraining Loss: 2.362 \tTraining Accuracy: 0.046\n",
      "Epoch: 225 \tTraining Loss: 2.362 \tTraining Accuracy: 0.046\n",
      "Epoch: 226 \tTraining Loss: 2.362 \tTraining Accuracy: 0.046\n",
      "Epoch: 227 \tTraining Loss: 2.362 \tTraining Accuracy: 0.046\n",
      "Epoch: 228 \tTraining Loss: 2.362 \tTraining Accuracy: 0.047\n",
      "Epoch: 229 \tTraining Loss: 2.362 \tTraining Accuracy: 0.047\n",
      "Epoch: 230 \tTraining Loss: 2.362 \tTraining Accuracy: 0.047\n",
      "Epoch: 231 \tTraining Loss: 2.362 \tTraining Accuracy: 0.047\n",
      "Epoch: 232 \tTraining Loss: 2.362 \tTraining Accuracy: 0.047\n",
      "Epoch: 233 \tTraining Loss: 2.362 \tTraining Accuracy: 0.048\n",
      "Epoch: 234 \tTraining Loss: 2.362 \tTraining Accuracy: 0.048\n",
      "Epoch: 235 \tTraining Loss: 2.361 \tTraining Accuracy: 0.048\n",
      "Epoch: 236 \tTraining Loss: 2.361 \tTraining Accuracy: 0.048\n",
      "Epoch: 237 \tTraining Loss: 2.361 \tTraining Accuracy: 0.049\n",
      "Epoch: 238 \tTraining Loss: 2.361 \tTraining Accuracy: 0.049\n",
      "Epoch: 239 \tTraining Loss: 2.361 \tTraining Accuracy: 0.049\n",
      "Epoch: 240 \tTraining Loss: 2.361 \tTraining Accuracy: 0.049\n",
      "Epoch: 241 \tTraining Loss: 2.361 \tTraining Accuracy: 0.049\n",
      "Epoch: 242 \tTraining Loss: 2.361 \tTraining Accuracy: 0.049\n",
      "Epoch: 243 \tTraining Loss: 2.361 \tTraining Accuracy: 0.049\n",
      "Epoch: 244 \tTraining Loss: 2.361 \tTraining Accuracy: 0.050\n",
      "Epoch: 245 \tTraining Loss: 2.361 \tTraining Accuracy: 0.050\n",
      "Epoch: 246 \tTraining Loss: 2.361 \tTraining Accuracy: 0.050\n",
      "Epoch: 247 \tTraining Loss: 2.361 \tTraining Accuracy: 0.050\n",
      "Epoch: 248 \tTraining Loss: 2.360 \tTraining Accuracy: 0.050\n",
      "Epoch: 249 \tTraining Loss: 2.360 \tTraining Accuracy: 0.050\n",
      "Epoch: 250 \tTraining Loss: 2.360 \tTraining Accuracy: 0.050\n",
      "Epoch: 251 \tTraining Loss: 2.360 \tTraining Accuracy: 0.050\n",
      "Epoch: 252 \tTraining Loss: 2.360 \tTraining Accuracy: 0.050\n",
      "Epoch: 253 \tTraining Loss: 2.360 \tTraining Accuracy: 0.051\n",
      "Epoch: 254 \tTraining Loss: 2.360 \tTraining Accuracy: 0.051\n",
      "Epoch: 255 \tTraining Loss: 2.360 \tTraining Accuracy: 0.051\n",
      "Epoch: 256 \tTraining Loss: 2.360 \tTraining Accuracy: 0.051\n",
      "Epoch: 257 \tTraining Loss: 2.360 \tTraining Accuracy: 0.051\n",
      "Epoch: 258 \tTraining Loss: 2.360 \tTraining Accuracy: 0.051\n",
      "Epoch: 259 \tTraining Loss: 2.360 \tTraining Accuracy: 0.051\n",
      "Epoch: 260 \tTraining Loss: 2.360 \tTraining Accuracy: 0.051\n",
      "Epoch: 261 \tTraining Loss: 2.359 \tTraining Accuracy: 0.051\n",
      "Epoch: 262 \tTraining Loss: 2.359 \tTraining Accuracy: 0.051\n",
      "Epoch: 263 \tTraining Loss: 2.359 \tTraining Accuracy: 0.051\n",
      "Epoch: 264 \tTraining Loss: 2.359 \tTraining Accuracy: 0.052\n",
      "Epoch: 265 \tTraining Loss: 2.359 \tTraining Accuracy: 0.052\n",
      "Epoch: 266 \tTraining Loss: 2.359 \tTraining Accuracy: 0.052\n",
      "Epoch: 267 \tTraining Loss: 2.359 \tTraining Accuracy: 0.052\n",
      "Epoch: 268 \tTraining Loss: 2.359 \tTraining Accuracy: 0.052\n",
      "Epoch: 269 \tTraining Loss: 2.359 \tTraining Accuracy: 0.052\n",
      "Epoch: 270 \tTraining Loss: 2.359 \tTraining Accuracy: 0.053\n",
      "Epoch: 271 \tTraining Loss: 2.359 \tTraining Accuracy: 0.053\n",
      "Epoch: 272 \tTraining Loss: 2.359 \tTraining Accuracy: 0.053\n",
      "Epoch: 273 \tTraining Loss: 2.359 \tTraining Accuracy: 0.053\n",
      "Epoch: 274 \tTraining Loss: 2.358 \tTraining Accuracy: 0.053\n",
      "Epoch: 275 \tTraining Loss: 2.358 \tTraining Accuracy: 0.053\n",
      "Epoch: 276 \tTraining Loss: 2.358 \tTraining Accuracy: 0.053\n",
      "Epoch: 277 \tTraining Loss: 2.358 \tTraining Accuracy: 0.053\n",
      "Epoch: 278 \tTraining Loss: 2.358 \tTraining Accuracy: 0.053\n",
      "Epoch: 279 \tTraining Loss: 2.358 \tTraining Accuracy: 0.053\n",
      "Epoch: 280 \tTraining Loss: 2.358 \tTraining Accuracy: 0.053\n",
      "Epoch: 281 \tTraining Loss: 2.358 \tTraining Accuracy: 0.053\n",
      "Epoch: 282 \tTraining Loss: 2.358 \tTraining Accuracy: 0.053\n",
      "Epoch: 283 \tTraining Loss: 2.358 \tTraining Accuracy: 0.054\n",
      "Epoch: 284 \tTraining Loss: 2.358 \tTraining Accuracy: 0.054\n",
      "Epoch: 285 \tTraining Loss: 2.358 \tTraining Accuracy: 0.054\n",
      "Epoch: 286 \tTraining Loss: 2.358 \tTraining Accuracy: 0.054\n",
      "Epoch: 287 \tTraining Loss: 2.358 \tTraining Accuracy: 0.054\n",
      "Epoch: 288 \tTraining Loss: 2.357 \tTraining Accuracy: 0.054\n",
      "Epoch: 289 \tTraining Loss: 2.357 \tTraining Accuracy: 0.054\n",
      "Epoch: 290 \tTraining Loss: 2.357 \tTraining Accuracy: 0.054\n",
      "Epoch: 291 \tTraining Loss: 2.357 \tTraining Accuracy: 0.054\n",
      "Epoch: 292 \tTraining Loss: 2.357 \tTraining Accuracy: 0.054\n",
      "Epoch: 293 \tTraining Loss: 2.357 \tTraining Accuracy: 0.054\n",
      "Epoch: 294 \tTraining Loss: 2.357 \tTraining Accuracy: 0.054\n",
      "Epoch: 295 \tTraining Loss: 2.357 \tTraining Accuracy: 0.054\n",
      "Epoch: 296 \tTraining Loss: 2.357 \tTraining Accuracy: 0.054\n",
      "Epoch: 297 \tTraining Loss: 2.357 \tTraining Accuracy: 0.055\n",
      "Epoch: 298 \tTraining Loss: 2.357 \tTraining Accuracy: 0.055\n",
      "Epoch: 299 \tTraining Loss: 2.357 \tTraining Accuracy: 0.055\n",
      "Epoch: 300 \tTraining Loss: 2.357 \tTraining Accuracy: 0.055\n",
      "Epoch: 301 \tTraining Loss: 2.356 \tTraining Accuracy: 0.055\n",
      "Epoch: 302 \tTraining Loss: 2.356 \tTraining Accuracy: 0.055\n",
      "Epoch: 303 \tTraining Loss: 2.356 \tTraining Accuracy: 0.055\n",
      "Epoch: 304 \tTraining Loss: 2.356 \tTraining Accuracy: 0.055\n",
      "Epoch: 305 \tTraining Loss: 2.356 \tTraining Accuracy: 0.056\n",
      "Epoch: 306 \tTraining Loss: 2.356 \tTraining Accuracy: 0.056\n",
      "Epoch: 307 \tTraining Loss: 2.356 \tTraining Accuracy: 0.056\n",
      "Epoch: 308 \tTraining Loss: 2.356 \tTraining Accuracy: 0.056\n",
      "Epoch: 309 \tTraining Loss: 2.356 \tTraining Accuracy: 0.056\n",
      "Epoch: 310 \tTraining Loss: 2.356 \tTraining Accuracy: 0.056\n",
      "Epoch: 311 \tTraining Loss: 2.356 \tTraining Accuracy: 0.057\n",
      "Epoch: 312 \tTraining Loss: 2.356 \tTraining Accuracy: 0.057\n",
      "Epoch: 313 \tTraining Loss: 2.356 \tTraining Accuracy: 0.057\n",
      "Epoch: 314 \tTraining Loss: 2.356 \tTraining Accuracy: 0.057\n",
      "Epoch: 315 \tTraining Loss: 2.355 \tTraining Accuracy: 0.057\n",
      "Epoch: 316 \tTraining Loss: 2.355 \tTraining Accuracy: 0.058\n",
      "Epoch: 317 \tTraining Loss: 2.355 \tTraining Accuracy: 0.058\n",
      "Epoch: 318 \tTraining Loss: 2.355 \tTraining Accuracy: 0.058\n",
      "Epoch: 319 \tTraining Loss: 2.355 \tTraining Accuracy: 0.058\n",
      "Epoch: 320 \tTraining Loss: 2.355 \tTraining Accuracy: 0.058\n",
      "Epoch: 321 \tTraining Loss: 2.355 \tTraining Accuracy: 0.058\n",
      "Epoch: 322 \tTraining Loss: 2.355 \tTraining Accuracy: 0.059\n",
      "Epoch: 323 \tTraining Loss: 2.355 \tTraining Accuracy: 0.059\n",
      "Epoch: 324 \tTraining Loss: 2.355 \tTraining Accuracy: 0.059\n",
      "Epoch: 325 \tTraining Loss: 2.355 \tTraining Accuracy: 0.059\n",
      "Epoch: 326 \tTraining Loss: 2.355 \tTraining Accuracy: 0.059\n",
      "Epoch: 327 \tTraining Loss: 2.355 \tTraining Accuracy: 0.059\n",
      "Epoch: 328 \tTraining Loss: 2.354 \tTraining Accuracy: 0.059\n",
      "Epoch: 329 \tTraining Loss: 2.354 \tTraining Accuracy: 0.060\n",
      "Epoch: 330 \tTraining Loss: 2.354 \tTraining Accuracy: 0.060\n",
      "Epoch: 331 \tTraining Loss: 2.354 \tTraining Accuracy: 0.060\n",
      "Epoch: 332 \tTraining Loss: 2.354 \tTraining Accuracy: 0.060\n",
      "Epoch: 333 \tTraining Loss: 2.354 \tTraining Accuracy: 0.060\n",
      "Epoch: 334 \tTraining Loss: 2.354 \tTraining Accuracy: 0.060\n",
      "Epoch: 335 \tTraining Loss: 2.354 \tTraining Accuracy: 0.060\n",
      "Epoch: 336 \tTraining Loss: 2.354 \tTraining Accuracy: 0.060\n",
      "Epoch: 337 \tTraining Loss: 2.354 \tTraining Accuracy: 0.061\n",
      "Epoch: 338 \tTraining Loss: 2.354 \tTraining Accuracy: 0.061\n",
      "Epoch: 339 \tTraining Loss: 2.354 \tTraining Accuracy: 0.061\n",
      "Epoch: 340 \tTraining Loss: 2.354 \tTraining Accuracy: 0.061\n",
      "Epoch: 341 \tTraining Loss: 2.354 \tTraining Accuracy: 0.061\n",
      "Epoch: 342 \tTraining Loss: 2.353 \tTraining Accuracy: 0.061\n",
      "Epoch: 343 \tTraining Loss: 2.353 \tTraining Accuracy: 0.061\n",
      "Epoch: 344 \tTraining Loss: 2.353 \tTraining Accuracy: 0.061\n",
      "Epoch: 345 \tTraining Loss: 2.353 \tTraining Accuracy: 0.061\n",
      "Epoch: 346 \tTraining Loss: 2.353 \tTraining Accuracy: 0.061\n",
      "Epoch: 347 \tTraining Loss: 2.353 \tTraining Accuracy: 0.061\n",
      "Epoch: 348 \tTraining Loss: 2.353 \tTraining Accuracy: 0.061\n",
      "Epoch: 349 \tTraining Loss: 2.353 \tTraining Accuracy: 0.061\n",
      "Epoch: 350 \tTraining Loss: 2.353 \tTraining Accuracy: 0.061\n",
      "Epoch: 351 \tTraining Loss: 2.353 \tTraining Accuracy: 0.061\n",
      "Epoch: 352 \tTraining Loss: 2.353 \tTraining Accuracy: 0.061\n",
      "Epoch: 353 \tTraining Loss: 2.353 \tTraining Accuracy: 0.062\n",
      "Epoch: 354 \tTraining Loss: 2.353 \tTraining Accuracy: 0.062\n",
      "Epoch: 355 \tTraining Loss: 2.353 \tTraining Accuracy: 0.062\n",
      "Epoch: 356 \tTraining Loss: 2.352 \tTraining Accuracy: 0.062\n",
      "Epoch: 357 \tTraining Loss: 2.352 \tTraining Accuracy: 0.062\n",
      "Epoch: 358 \tTraining Loss: 2.352 \tTraining Accuracy: 0.062\n",
      "Epoch: 359 \tTraining Loss: 2.352 \tTraining Accuracy: 0.062\n",
      "Epoch: 360 \tTraining Loss: 2.352 \tTraining Accuracy: 0.063\n",
      "Epoch: 361 \tTraining Loss: 2.352 \tTraining Accuracy: 0.063\n",
      "Epoch: 362 \tTraining Loss: 2.352 \tTraining Accuracy: 0.063\n",
      "Epoch: 363 \tTraining Loss: 2.352 \tTraining Accuracy: 0.063\n",
      "Epoch: 364 \tTraining Loss: 2.352 \tTraining Accuracy: 0.063\n",
      "Epoch: 365 \tTraining Loss: 2.352 \tTraining Accuracy: 0.063\n",
      "Epoch: 366 \tTraining Loss: 2.352 \tTraining Accuracy: 0.063\n",
      "Epoch: 367 \tTraining Loss: 2.352 \tTraining Accuracy: 0.064\n",
      "Epoch: 368 \tTraining Loss: 2.352 \tTraining Accuracy: 0.064\n",
      "Epoch: 369 \tTraining Loss: 2.352 \tTraining Accuracy: 0.064\n",
      "Epoch: 370 \tTraining Loss: 2.351 \tTraining Accuracy: 0.064\n",
      "Epoch: 371 \tTraining Loss: 2.351 \tTraining Accuracy: 0.064\n",
      "Epoch: 372 \tTraining Loss: 2.351 \tTraining Accuracy: 0.064\n",
      "Epoch: 373 \tTraining Loss: 2.351 \tTraining Accuracy: 0.065\n",
      "Epoch: 374 \tTraining Loss: 2.351 \tTraining Accuracy: 0.065\n",
      "Epoch: 375 \tTraining Loss: 2.351 \tTraining Accuracy: 0.065\n",
      "Epoch: 376 \tTraining Loss: 2.351 \tTraining Accuracy: 0.065\n",
      "Epoch: 377 \tTraining Loss: 2.351 \tTraining Accuracy: 0.065\n",
      "Epoch: 378 \tTraining Loss: 2.351 \tTraining Accuracy: 0.065\n",
      "Epoch: 379 \tTraining Loss: 2.351 \tTraining Accuracy: 0.065\n",
      "Epoch: 380 \tTraining Loss: 2.351 \tTraining Accuracy: 0.066\n",
      "Epoch: 381 \tTraining Loss: 2.351 \tTraining Accuracy: 0.066\n",
      "Epoch: 382 \tTraining Loss: 2.351 \tTraining Accuracy: 0.066\n",
      "Epoch: 383 \tTraining Loss: 2.351 \tTraining Accuracy: 0.066\n",
      "Epoch: 384 \tTraining Loss: 2.350 \tTraining Accuracy: 0.066\n",
      "Epoch: 385 \tTraining Loss: 2.350 \tTraining Accuracy: 0.066\n",
      "Epoch: 386 \tTraining Loss: 2.350 \tTraining Accuracy: 0.067\n",
      "Epoch: 387 \tTraining Loss: 2.350 \tTraining Accuracy: 0.067\n",
      "Epoch: 388 \tTraining Loss: 2.350 \tTraining Accuracy: 0.067\n",
      "Epoch: 389 \tTraining Loss: 2.350 \tTraining Accuracy: 0.067\n",
      "Epoch: 390 \tTraining Loss: 2.350 \tTraining Accuracy: 0.067\n",
      "Epoch: 391 \tTraining Loss: 2.350 \tTraining Accuracy: 0.067\n",
      "Epoch: 392 \tTraining Loss: 2.350 \tTraining Accuracy: 0.068\n",
      "Epoch: 393 \tTraining Loss: 2.350 \tTraining Accuracy: 0.068\n",
      "Epoch: 394 \tTraining Loss: 2.350 \tTraining Accuracy: 0.068\n",
      "Epoch: 395 \tTraining Loss: 2.350 \tTraining Accuracy: 0.068\n",
      "Epoch: 396 \tTraining Loss: 2.350 \tTraining Accuracy: 0.068\n",
      "Epoch: 397 \tTraining Loss: 2.349 \tTraining Accuracy: 0.068\n",
      "Epoch: 398 \tTraining Loss: 2.349 \tTraining Accuracy: 0.068\n",
      "Epoch: 399 \tTraining Loss: 2.349 \tTraining Accuracy: 0.068\n",
      "Epoch: 400 \tTraining Loss: 2.349 \tTraining Accuracy: 0.068\n",
      "Epoch: 401 \tTraining Loss: 2.349 \tTraining Accuracy: 0.069\n",
      "Epoch: 402 \tTraining Loss: 2.349 \tTraining Accuracy: 0.069\n",
      "Epoch: 403 \tTraining Loss: 2.349 \tTraining Accuracy: 0.069\n",
      "Epoch: 404 \tTraining Loss: 2.349 \tTraining Accuracy: 0.069\n",
      "Epoch: 405 \tTraining Loss: 2.349 \tTraining Accuracy: 0.069\n",
      "Epoch: 406 \tTraining Loss: 2.349 \tTraining Accuracy: 0.069\n",
      "Epoch: 407 \tTraining Loss: 2.349 \tTraining Accuracy: 0.069\n",
      "Epoch: 408 \tTraining Loss: 2.349 \tTraining Accuracy: 0.070\n",
      "Epoch: 409 \tTraining Loss: 2.349 \tTraining Accuracy: 0.070\n",
      "Epoch: 410 \tTraining Loss: 2.349 \tTraining Accuracy: 0.070\n",
      "Epoch: 411 \tTraining Loss: 2.349 \tTraining Accuracy: 0.070\n",
      "Epoch: 412 \tTraining Loss: 2.348 \tTraining Accuracy: 0.070\n",
      "Epoch: 413 \tTraining Loss: 2.348 \tTraining Accuracy: 0.070\n",
      "Epoch: 414 \tTraining Loss: 2.348 \tTraining Accuracy: 0.070\n",
      "Epoch: 415 \tTraining Loss: 2.348 \tTraining Accuracy: 0.070\n",
      "Epoch: 416 \tTraining Loss: 2.348 \tTraining Accuracy: 0.070\n",
      "Epoch: 417 \tTraining Loss: 2.348 \tTraining Accuracy: 0.070\n",
      "Epoch: 418 \tTraining Loss: 2.348 \tTraining Accuracy: 0.070\n",
      "Epoch: 419 \tTraining Loss: 2.348 \tTraining Accuracy: 0.070\n",
      "Epoch: 420 \tTraining Loss: 2.348 \tTraining Accuracy: 0.070\n",
      "Epoch: 421 \tTraining Loss: 2.348 \tTraining Accuracy: 0.070\n",
      "Epoch: 422 \tTraining Loss: 2.348 \tTraining Accuracy: 0.070\n",
      "Epoch: 423 \tTraining Loss: 2.348 \tTraining Accuracy: 0.070\n",
      "Epoch: 424 \tTraining Loss: 2.348 \tTraining Accuracy: 0.071\n",
      "Epoch: 425 \tTraining Loss: 2.348 \tTraining Accuracy: 0.071\n",
      "Epoch: 426 \tTraining Loss: 2.347 \tTraining Accuracy: 0.071\n",
      "Epoch: 427 \tTraining Loss: 2.347 \tTraining Accuracy: 0.071\n",
      "Epoch: 428 \tTraining Loss: 2.347 \tTraining Accuracy: 0.071\n",
      "Epoch: 429 \tTraining Loss: 2.347 \tTraining Accuracy: 0.071\n",
      "Epoch: 430 \tTraining Loss: 2.347 \tTraining Accuracy: 0.071\n",
      "Epoch: 431 \tTraining Loss: 2.347 \tTraining Accuracy: 0.071\n",
      "Epoch: 432 \tTraining Loss: 2.347 \tTraining Accuracy: 0.071\n",
      "Epoch: 433 \tTraining Loss: 2.347 \tTraining Accuracy: 0.072\n",
      "Epoch: 434 \tTraining Loss: 2.347 \tTraining Accuracy: 0.072\n",
      "Epoch: 435 \tTraining Loss: 2.347 \tTraining Accuracy: 0.072\n",
      "Epoch: 436 \tTraining Loss: 2.347 \tTraining Accuracy: 0.072\n",
      "Epoch: 437 \tTraining Loss: 2.347 \tTraining Accuracy: 0.072\n",
      "Epoch: 438 \tTraining Loss: 2.347 \tTraining Accuracy: 0.072\n",
      "Epoch: 439 \tTraining Loss: 2.347 \tTraining Accuracy: 0.072\n",
      "Epoch: 440 \tTraining Loss: 2.346 \tTraining Accuracy: 0.072\n",
      "Epoch: 441 \tTraining Loss: 2.346 \tTraining Accuracy: 0.072\n",
      "Epoch: 442 \tTraining Loss: 2.346 \tTraining Accuracy: 0.072\n",
      "Epoch: 443 \tTraining Loss: 2.346 \tTraining Accuracy: 0.072\n",
      "Epoch: 444 \tTraining Loss: 2.346 \tTraining Accuracy: 0.072\n",
      "Epoch: 445 \tTraining Loss: 2.346 \tTraining Accuracy: 0.072\n",
      "Epoch: 446 \tTraining Loss: 2.346 \tTraining Accuracy: 0.072\n",
      "Epoch: 447 \tTraining Loss: 2.346 \tTraining Accuracy: 0.072\n",
      "Epoch: 448 \tTraining Loss: 2.346 \tTraining Accuracy: 0.072\n",
      "Epoch: 449 \tTraining Loss: 2.346 \tTraining Accuracy: 0.073\n",
      "Epoch: 450 \tTraining Loss: 2.346 \tTraining Accuracy: 0.073\n",
      "Epoch: 451 \tTraining Loss: 2.346 \tTraining Accuracy: 0.073\n",
      "Epoch: 452 \tTraining Loss: 2.346 \tTraining Accuracy: 0.073\n",
      "Epoch: 453 \tTraining Loss: 2.346 \tTraining Accuracy: 0.073\n",
      "Epoch: 454 \tTraining Loss: 2.345 \tTraining Accuracy: 0.073\n",
      "Epoch: 455 \tTraining Loss: 2.345 \tTraining Accuracy: 0.073\n",
      "Epoch: 456 \tTraining Loss: 2.345 \tTraining Accuracy: 0.073\n",
      "Epoch: 457 \tTraining Loss: 2.345 \tTraining Accuracy: 0.073\n",
      "Epoch: 458 \tTraining Loss: 2.345 \tTraining Accuracy: 0.073\n",
      "Epoch: 459 \tTraining Loss: 2.345 \tTraining Accuracy: 0.073\n",
      "Epoch: 460 \tTraining Loss: 2.345 \tTraining Accuracy: 0.074\n",
      "Epoch: 461 \tTraining Loss: 2.345 \tTraining Accuracy: 0.074\n",
      "Epoch: 462 \tTraining Loss: 2.345 \tTraining Accuracy: 0.073\n",
      "Epoch: 463 \tTraining Loss: 2.345 \tTraining Accuracy: 0.073\n",
      "Epoch: 464 \tTraining Loss: 2.345 \tTraining Accuracy: 0.074\n",
      "Epoch: 465 \tTraining Loss: 2.345 \tTraining Accuracy: 0.074\n",
      "Epoch: 466 \tTraining Loss: 2.345 \tTraining Accuracy: 0.074\n",
      "Epoch: 467 \tTraining Loss: 2.345 \tTraining Accuracy: 0.074\n",
      "Epoch: 468 \tTraining Loss: 2.344 \tTraining Accuracy: 0.074\n",
      "Epoch: 469 \tTraining Loss: 2.344 \tTraining Accuracy: 0.074\n",
      "Epoch: 470 \tTraining Loss: 2.344 \tTraining Accuracy: 0.074\n",
      "Epoch: 471 \tTraining Loss: 2.344 \tTraining Accuracy: 0.074\n",
      "Epoch: 472 \tTraining Loss: 2.344 \tTraining Accuracy: 0.074\n",
      "Epoch: 473 \tTraining Loss: 2.344 \tTraining Accuracy: 0.074\n",
      "Epoch: 474 \tTraining Loss: 2.344 \tTraining Accuracy: 0.074\n",
      "Epoch: 475 \tTraining Loss: 2.344 \tTraining Accuracy: 0.074\n",
      "Epoch: 476 \tTraining Loss: 2.344 \tTraining Accuracy: 0.074\n",
      "Epoch: 477 \tTraining Loss: 2.344 \tTraining Accuracy: 0.074\n",
      "Epoch: 478 \tTraining Loss: 2.344 \tTraining Accuracy: 0.074\n",
      "Epoch: 479 \tTraining Loss: 2.344 \tTraining Accuracy: 0.074\n",
      "Epoch: 480 \tTraining Loss: 2.344 \tTraining Accuracy: 0.075\n",
      "Epoch: 481 \tTraining Loss: 2.344 \tTraining Accuracy: 0.075\n",
      "Epoch: 482 \tTraining Loss: 2.344 \tTraining Accuracy: 0.075\n",
      "Epoch: 483 \tTraining Loss: 2.343 \tTraining Accuracy: 0.075\n",
      "Epoch: 484 \tTraining Loss: 2.343 \tTraining Accuracy: 0.075\n",
      "Epoch: 485 \tTraining Loss: 2.343 \tTraining Accuracy: 0.075\n",
      "Epoch: 486 \tTraining Loss: 2.343 \tTraining Accuracy: 0.075\n",
      "Epoch: 487 \tTraining Loss: 2.343 \tTraining Accuracy: 0.075\n",
      "Epoch: 488 \tTraining Loss: 2.343 \tTraining Accuracy: 0.075\n",
      "Epoch: 489 \tTraining Loss: 2.343 \tTraining Accuracy: 0.075\n",
      "Epoch: 490 \tTraining Loss: 2.343 \tTraining Accuracy: 0.075\n",
      "Epoch: 491 \tTraining Loss: 2.343 \tTraining Accuracy: 0.075\n",
      "Epoch: 492 \tTraining Loss: 2.343 \tTraining Accuracy: 0.076\n",
      "Epoch: 493 \tTraining Loss: 2.343 \tTraining Accuracy: 0.076\n",
      "Epoch: 494 \tTraining Loss: 2.343 \tTraining Accuracy: 0.076\n",
      "Epoch: 495 \tTraining Loss: 2.343 \tTraining Accuracy: 0.076\n",
      "Epoch: 496 \tTraining Loss: 2.343 \tTraining Accuracy: 0.076\n",
      "Epoch: 497 \tTraining Loss: 2.342 \tTraining Accuracy: 0.076\n",
      "Epoch: 498 \tTraining Loss: 2.342 \tTraining Accuracy: 0.076\n",
      "Epoch: 499 \tTraining Loss: 2.342 \tTraining Accuracy: 0.076\n",
      "Epoch: 500 \tTraining Loss: 2.342 \tTraining Accuracy: 0.076\n",
      "Epoch: 501 \tTraining Loss: 2.342 \tTraining Accuracy: 0.076\n",
      "Epoch: 502 \tTraining Loss: 2.342 \tTraining Accuracy: 0.076\n",
      "Epoch: 503 \tTraining Loss: 2.342 \tTraining Accuracy: 0.076\n",
      "Epoch: 504 \tTraining Loss: 2.342 \tTraining Accuracy: 0.076\n",
      "Epoch: 505 \tTraining Loss: 2.342 \tTraining Accuracy: 0.077\n",
      "Epoch: 506 \tTraining Loss: 2.342 \tTraining Accuracy: 0.077\n",
      "Epoch: 507 \tTraining Loss: 2.342 \tTraining Accuracy: 0.077\n",
      "Epoch: 508 \tTraining Loss: 2.342 \tTraining Accuracy: 0.077\n",
      "Epoch: 509 \tTraining Loss: 2.342 \tTraining Accuracy: 0.077\n",
      "Epoch: 510 \tTraining Loss: 2.342 \tTraining Accuracy: 0.078\n",
      "Epoch: 511 \tTraining Loss: 2.342 \tTraining Accuracy: 0.078\n",
      "Epoch: 512 \tTraining Loss: 2.341 \tTraining Accuracy: 0.078\n",
      "Epoch: 513 \tTraining Loss: 2.341 \tTraining Accuracy: 0.078\n",
      "Epoch: 514 \tTraining Loss: 2.341 \tTraining Accuracy: 0.078\n",
      "Epoch: 515 \tTraining Loss: 2.341 \tTraining Accuracy: 0.078\n",
      "Epoch: 516 \tTraining Loss: 2.341 \tTraining Accuracy: 0.078\n",
      "Epoch: 517 \tTraining Loss: 2.341 \tTraining Accuracy: 0.078\n",
      "Epoch: 518 \tTraining Loss: 2.341 \tTraining Accuracy: 0.078\n",
      "Epoch: 519 \tTraining Loss: 2.341 \tTraining Accuracy: 0.078\n",
      "Epoch: 520 \tTraining Loss: 2.341 \tTraining Accuracy: 0.078\n",
      "Epoch: 521 \tTraining Loss: 2.341 \tTraining Accuracy: 0.078\n",
      "Epoch: 522 \tTraining Loss: 2.341 \tTraining Accuracy: 0.078\n",
      "Epoch: 523 \tTraining Loss: 2.341 \tTraining Accuracy: 0.079\n",
      "Epoch: 524 \tTraining Loss: 2.341 \tTraining Accuracy: 0.079\n",
      "Epoch: 525 \tTraining Loss: 2.341 \tTraining Accuracy: 0.079\n",
      "Epoch: 526 \tTraining Loss: 2.340 \tTraining Accuracy: 0.079\n",
      "Epoch: 527 \tTraining Loss: 2.340 \tTraining Accuracy: 0.079\n",
      "Epoch: 528 \tTraining Loss: 2.340 \tTraining Accuracy: 0.079\n",
      "Epoch: 529 \tTraining Loss: 2.340 \tTraining Accuracy: 0.079\n",
      "Epoch: 530 \tTraining Loss: 2.340 \tTraining Accuracy: 0.079\n",
      "Epoch: 531 \tTraining Loss: 2.340 \tTraining Accuracy: 0.079\n",
      "Epoch: 532 \tTraining Loss: 2.340 \tTraining Accuracy: 0.079\n",
      "Epoch: 533 \tTraining Loss: 2.340 \tTraining Accuracy: 0.079\n",
      "Epoch: 534 \tTraining Loss: 2.340 \tTraining Accuracy: 0.080\n",
      "Epoch: 535 \tTraining Loss: 2.340 \tTraining Accuracy: 0.080\n",
      "Epoch: 536 \tTraining Loss: 2.340 \tTraining Accuracy: 0.080\n",
      "Epoch: 537 \tTraining Loss: 2.340 \tTraining Accuracy: 0.080\n",
      "Epoch: 538 \tTraining Loss: 2.340 \tTraining Accuracy: 0.080\n",
      "Epoch: 539 \tTraining Loss: 2.340 \tTraining Accuracy: 0.080\n",
      "Epoch: 540 \tTraining Loss: 2.340 \tTraining Accuracy: 0.080\n",
      "Epoch: 541 \tTraining Loss: 2.339 \tTraining Accuracy: 0.080\n",
      "Epoch: 542 \tTraining Loss: 2.339 \tTraining Accuracy: 0.080\n",
      "Epoch: 543 \tTraining Loss: 2.339 \tTraining Accuracy: 0.080\n",
      "Epoch: 544 \tTraining Loss: 2.339 \tTraining Accuracy: 0.080\n",
      "Epoch: 545 \tTraining Loss: 2.339 \tTraining Accuracy: 0.080\n",
      "Epoch: 546 \tTraining Loss: 2.339 \tTraining Accuracy: 0.081\n",
      "Epoch: 547 \tTraining Loss: 2.339 \tTraining Accuracy: 0.081\n",
      "Epoch: 548 \tTraining Loss: 2.339 \tTraining Accuracy: 0.081\n",
      "Epoch: 549 \tTraining Loss: 2.339 \tTraining Accuracy: 0.081\n",
      "Epoch: 550 \tTraining Loss: 2.339 \tTraining Accuracy: 0.081\n",
      "Epoch: 551 \tTraining Loss: 2.339 \tTraining Accuracy: 0.081\n",
      "Epoch: 552 \tTraining Loss: 2.339 \tTraining Accuracy: 0.081\n",
      "Epoch: 553 \tTraining Loss: 2.339 \tTraining Accuracy: 0.081\n",
      "Epoch: 554 \tTraining Loss: 2.339 \tTraining Accuracy: 0.081\n",
      "Epoch: 555 \tTraining Loss: 2.339 \tTraining Accuracy: 0.081\n",
      "Epoch: 556 \tTraining Loss: 2.338 \tTraining Accuracy: 0.081\n",
      "Epoch: 557 \tTraining Loss: 2.338 \tTraining Accuracy: 0.081\n",
      "Epoch: 558 \tTraining Loss: 2.338 \tTraining Accuracy: 0.081\n",
      "Epoch: 559 \tTraining Loss: 2.338 \tTraining Accuracy: 0.082\n",
      "Epoch: 560 \tTraining Loss: 2.338 \tTraining Accuracy: 0.082\n",
      "Epoch: 561 \tTraining Loss: 2.338 \tTraining Accuracy: 0.082\n",
      "Epoch: 562 \tTraining Loss: 2.338 \tTraining Accuracy: 0.082\n",
      "Epoch: 563 \tTraining Loss: 2.338 \tTraining Accuracy: 0.082\n",
      "Epoch: 564 \tTraining Loss: 2.338 \tTraining Accuracy: 0.082\n",
      "Epoch: 565 \tTraining Loss: 2.338 \tTraining Accuracy: 0.082\n",
      "Epoch: 566 \tTraining Loss: 2.338 \tTraining Accuracy: 0.082\n",
      "Epoch: 567 \tTraining Loss: 2.338 \tTraining Accuracy: 0.082\n",
      "Epoch: 568 \tTraining Loss: 2.338 \tTraining Accuracy: 0.082\n",
      "Epoch: 569 \tTraining Loss: 2.338 \tTraining Accuracy: 0.082\n",
      "Epoch: 570 \tTraining Loss: 2.337 \tTraining Accuracy: 0.082\n",
      "Epoch: 571 \tTraining Loss: 2.337 \tTraining Accuracy: 0.083\n",
      "Epoch: 572 \tTraining Loss: 2.337 \tTraining Accuracy: 0.083\n",
      "Epoch: 573 \tTraining Loss: 2.337 \tTraining Accuracy: 0.083\n",
      "Epoch: 574 \tTraining Loss: 2.337 \tTraining Accuracy: 0.083\n",
      "Epoch: 575 \tTraining Loss: 2.337 \tTraining Accuracy: 0.083\n",
      "Epoch: 576 \tTraining Loss: 2.337 \tTraining Accuracy: 0.083\n",
      "Epoch: 577 \tTraining Loss: 2.337 \tTraining Accuracy: 0.083\n",
      "Epoch: 578 \tTraining Loss: 2.337 \tTraining Accuracy: 0.083\n",
      "Epoch: 579 \tTraining Loss: 2.337 \tTraining Accuracy: 0.083\n",
      "Epoch: 580 \tTraining Loss: 2.337 \tTraining Accuracy: 0.083\n",
      "Epoch: 581 \tTraining Loss: 2.337 \tTraining Accuracy: 0.083\n",
      "Epoch: 582 \tTraining Loss: 2.337 \tTraining Accuracy: 0.083\n",
      "Epoch: 583 \tTraining Loss: 2.337 \tTraining Accuracy: 0.084\n",
      "Epoch: 584 \tTraining Loss: 2.337 \tTraining Accuracy: 0.084\n",
      "Epoch: 585 \tTraining Loss: 2.336 \tTraining Accuracy: 0.084\n",
      "Epoch: 586 \tTraining Loss: 2.336 \tTraining Accuracy: 0.084\n",
      "Epoch: 587 \tTraining Loss: 2.336 \tTraining Accuracy: 0.084\n",
      "Epoch: 588 \tTraining Loss: 2.336 \tTraining Accuracy: 0.084\n",
      "Epoch: 589 \tTraining Loss: 2.336 \tTraining Accuracy: 0.084\n",
      "Epoch: 590 \tTraining Loss: 2.336 \tTraining Accuracy: 0.084\n",
      "Epoch: 591 \tTraining Loss: 2.336 \tTraining Accuracy: 0.084\n",
      "Epoch: 592 \tTraining Loss: 2.336 \tTraining Accuracy: 0.085\n",
      "Epoch: 593 \tTraining Loss: 2.336 \tTraining Accuracy: 0.085\n",
      "Epoch: 594 \tTraining Loss: 2.336 \tTraining Accuracy: 0.085\n",
      "Epoch: 595 \tTraining Loss: 2.336 \tTraining Accuracy: 0.085\n",
      "Epoch: 596 \tTraining Loss: 2.336 \tTraining Accuracy: 0.085\n",
      "Epoch: 597 \tTraining Loss: 2.336 \tTraining Accuracy: 0.085\n",
      "Epoch: 598 \tTraining Loss: 2.336 \tTraining Accuracy: 0.085\n",
      "Epoch: 599 \tTraining Loss: 2.336 \tTraining Accuracy: 0.085\n",
      "Epoch: 600 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 601 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 602 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 603 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 604 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 605 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 606 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 607 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 608 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 609 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 610 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 611 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 612 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 613 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 614 \tTraining Loss: 2.335 \tTraining Accuracy: 0.086\n",
      "Epoch: 615 \tTraining Loss: 2.334 \tTraining Accuracy: 0.086\n",
      "Epoch: 616 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 617 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 618 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 619 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 620 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 621 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 622 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 623 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 624 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 625 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 626 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 627 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 628 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 629 \tTraining Loss: 2.334 \tTraining Accuracy: 0.087\n",
      "Epoch: 630 \tTraining Loss: 2.333 \tTraining Accuracy: 0.087\n",
      "Epoch: 631 \tTraining Loss: 2.333 \tTraining Accuracy: 0.087\n",
      "Epoch: 632 \tTraining Loss: 2.333 \tTraining Accuracy: 0.087\n",
      "Epoch: 633 \tTraining Loss: 2.333 \tTraining Accuracy: 0.088\n",
      "Epoch: 634 \tTraining Loss: 2.333 \tTraining Accuracy: 0.088\n",
      "Epoch: 635 \tTraining Loss: 2.333 \tTraining Accuracy: 0.088\n",
      "Epoch: 636 \tTraining Loss: 2.333 \tTraining Accuracy: 0.088\n",
      "Epoch: 637 \tTraining Loss: 2.333 \tTraining Accuracy: 0.088\n",
      "Epoch: 638 \tTraining Loss: 2.333 \tTraining Accuracy: 0.088\n",
      "Epoch: 639 \tTraining Loss: 2.333 \tTraining Accuracy: 0.089\n",
      "Epoch: 640 \tTraining Loss: 2.333 \tTraining Accuracy: 0.089\n",
      "Epoch: 641 \tTraining Loss: 2.333 \tTraining Accuracy: 0.089\n",
      "Epoch: 642 \tTraining Loss: 2.333 \tTraining Accuracy: 0.089\n",
      "Epoch: 643 \tTraining Loss: 2.333 \tTraining Accuracy: 0.089\n",
      "Epoch: 644 \tTraining Loss: 2.333 \tTraining Accuracy: 0.089\n",
      "Epoch: 645 \tTraining Loss: 2.333 \tTraining Accuracy: 0.089\n",
      "Epoch: 646 \tTraining Loss: 2.332 \tTraining Accuracy: 0.089\n",
      "Epoch: 647 \tTraining Loss: 2.332 \tTraining Accuracy: 0.089\n",
      "Epoch: 648 \tTraining Loss: 2.332 \tTraining Accuracy: 0.090\n",
      "Epoch: 649 \tTraining Loss: 2.332 \tTraining Accuracy: 0.090\n",
      "Epoch: 650 \tTraining Loss: 2.332 \tTraining Accuracy: 0.090\n",
      "Epoch: 651 \tTraining Loss: 2.332 \tTraining Accuracy: 0.090\n",
      "Epoch: 652 \tTraining Loss: 2.332 \tTraining Accuracy: 0.090\n",
      "Epoch: 653 \tTraining Loss: 2.332 \tTraining Accuracy: 0.090\n",
      "Epoch: 654 \tTraining Loss: 2.332 \tTraining Accuracy: 0.090\n",
      "Epoch: 655 \tTraining Loss: 2.332 \tTraining Accuracy: 0.090\n",
      "Epoch: 656 \tTraining Loss: 2.332 \tTraining Accuracy: 0.091\n",
      "Epoch: 657 \tTraining Loss: 2.332 \tTraining Accuracy: 0.091\n",
      "Epoch: 658 \tTraining Loss: 2.332 \tTraining Accuracy: 0.091\n",
      "Epoch: 659 \tTraining Loss: 2.332 \tTraining Accuracy: 0.091\n",
      "Epoch: 660 \tTraining Loss: 2.332 \tTraining Accuracy: 0.091\n",
      "Epoch: 661 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 662 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 663 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 664 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 665 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 666 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 667 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 668 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 669 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 670 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 671 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 672 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 673 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 674 \tTraining Loss: 2.331 \tTraining Accuracy: 0.091\n",
      "Epoch: 675 \tTraining Loss: 2.331 \tTraining Accuracy: 0.092\n",
      "Epoch: 676 \tTraining Loss: 2.330 \tTraining Accuracy: 0.092\n",
      "Epoch: 677 \tTraining Loss: 2.330 \tTraining Accuracy: 0.092\n",
      "Epoch: 678 \tTraining Loss: 2.330 \tTraining Accuracy: 0.092\n",
      "Epoch: 679 \tTraining Loss: 2.330 \tTraining Accuracy: 0.092\n",
      "Epoch: 680 \tTraining Loss: 2.330 \tTraining Accuracy: 0.092\n",
      "Epoch: 681 \tTraining Loss: 2.330 \tTraining Accuracy: 0.092\n",
      "Epoch: 682 \tTraining Loss: 2.330 \tTraining Accuracy: 0.092\n",
      "Epoch: 683 \tTraining Loss: 2.330 \tTraining Accuracy: 0.092\n",
      "Epoch: 684 \tTraining Loss: 2.330 \tTraining Accuracy: 0.092\n",
      "Epoch: 685 \tTraining Loss: 2.330 \tTraining Accuracy: 0.093\n",
      "Epoch: 686 \tTraining Loss: 2.330 \tTraining Accuracy: 0.093\n",
      "Epoch: 687 \tTraining Loss: 2.330 \tTraining Accuracy: 0.093\n",
      "Epoch: 688 \tTraining Loss: 2.330 \tTraining Accuracy: 0.093\n",
      "Epoch: 689 \tTraining Loss: 2.330 \tTraining Accuracy: 0.093\n",
      "Epoch: 690 \tTraining Loss: 2.330 \tTraining Accuracy: 0.093\n",
      "Epoch: 691 \tTraining Loss: 2.330 \tTraining Accuracy: 0.093\n",
      "Epoch: 692 \tTraining Loss: 2.329 \tTraining Accuracy: 0.093\n",
      "Epoch: 693 \tTraining Loss: 2.329 \tTraining Accuracy: 0.093\n",
      "Epoch: 694 \tTraining Loss: 2.329 \tTraining Accuracy: 0.093\n",
      "Epoch: 695 \tTraining Loss: 2.329 \tTraining Accuracy: 0.093\n",
      "Epoch: 696 \tTraining Loss: 2.329 \tTraining Accuracy: 0.093\n",
      "Epoch: 697 \tTraining Loss: 2.329 \tTraining Accuracy: 0.093\n",
      "Epoch: 698 \tTraining Loss: 2.329 \tTraining Accuracy: 0.093\n",
      "Epoch: 699 \tTraining Loss: 2.329 \tTraining Accuracy: 0.093\n",
      "Epoch: 700 \tTraining Loss: 2.329 \tTraining Accuracy: 0.093\n",
      "Epoch: 701 \tTraining Loss: 2.329 \tTraining Accuracy: 0.093\n",
      "Epoch: 702 \tTraining Loss: 2.329 \tTraining Accuracy: 0.094\n",
      "Epoch: 703 \tTraining Loss: 2.329 \tTraining Accuracy: 0.094\n",
      "Epoch: 704 \tTraining Loss: 2.329 \tTraining Accuracy: 0.094\n",
      "Epoch: 705 \tTraining Loss: 2.329 \tTraining Accuracy: 0.094\n",
      "Epoch: 706 \tTraining Loss: 2.329 \tTraining Accuracy: 0.094\n",
      "Epoch: 707 \tTraining Loss: 2.328 \tTraining Accuracy: 0.094\n",
      "Epoch: 708 \tTraining Loss: 2.328 \tTraining Accuracy: 0.094\n",
      "Epoch: 709 \tTraining Loss: 2.328 \tTraining Accuracy: 0.094\n",
      "Epoch: 710 \tTraining Loss: 2.328 \tTraining Accuracy: 0.094\n",
      "Epoch: 711 \tTraining Loss: 2.328 \tTraining Accuracy: 0.094\n",
      "Epoch: 712 \tTraining Loss: 2.328 \tTraining Accuracy: 0.094\n",
      "Epoch: 713 \tTraining Loss: 2.328 \tTraining Accuracy: 0.094\n",
      "Epoch: 714 \tTraining Loss: 2.328 \tTraining Accuracy: 0.094\n",
      "Epoch: 715 \tTraining Loss: 2.328 \tTraining Accuracy: 0.094\n",
      "Epoch: 716 \tTraining Loss: 2.328 \tTraining Accuracy: 0.094\n",
      "Epoch: 717 \tTraining Loss: 2.328 \tTraining Accuracy: 0.095\n",
      "Epoch: 718 \tTraining Loss: 2.328 \tTraining Accuracy: 0.095\n",
      "Epoch: 719 \tTraining Loss: 2.328 \tTraining Accuracy: 0.095\n",
      "Epoch: 720 \tTraining Loss: 2.328 \tTraining Accuracy: 0.095\n",
      "Epoch: 721 \tTraining Loss: 2.328 \tTraining Accuracy: 0.095\n",
      "Epoch: 722 \tTraining Loss: 2.328 \tTraining Accuracy: 0.095\n",
      "Epoch: 723 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 724 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 725 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 726 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 727 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 728 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 729 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 730 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 731 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 732 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 733 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 734 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 735 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 736 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 737 \tTraining Loss: 2.327 \tTraining Accuracy: 0.095\n",
      "Epoch: 738 \tTraining Loss: 2.326 \tTraining Accuracy: 0.095\n",
      "Epoch: 739 \tTraining Loss: 2.326 \tTraining Accuracy: 0.095\n",
      "Epoch: 740 \tTraining Loss: 2.326 \tTraining Accuracy: 0.095\n",
      "Epoch: 741 \tTraining Loss: 2.326 \tTraining Accuracy: 0.095\n",
      "Epoch: 742 \tTraining Loss: 2.326 \tTraining Accuracy: 0.095\n",
      "Epoch: 743 \tTraining Loss: 2.326 \tTraining Accuracy: 0.096\n",
      "Epoch: 744 \tTraining Loss: 2.326 \tTraining Accuracy: 0.096\n",
      "Epoch: 745 \tTraining Loss: 2.326 \tTraining Accuracy: 0.096\n",
      "Epoch: 746 \tTraining Loss: 2.326 \tTraining Accuracy: 0.096\n",
      "Epoch: 747 \tTraining Loss: 2.326 \tTraining Accuracy: 0.096\n",
      "Epoch: 748 \tTraining Loss: 2.326 \tTraining Accuracy: 0.096\n",
      "Epoch: 749 \tTraining Loss: 2.326 \tTraining Accuracy: 0.096\n",
      "Epoch: 750 \tTraining Loss: 2.326 \tTraining Accuracy: 0.096\n",
      "Epoch: 751 \tTraining Loss: 2.326 \tTraining Accuracy: 0.096\n",
      "Epoch: 752 \tTraining Loss: 2.326 \tTraining Accuracy: 0.096\n",
      "Epoch: 753 \tTraining Loss: 2.326 \tTraining Accuracy: 0.096\n",
      "Epoch: 754 \tTraining Loss: 2.325 \tTraining Accuracy: 0.096\n",
      "Epoch: 755 \tTraining Loss: 2.325 \tTraining Accuracy: 0.096\n",
      "Epoch: 756 \tTraining Loss: 2.325 \tTraining Accuracy: 0.096\n",
      "Epoch: 757 \tTraining Loss: 2.325 \tTraining Accuracy: 0.096\n",
      "Epoch: 758 \tTraining Loss: 2.325 \tTraining Accuracy: 0.096\n",
      "Epoch: 759 \tTraining Loss: 2.325 \tTraining Accuracy: 0.096\n",
      "Epoch: 760 \tTraining Loss: 2.325 \tTraining Accuracy: 0.096\n",
      "Epoch: 761 \tTraining Loss: 2.325 \tTraining Accuracy: 0.097\n",
      "Epoch: 762 \tTraining Loss: 2.325 \tTraining Accuracy: 0.097\n",
      "Epoch: 763 \tTraining Loss: 2.325 \tTraining Accuracy: 0.097\n",
      "Epoch: 764 \tTraining Loss: 2.325 \tTraining Accuracy: 0.097\n",
      "Epoch: 765 \tTraining Loss: 2.325 \tTraining Accuracy: 0.097\n",
      "Epoch: 766 \tTraining Loss: 2.325 \tTraining Accuracy: 0.097\n",
      "Epoch: 767 \tTraining Loss: 2.325 \tTraining Accuracy: 0.097\n",
      "Epoch: 768 \tTraining Loss: 2.325 \tTraining Accuracy: 0.097\n",
      "Epoch: 769 \tTraining Loss: 2.325 \tTraining Accuracy: 0.097\n",
      "Epoch: 770 \tTraining Loss: 2.324 \tTraining Accuracy: 0.097\n",
      "Epoch: 771 \tTraining Loss: 2.324 \tTraining Accuracy: 0.097\n",
      "Epoch: 772 \tTraining Loss: 2.324 \tTraining Accuracy: 0.097\n",
      "Epoch: 773 \tTraining Loss: 2.324 \tTraining Accuracy: 0.097\n",
      "Epoch: 774 \tTraining Loss: 2.324 \tTraining Accuracy: 0.097\n",
      "Epoch: 775 \tTraining Loss: 2.324 \tTraining Accuracy: 0.098\n",
      "Epoch: 776 \tTraining Loss: 2.324 \tTraining Accuracy: 0.098\n",
      "Epoch: 777 \tTraining Loss: 2.324 \tTraining Accuracy: 0.098\n",
      "Epoch: 778 \tTraining Loss: 2.324 \tTraining Accuracy: 0.098\n",
      "Epoch: 779 \tTraining Loss: 2.324 \tTraining Accuracy: 0.098\n",
      "Epoch: 780 \tTraining Loss: 2.324 \tTraining Accuracy: 0.098\n",
      "Epoch: 781 \tTraining Loss: 2.324 \tTraining Accuracy: 0.098\n",
      "Epoch: 782 \tTraining Loss: 2.324 \tTraining Accuracy: 0.098\n",
      "Epoch: 783 \tTraining Loss: 2.324 \tTraining Accuracy: 0.098\n",
      "Epoch: 784 \tTraining Loss: 2.324 \tTraining Accuracy: 0.098\n",
      "Epoch: 785 \tTraining Loss: 2.324 \tTraining Accuracy: 0.098\n",
      "Epoch: 786 \tTraining Loss: 2.323 \tTraining Accuracy: 0.098\n",
      "Epoch: 787 \tTraining Loss: 2.323 \tTraining Accuracy: 0.098\n",
      "Epoch: 788 \tTraining Loss: 2.323 \tTraining Accuracy: 0.098\n",
      "Epoch: 789 \tTraining Loss: 2.323 \tTraining Accuracy: 0.098\n",
      "Epoch: 790 \tTraining Loss: 2.323 \tTraining Accuracy: 0.098\n",
      "Epoch: 791 \tTraining Loss: 2.323 \tTraining Accuracy: 0.098\n",
      "Epoch: 792 \tTraining Loss: 2.323 \tTraining Accuracy: 0.098\n",
      "Epoch: 793 \tTraining Loss: 2.323 \tTraining Accuracy: 0.099\n",
      "Epoch: 794 \tTraining Loss: 2.323 \tTraining Accuracy: 0.099\n",
      "Epoch: 795 \tTraining Loss: 2.323 \tTraining Accuracy: 0.098\n",
      "Epoch: 796 \tTraining Loss: 2.323 \tTraining Accuracy: 0.099\n",
      "Epoch: 797 \tTraining Loss: 2.323 \tTraining Accuracy: 0.099\n",
      "Epoch: 798 \tTraining Loss: 2.323 \tTraining Accuracy: 0.099\n",
      "Epoch: 799 \tTraining Loss: 2.323 \tTraining Accuracy: 0.099\n",
      "Epoch: 800 \tTraining Loss: 2.323 \tTraining Accuracy: 0.099\n",
      "Epoch: 801 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 802 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 803 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 804 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 805 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 806 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 807 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 808 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 809 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 810 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 811 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 812 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 813 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 814 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 815 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 816 \tTraining Loss: 2.322 \tTraining Accuracy: 0.099\n",
      "Epoch: 817 \tTraining Loss: 2.321 \tTraining Accuracy: 0.099\n",
      "Epoch: 818 \tTraining Loss: 2.321 \tTraining Accuracy: 0.099\n",
      "Epoch: 819 \tTraining Loss: 2.321 \tTraining Accuracy: 0.099\n",
      "Epoch: 820 \tTraining Loss: 2.321 \tTraining Accuracy: 0.099\n",
      "Epoch: 821 \tTraining Loss: 2.321 \tTraining Accuracy: 0.099\n",
      "Epoch: 822 \tTraining Loss: 2.321 \tTraining Accuracy: 0.099\n",
      "Epoch: 823 \tTraining Loss: 2.321 \tTraining Accuracy: 0.099\n",
      "Epoch: 824 \tTraining Loss: 2.321 \tTraining Accuracy: 0.100\n",
      "Epoch: 825 \tTraining Loss: 2.321 \tTraining Accuracy: 0.100\n",
      "Epoch: 826 \tTraining Loss: 2.321 \tTraining Accuracy: 0.100\n",
      "Epoch: 827 \tTraining Loss: 2.321 \tTraining Accuracy: 0.100\n",
      "Epoch: 828 \tTraining Loss: 2.321 \tTraining Accuracy: 0.100\n",
      "Epoch: 829 \tTraining Loss: 2.321 \tTraining Accuracy: 0.100\n",
      "Epoch: 830 \tTraining Loss: 2.321 \tTraining Accuracy: 0.100\n",
      "Epoch: 831 \tTraining Loss: 2.321 \tTraining Accuracy: 0.100\n",
      "Epoch: 832 \tTraining Loss: 2.321 \tTraining Accuracy: 0.100\n",
      "Epoch: 833 \tTraining Loss: 2.320 \tTraining Accuracy: 0.100\n",
      "Epoch: 834 \tTraining Loss: 2.320 \tTraining Accuracy: 0.100\n",
      "Epoch: 835 \tTraining Loss: 2.320 \tTraining Accuracy: 0.101\n",
      "Epoch: 836 \tTraining Loss: 2.320 \tTraining Accuracy: 0.101\n",
      "Epoch: 837 \tTraining Loss: 2.320 \tTraining Accuracy: 0.101\n",
      "Epoch: 838 \tTraining Loss: 2.320 \tTraining Accuracy: 0.101\n",
      "Epoch: 839 \tTraining Loss: 2.320 \tTraining Accuracy: 0.101\n",
      "Epoch: 840 \tTraining Loss: 2.320 \tTraining Accuracy: 0.101\n",
      "Epoch: 841 \tTraining Loss: 2.320 \tTraining Accuracy: 0.101\n",
      "Epoch: 842 \tTraining Loss: 2.320 \tTraining Accuracy: 0.101\n",
      "Epoch: 843 \tTraining Loss: 2.320 \tTraining Accuracy: 0.101\n",
      "Epoch: 844 \tTraining Loss: 2.320 \tTraining Accuracy: 0.101\n",
      "Epoch: 845 \tTraining Loss: 2.320 \tTraining Accuracy: 0.101\n",
      "Epoch: 846 \tTraining Loss: 2.320 \tTraining Accuracy: 0.102\n",
      "Epoch: 847 \tTraining Loss: 2.320 \tTraining Accuracy: 0.102\n",
      "Epoch: 848 \tTraining Loss: 2.320 \tTraining Accuracy: 0.102\n",
      "Epoch: 849 \tTraining Loss: 2.320 \tTraining Accuracy: 0.102\n",
      "Epoch: 850 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 851 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 852 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 853 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 854 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 855 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 856 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 857 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 858 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 859 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 860 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 861 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 862 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 863 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 864 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 865 \tTraining Loss: 2.319 \tTraining Accuracy: 0.102\n",
      "Epoch: 866 \tTraining Loss: 2.318 \tTraining Accuracy: 0.102\n",
      "Epoch: 867 \tTraining Loss: 2.318 \tTraining Accuracy: 0.102\n",
      "Epoch: 868 \tTraining Loss: 2.318 \tTraining Accuracy: 0.102\n",
      "Epoch: 869 \tTraining Loss: 2.318 \tTraining Accuracy: 0.102\n",
      "Epoch: 870 \tTraining Loss: 2.318 \tTraining Accuracy: 0.102\n",
      "Epoch: 871 \tTraining Loss: 2.318 \tTraining Accuracy: 0.102\n",
      "Epoch: 872 \tTraining Loss: 2.318 \tTraining Accuracy: 0.103\n",
      "Epoch: 873 \tTraining Loss: 2.318 \tTraining Accuracy: 0.103\n",
      "Epoch: 874 \tTraining Loss: 2.318 \tTraining Accuracy: 0.103\n",
      "Epoch: 875 \tTraining Loss: 2.318 \tTraining Accuracy: 0.103\n",
      "Epoch: 876 \tTraining Loss: 2.318 \tTraining Accuracy: 0.103\n",
      "Epoch: 877 \tTraining Loss: 2.318 \tTraining Accuracy: 0.103\n",
      "Epoch: 878 \tTraining Loss: 2.318 \tTraining Accuracy: 0.103\n",
      "Epoch: 879 \tTraining Loss: 2.318 \tTraining Accuracy: 0.103\n",
      "Epoch: 880 \tTraining Loss: 2.318 \tTraining Accuracy: 0.104\n",
      "Epoch: 881 \tTraining Loss: 2.318 \tTraining Accuracy: 0.104\n",
      "Epoch: 882 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 883 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 884 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 885 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 886 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 887 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 888 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 889 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 890 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 891 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 892 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 893 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 894 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 895 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 896 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 897 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 898 \tTraining Loss: 2.317 \tTraining Accuracy: 0.104\n",
      "Epoch: 899 \tTraining Loss: 2.316 \tTraining Accuracy: 0.104\n",
      "Epoch: 900 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 901 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 902 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 903 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 904 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 905 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 906 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 907 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 908 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 909 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 910 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 911 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 912 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 913 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 914 \tTraining Loss: 2.316 \tTraining Accuracy: 0.105\n",
      "Epoch: 915 \tTraining Loss: 2.315 \tTraining Accuracy: 0.105\n",
      "Epoch: 916 \tTraining Loss: 2.315 \tTraining Accuracy: 0.105\n",
      "Epoch: 917 \tTraining Loss: 2.315 \tTraining Accuracy: 0.105\n",
      "Epoch: 918 \tTraining Loss: 2.315 \tTraining Accuracy: 0.105\n",
      "Epoch: 919 \tTraining Loss: 2.315 \tTraining Accuracy: 0.105\n",
      "Epoch: 920 \tTraining Loss: 2.315 \tTraining Accuracy: 0.106\n",
      "Epoch: 921 \tTraining Loss: 2.315 \tTraining Accuracy: 0.106\n",
      "Epoch: 922 \tTraining Loss: 2.315 \tTraining Accuracy: 0.106\n",
      "Epoch: 923 \tTraining Loss: 2.315 \tTraining Accuracy: 0.106\n",
      "Epoch: 924 \tTraining Loss: 2.315 \tTraining Accuracy: 0.106\n",
      "Epoch: 925 \tTraining Loss: 2.315 \tTraining Accuracy: 0.106\n",
      "Epoch: 926 \tTraining Loss: 2.315 \tTraining Accuracy: 0.106\n",
      "Epoch: 927 \tTraining Loss: 2.315 \tTraining Accuracy: 0.106\n",
      "Epoch: 928 \tTraining Loss: 2.315 \tTraining Accuracy: 0.106\n",
      "Epoch: 929 \tTraining Loss: 2.315 \tTraining Accuracy: 0.106\n",
      "Epoch: 930 \tTraining Loss: 2.315 \tTraining Accuracy: 0.106\n",
      "Epoch: 931 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 932 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 933 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 934 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 935 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 936 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 937 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 938 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 939 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 940 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 941 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 942 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 943 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 944 \tTraining Loss: 2.314 \tTraining Accuracy: 0.106\n",
      "Epoch: 945 \tTraining Loss: 2.314 \tTraining Accuracy: 0.107\n",
      "Epoch: 946 \tTraining Loss: 2.314 \tTraining Accuracy: 0.107\n",
      "Epoch: 947 \tTraining Loss: 2.314 \tTraining Accuracy: 0.107\n",
      "Epoch: 948 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 949 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 950 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 951 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 952 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 953 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 954 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 955 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 956 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 957 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 958 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 959 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 960 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 961 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 962 \tTraining Loss: 2.313 \tTraining Accuracy: 0.107\n",
      "Epoch: 963 \tTraining Loss: 2.313 \tTraining Accuracy: 0.108\n",
      "Epoch: 964 \tTraining Loss: 2.313 \tTraining Accuracy: 0.108\n",
      "Epoch: 965 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 966 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 967 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 968 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 969 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 970 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 971 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 972 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 973 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 974 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 975 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 976 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 977 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 978 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 979 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 980 \tTraining Loss: 2.312 \tTraining Accuracy: 0.108\n",
      "Epoch: 981 \tTraining Loss: 2.311 \tTraining Accuracy: 0.108\n",
      "Epoch: 982 \tTraining Loss: 2.311 \tTraining Accuracy: 0.108\n",
      "Epoch: 983 \tTraining Loss: 2.311 \tTraining Accuracy: 0.108\n",
      "Epoch: 984 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 985 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 986 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 987 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 988 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 989 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 990 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 991 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 992 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 993 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 994 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 995 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 996 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 997 \tTraining Loss: 2.311 \tTraining Accuracy: 0.109\n",
      "Epoch: 998 \tTraining Loss: 2.310 \tTraining Accuracy: 0.109\n",
      "Epoch: 999 \tTraining Loss: 2.310 \tTraining Accuracy: 0.109\n",
      "Epoch: 1000 \tTraining Loss: 2.310 \tTraining Accuracy: 0.110\n",
      "Epoch: 1001 \tTraining Loss: 2.310 \tTraining Accuracy: 0.110\n",
      "Epoch: 1002 \tTraining Loss: 2.310 \tTraining Accuracy: 0.110\n",
      "Epoch: 1003 \tTraining Loss: 2.310 \tTraining Accuracy: 0.110\n",
      "Epoch: 1004 \tTraining Loss: 2.310 \tTraining Accuracy: 0.110\n",
      "Epoch: 1005 \tTraining Loss: 2.310 \tTraining Accuracy: 0.110\n",
      "Epoch: 1006 \tTraining Loss: 2.310 \tTraining Accuracy: 0.110\n",
      "Epoch: 1007 \tTraining Loss: 2.310 \tTraining Accuracy: 0.110\n",
      "Epoch: 1008 \tTraining Loss: 2.310 \tTraining Accuracy: 0.110\n",
      "Epoch: 1009 \tTraining Loss: 2.310 \tTraining Accuracy: 0.111\n",
      "Epoch: 1010 \tTraining Loss: 2.310 \tTraining Accuracy: 0.111\n",
      "Epoch: 1011 \tTraining Loss: 2.310 \tTraining Accuracy: 0.111\n",
      "Epoch: 1012 \tTraining Loss: 2.310 \tTraining Accuracy: 0.111\n",
      "Epoch: 1013 \tTraining Loss: 2.310 \tTraining Accuracy: 0.111\n",
      "Epoch: 1014 \tTraining Loss: 2.310 \tTraining Accuracy: 0.111\n",
      "Epoch: 1015 \tTraining Loss: 2.309 \tTraining Accuracy: 0.111\n",
      "Epoch: 1016 \tTraining Loss: 2.309 \tTraining Accuracy: 0.111\n",
      "Epoch: 1017 \tTraining Loss: 2.309 \tTraining Accuracy: 0.111\n",
      "Epoch: 1018 \tTraining Loss: 2.309 \tTraining Accuracy: 0.111\n",
      "Epoch: 1019 \tTraining Loss: 2.309 \tTraining Accuracy: 0.111\n",
      "Epoch: 1020 \tTraining Loss: 2.309 \tTraining Accuracy: 0.111\n",
      "Epoch: 1021 \tTraining Loss: 2.309 \tTraining Accuracy: 0.112\n",
      "Epoch: 1022 \tTraining Loss: 2.309 \tTraining Accuracy: 0.112\n",
      "Epoch: 1023 \tTraining Loss: 2.309 \tTraining Accuracy: 0.112\n",
      "Epoch: 1024 \tTraining Loss: 2.309 \tTraining Accuracy: 0.112\n",
      "Epoch: 1025 \tTraining Loss: 2.309 \tTraining Accuracy: 0.112\n",
      "Epoch: 1026 \tTraining Loss: 2.309 \tTraining Accuracy: 0.112\n",
      "Epoch: 1027 \tTraining Loss: 2.309 \tTraining Accuracy: 0.112\n",
      "Epoch: 1028 \tTraining Loss: 2.309 \tTraining Accuracy: 0.112\n",
      "Epoch: 1029 \tTraining Loss: 2.309 \tTraining Accuracy: 0.112\n",
      "Epoch: 1030 \tTraining Loss: 2.309 \tTraining Accuracy: 0.112\n",
      "Epoch: 1031 \tTraining Loss: 2.309 \tTraining Accuracy: 0.112\n",
      "Epoch: 1032 \tTraining Loss: 2.308 \tTraining Accuracy: 0.112\n",
      "Epoch: 1033 \tTraining Loss: 2.308 \tTraining Accuracy: 0.112\n",
      "Epoch: 1034 \tTraining Loss: 2.308 \tTraining Accuracy: 0.112\n",
      "Epoch: 1035 \tTraining Loss: 2.308 \tTraining Accuracy: 0.112\n",
      "Epoch: 1036 \tTraining Loss: 2.308 \tTraining Accuracy: 0.112\n",
      "Epoch: 1037 \tTraining Loss: 2.308 \tTraining Accuracy: 0.112\n",
      "Epoch: 1038 \tTraining Loss: 2.308 \tTraining Accuracy: 0.113\n",
      "Epoch: 1039 \tTraining Loss: 2.308 \tTraining Accuracy: 0.113\n",
      "Epoch: 1040 \tTraining Loss: 2.308 \tTraining Accuracy: 0.113\n",
      "Epoch: 1041 \tTraining Loss: 2.308 \tTraining Accuracy: 0.113\n",
      "Epoch: 1042 \tTraining Loss: 2.308 \tTraining Accuracy: 0.113\n",
      "Epoch: 1043 \tTraining Loss: 2.308 \tTraining Accuracy: 0.113\n",
      "Epoch: 1044 \tTraining Loss: 2.308 \tTraining Accuracy: 0.113\n",
      "Epoch: 1045 \tTraining Loss: 2.308 \tTraining Accuracy: 0.113\n",
      "Epoch: 1046 \tTraining Loss: 2.308 \tTraining Accuracy: 0.113\n",
      "Epoch: 1047 \tTraining Loss: 2.308 \tTraining Accuracy: 0.113\n",
      "Epoch: 1048 \tTraining Loss: 2.308 \tTraining Accuracy: 0.113\n",
      "Epoch: 1049 \tTraining Loss: 2.307 \tTraining Accuracy: 0.113\n",
      "Epoch: 1050 \tTraining Loss: 2.307 \tTraining Accuracy: 0.113\n",
      "Epoch: 1051 \tTraining Loss: 2.307 \tTraining Accuracy: 0.113\n",
      "Epoch: 1052 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1053 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1054 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1055 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1056 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1057 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1058 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1059 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1060 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1061 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1062 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1063 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1064 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1065 \tTraining Loss: 2.307 \tTraining Accuracy: 0.114\n",
      "Epoch: 1066 \tTraining Loss: 2.306 \tTraining Accuracy: 0.114\n",
      "Epoch: 1067 \tTraining Loss: 2.306 \tTraining Accuracy: 0.114\n",
      "Epoch: 1068 \tTraining Loss: 2.306 \tTraining Accuracy: 0.114\n",
      "Epoch: 1069 \tTraining Loss: 2.306 \tTraining Accuracy: 0.114\n",
      "Epoch: 1070 \tTraining Loss: 2.306 \tTraining Accuracy: 0.114\n",
      "Epoch: 1071 \tTraining Loss: 2.306 \tTraining Accuracy: 0.115\n",
      "Epoch: 1072 \tTraining Loss: 2.306 \tTraining Accuracy: 0.115\n",
      "Epoch: 1073 \tTraining Loss: 2.306 \tTraining Accuracy: 0.115\n",
      "Epoch: 1074 \tTraining Loss: 2.306 \tTraining Accuracy: 0.115\n",
      "Epoch: 1075 \tTraining Loss: 2.306 \tTraining Accuracy: 0.115\n",
      "Epoch: 1076 \tTraining Loss: 2.306 \tTraining Accuracy: 0.115\n",
      "Epoch: 1077 \tTraining Loss: 2.306 \tTraining Accuracy: 0.115\n",
      "Epoch: 1078 \tTraining Loss: 2.306 \tTraining Accuracy: 0.115\n",
      "Epoch: 1079 \tTraining Loss: 2.306 \tTraining Accuracy: 0.115\n",
      "Epoch: 1080 \tTraining Loss: 2.306 \tTraining Accuracy: 0.115\n",
      "Epoch: 1081 \tTraining Loss: 2.306 \tTraining Accuracy: 0.115\n",
      "Epoch: 1082 \tTraining Loss: 2.306 \tTraining Accuracy: 0.115\n",
      "Epoch: 1083 \tTraining Loss: 2.306 \tTraining Accuracy: 0.115\n",
      "Epoch: 1084 \tTraining Loss: 2.305 \tTraining Accuracy: 0.115\n",
      "Epoch: 1085 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1086 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1087 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1088 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1089 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1090 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1091 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1092 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1093 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1094 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1095 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1096 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1097 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1098 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1099 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1100 \tTraining Loss: 2.305 \tTraining Accuracy: 0.116\n",
      "Epoch: 1101 \tTraining Loss: 2.304 \tTraining Accuracy: 0.116\n",
      "Epoch: 1102 \tTraining Loss: 2.304 \tTraining Accuracy: 0.116\n",
      "Epoch: 1103 \tTraining Loss: 2.304 \tTraining Accuracy: 0.116\n",
      "Epoch: 1104 \tTraining Loss: 2.304 \tTraining Accuracy: 0.116\n",
      "Epoch: 1105 \tTraining Loss: 2.304 \tTraining Accuracy: 0.116\n",
      "Epoch: 1106 \tTraining Loss: 2.304 \tTraining Accuracy: 0.116\n",
      "Epoch: 1107 \tTraining Loss: 2.304 \tTraining Accuracy: 0.116\n",
      "Epoch: 1108 \tTraining Loss: 2.304 \tTraining Accuracy: 0.116\n",
      "Epoch: 1109 \tTraining Loss: 2.304 \tTraining Accuracy: 0.116\n",
      "Epoch: 1110 \tTraining Loss: 2.304 \tTraining Accuracy: 0.117\n",
      "Epoch: 1111 \tTraining Loss: 2.304 \tTraining Accuracy: 0.117\n",
      "Epoch: 1112 \tTraining Loss: 2.304 \tTraining Accuracy: 0.117\n",
      "Epoch: 1113 \tTraining Loss: 2.304 \tTraining Accuracy: 0.117\n",
      "Epoch: 1114 \tTraining Loss: 2.304 \tTraining Accuracy: 0.117\n",
      "Epoch: 1115 \tTraining Loss: 2.304 \tTraining Accuracy: 0.117\n",
      "Epoch: 1116 \tTraining Loss: 2.304 \tTraining Accuracy: 0.117\n",
      "Epoch: 1117 \tTraining Loss: 2.304 \tTraining Accuracy: 0.117\n",
      "Epoch: 1118 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1119 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1120 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1121 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1122 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1123 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1124 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1125 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1126 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1127 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1128 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1129 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1130 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1131 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1132 \tTraining Loss: 2.303 \tTraining Accuracy: 0.117\n",
      "Epoch: 1133 \tTraining Loss: 2.303 \tTraining Accuracy: 0.118\n",
      "Epoch: 1134 \tTraining Loss: 2.303 \tTraining Accuracy: 0.118\n",
      "Epoch: 1135 \tTraining Loss: 2.303 \tTraining Accuracy: 0.118\n",
      "Epoch: 1136 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1137 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1138 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1139 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1140 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1141 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1142 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1143 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1144 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1145 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1146 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1147 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1148 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1149 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1150 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1151 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1152 \tTraining Loss: 2.302 \tTraining Accuracy: 0.118\n",
      "Epoch: 1153 \tTraining Loss: 2.301 \tTraining Accuracy: 0.118\n",
      "Epoch: 1154 \tTraining Loss: 2.301 \tTraining Accuracy: 0.119\n",
      "Epoch: 1155 \tTraining Loss: 2.301 \tTraining Accuracy: 0.119\n",
      "Epoch: 1156 \tTraining Loss: 2.301 \tTraining Accuracy: 0.119\n",
      "Epoch: 1157 \tTraining Loss: 2.301 \tTraining Accuracy: 0.119\n",
      "Epoch: 1158 \tTraining Loss: 2.301 \tTraining Accuracy: 0.119\n",
      "Epoch: 1159 \tTraining Loss: 2.301 \tTraining Accuracy: 0.119\n",
      "Epoch: 1160 \tTraining Loss: 2.301 \tTraining Accuracy: 0.119\n",
      "Epoch: 1161 \tTraining Loss: 2.301 \tTraining Accuracy: 0.119\n",
      "Epoch: 1162 \tTraining Loss: 2.301 \tTraining Accuracy: 0.119\n",
      "Epoch: 1163 \tTraining Loss: 2.301 \tTraining Accuracy: 0.119\n",
      "Epoch: 1164 \tTraining Loss: 2.301 \tTraining Accuracy: 0.119\n",
      "Epoch: 1165 \tTraining Loss: 2.301 \tTraining Accuracy: 0.119\n",
      "Epoch: 1166 \tTraining Loss: 2.301 \tTraining Accuracy: 0.120\n",
      "Epoch: 1167 \tTraining Loss: 2.301 \tTraining Accuracy: 0.120\n",
      "Epoch: 1168 \tTraining Loss: 2.301 \tTraining Accuracy: 0.120\n",
      "Epoch: 1169 \tTraining Loss: 2.301 \tTraining Accuracy: 0.120\n",
      "Epoch: 1170 \tTraining Loss: 2.301 \tTraining Accuracy: 0.120\n",
      "Epoch: 1171 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1172 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1173 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1174 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1175 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1176 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1177 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1178 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1179 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1180 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1181 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1182 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1183 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1184 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1185 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1186 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1187 \tTraining Loss: 2.300 \tTraining Accuracy: 0.120\n",
      "Epoch: 1188 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1189 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1190 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1191 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1192 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1193 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1194 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1195 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1196 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1197 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1198 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1199 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1200 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1201 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1202 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1203 \tTraining Loss: 2.299 \tTraining Accuracy: 0.120\n",
      "Epoch: 1204 \tTraining Loss: 2.299 \tTraining Accuracy: 0.121\n",
      "Epoch: 1205 \tTraining Loss: 2.299 \tTraining Accuracy: 0.121\n",
      "Epoch: 1206 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1207 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1208 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1209 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1210 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1211 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1212 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1213 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1214 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1215 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1216 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1217 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1218 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1219 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1220 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1221 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1222 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1223 \tTraining Loss: 2.298 \tTraining Accuracy: 0.121\n",
      "Epoch: 1224 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1225 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1226 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1227 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1228 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1229 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1230 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1231 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1232 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1233 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1234 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1235 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1236 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1237 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1238 \tTraining Loss: 2.297 \tTraining Accuracy: 0.121\n",
      "Epoch: 1239 \tTraining Loss: 2.297 \tTraining Accuracy: 0.122\n",
      "Epoch: 1240 \tTraining Loss: 2.297 \tTraining Accuracy: 0.122\n",
      "Epoch: 1241 \tTraining Loss: 2.297 \tTraining Accuracy: 0.122\n",
      "Epoch: 1242 \tTraining Loss: 2.296 \tTraining Accuracy: 0.122\n",
      "Epoch: 1243 \tTraining Loss: 2.296 \tTraining Accuracy: 0.122\n",
      "Epoch: 1244 \tTraining Loss: 2.296 \tTraining Accuracy: 0.122\n",
      "Epoch: 1245 \tTraining Loss: 2.296 \tTraining Accuracy: 0.122\n",
      "Epoch: 1246 \tTraining Loss: 2.296 \tTraining Accuracy: 0.122\n",
      "Epoch: 1247 \tTraining Loss: 2.296 \tTraining Accuracy: 0.122\n",
      "Epoch: 1248 \tTraining Loss: 2.296 \tTraining Accuracy: 0.122\n",
      "Epoch: 1249 \tTraining Loss: 2.296 \tTraining Accuracy: 0.122\n",
      "Epoch: 1250 \tTraining Loss: 2.296 \tTraining Accuracy: 0.122\n",
      "Epoch: 1251 \tTraining Loss: 2.296 \tTraining Accuracy: 0.122\n",
      "Epoch: 1252 \tTraining Loss: 2.296 \tTraining Accuracy: 0.122\n",
      "Epoch: 1253 \tTraining Loss: 2.296 \tTraining Accuracy: 0.123\n",
      "Epoch: 1254 \tTraining Loss: 2.296 \tTraining Accuracy: 0.123\n",
      "Epoch: 1255 \tTraining Loss: 2.296 \tTraining Accuracy: 0.123\n",
      "Epoch: 1256 \tTraining Loss: 2.296 \tTraining Accuracy: 0.123\n",
      "Epoch: 1257 \tTraining Loss: 2.296 \tTraining Accuracy: 0.123\n",
      "Epoch: 1258 \tTraining Loss: 2.296 \tTraining Accuracy: 0.123\n",
      "Epoch: 1259 \tTraining Loss: 2.296 \tTraining Accuracy: 0.123\n",
      "Epoch: 1260 \tTraining Loss: 2.295 \tTraining Accuracy: 0.123\n",
      "Epoch: 1261 \tTraining Loss: 2.295 \tTraining Accuracy: 0.123\n",
      "Epoch: 1262 \tTraining Loss: 2.295 \tTraining Accuracy: 0.123\n",
      "Epoch: 1263 \tTraining Loss: 2.295 \tTraining Accuracy: 0.123\n",
      "Epoch: 1264 \tTraining Loss: 2.295 \tTraining Accuracy: 0.123\n",
      "Epoch: 1265 \tTraining Loss: 2.295 \tTraining Accuracy: 0.123\n",
      "Epoch: 1266 \tTraining Loss: 2.295 \tTraining Accuracy: 0.123\n",
      "Epoch: 1267 \tTraining Loss: 2.295 \tTraining Accuracy: 0.123\n",
      "Epoch: 1268 \tTraining Loss: 2.295 \tTraining Accuracy: 0.123\n",
      "Epoch: 1269 \tTraining Loss: 2.295 \tTraining Accuracy: 0.123\n",
      "Epoch: 1270 \tTraining Loss: 2.295 \tTraining Accuracy: 0.123\n",
      "Epoch: 1271 \tTraining Loss: 2.295 \tTraining Accuracy: 0.124\n",
      "Epoch: 1272 \tTraining Loss: 2.295 \tTraining Accuracy: 0.124\n",
      "Epoch: 1273 \tTraining Loss: 2.295 \tTraining Accuracy: 0.124\n",
      "Epoch: 1274 \tTraining Loss: 2.295 \tTraining Accuracy: 0.124\n",
      "Epoch: 1275 \tTraining Loss: 2.295 \tTraining Accuracy: 0.124\n",
      "Epoch: 1276 \tTraining Loss: 2.295 \tTraining Accuracy: 0.124\n",
      "Epoch: 1277 \tTraining Loss: 2.295 \tTraining Accuracy: 0.124\n",
      "Epoch: 1278 \tTraining Loss: 2.294 \tTraining Accuracy: 0.124\n",
      "Epoch: 1279 \tTraining Loss: 2.294 \tTraining Accuracy: 0.124\n",
      "Epoch: 1280 \tTraining Loss: 2.294 \tTraining Accuracy: 0.124\n",
      "Epoch: 1281 \tTraining Loss: 2.294 \tTraining Accuracy: 0.124\n",
      "Epoch: 1282 \tTraining Loss: 2.294 \tTraining Accuracy: 0.124\n",
      "Epoch: 1283 \tTraining Loss: 2.294 \tTraining Accuracy: 0.124\n",
      "Epoch: 1284 \tTraining Loss: 2.294 \tTraining Accuracy: 0.124\n",
      "Epoch: 1285 \tTraining Loss: 2.294 \tTraining Accuracy: 0.124\n",
      "Epoch: 1286 \tTraining Loss: 2.294 \tTraining Accuracy: 0.124\n",
      "Epoch: 1287 \tTraining Loss: 2.294 \tTraining Accuracy: 0.124\n",
      "Epoch: 1288 \tTraining Loss: 2.294 \tTraining Accuracy: 0.124\n",
      "Epoch: 1289 \tTraining Loss: 2.294 \tTraining Accuracy: 0.124\n",
      "Epoch: 1290 \tTraining Loss: 2.294 \tTraining Accuracy: 0.124\n",
      "Epoch: 1291 \tTraining Loss: 2.294 \tTraining Accuracy: 0.125\n",
      "Epoch: 1292 \tTraining Loss: 2.294 \tTraining Accuracy: 0.125\n",
      "Epoch: 1293 \tTraining Loss: 2.294 \tTraining Accuracy: 0.125\n",
      "Epoch: 1294 \tTraining Loss: 2.294 \tTraining Accuracy: 0.125\n",
      "Epoch: 1295 \tTraining Loss: 2.294 \tTraining Accuracy: 0.125\n",
      "Epoch: 1296 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1297 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1298 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1299 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1300 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1301 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1302 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1303 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1304 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1305 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1306 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1307 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1308 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1309 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1310 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1311 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1312 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1313 \tTraining Loss: 2.293 \tTraining Accuracy: 0.125\n",
      "Epoch: 1314 \tTraining Loss: 2.292 \tTraining Accuracy: 0.125\n",
      "Epoch: 1315 \tTraining Loss: 2.292 \tTraining Accuracy: 0.125\n",
      "Epoch: 1316 \tTraining Loss: 2.292 \tTraining Accuracy: 0.125\n",
      "Epoch: 1317 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1318 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1319 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1320 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1321 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1322 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1323 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1324 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1325 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1326 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1327 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1328 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1329 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1330 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1331 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1332 \tTraining Loss: 2.292 \tTraining Accuracy: 0.126\n",
      "Epoch: 1333 \tTraining Loss: 2.291 \tTraining Accuracy: 0.126\n",
      "Epoch: 1334 \tTraining Loss: 2.291 \tTraining Accuracy: 0.126\n",
      "Epoch: 1335 \tTraining Loss: 2.291 \tTraining Accuracy: 0.126\n",
      "Epoch: 1336 \tTraining Loss: 2.291 \tTraining Accuracy: 0.126\n",
      "Epoch: 1337 \tTraining Loss: 2.291 \tTraining Accuracy: 0.127\n",
      "Epoch: 1338 \tTraining Loss: 2.291 \tTraining Accuracy: 0.127\n",
      "Epoch: 1339 \tTraining Loss: 2.291 \tTraining Accuracy: 0.127\n",
      "Epoch: 1340 \tTraining Loss: 2.291 \tTraining Accuracy: 0.127\n",
      "Epoch: 1341 \tTraining Loss: 2.291 \tTraining Accuracy: 0.127\n",
      "Epoch: 1342 \tTraining Loss: 2.291 \tTraining Accuracy: 0.127\n",
      "Epoch: 1343 \tTraining Loss: 2.291 \tTraining Accuracy: 0.127\n",
      "Epoch: 1344 \tTraining Loss: 2.291 \tTraining Accuracy: 0.127\n",
      "Epoch: 1345 \tTraining Loss: 2.291 \tTraining Accuracy: 0.127\n",
      "Epoch: 1346 \tTraining Loss: 2.291 \tTraining Accuracy: 0.127\n",
      "Epoch: 1347 \tTraining Loss: 2.291 \tTraining Accuracy: 0.127\n",
      "Epoch: 1348 \tTraining Loss: 2.291 \tTraining Accuracy: 0.127\n",
      "Epoch: 1349 \tTraining Loss: 2.291 \tTraining Accuracy: 0.127\n",
      "Epoch: 1350 \tTraining Loss: 2.291 \tTraining Accuracy: 0.128\n",
      "Epoch: 1351 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1352 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1353 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1354 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1355 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1356 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1357 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1358 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1359 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1360 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1361 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1362 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1363 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1364 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1365 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1366 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1367 \tTraining Loss: 2.290 \tTraining Accuracy: 0.128\n",
      "Epoch: 1368 \tTraining Loss: 2.290 \tTraining Accuracy: 0.129\n",
      "Epoch: 1369 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1370 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1371 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1372 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1373 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1374 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1375 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1376 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1377 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1378 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1379 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1380 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1381 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1382 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1383 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1384 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1385 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1386 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1387 \tTraining Loss: 2.289 \tTraining Accuracy: 0.129\n",
      "Epoch: 1388 \tTraining Loss: 2.288 \tTraining Accuracy: 0.129\n",
      "Epoch: 1389 \tTraining Loss: 2.288 \tTraining Accuracy: 0.129\n",
      "Epoch: 1390 \tTraining Loss: 2.288 \tTraining Accuracy: 0.129\n",
      "Epoch: 1391 \tTraining Loss: 2.288 \tTraining Accuracy: 0.129\n",
      "Epoch: 1392 \tTraining Loss: 2.288 \tTraining Accuracy: 0.129\n",
      "Epoch: 1393 \tTraining Loss: 2.288 \tTraining Accuracy: 0.129\n",
      "Epoch: 1394 \tTraining Loss: 2.288 \tTraining Accuracy: 0.130\n",
      "Epoch: 1395 \tTraining Loss: 2.288 \tTraining Accuracy: 0.130\n",
      "Epoch: 1396 \tTraining Loss: 2.288 \tTraining Accuracy: 0.130\n",
      "Epoch: 1397 \tTraining Loss: 2.288 \tTraining Accuracy: 0.130\n",
      "Epoch: 1398 \tTraining Loss: 2.288 \tTraining Accuracy: 0.130\n",
      "Epoch: 1399 \tTraining Loss: 2.288 \tTraining Accuracy: 0.130\n",
      "Epoch: 1400 \tTraining Loss: 2.288 \tTraining Accuracy: 0.130\n",
      "Epoch: 1401 \tTraining Loss: 2.288 \tTraining Accuracy: 0.130\n",
      "Epoch: 1402 \tTraining Loss: 2.288 \tTraining Accuracy: 0.130\n",
      "Epoch: 1403 \tTraining Loss: 2.288 \tTraining Accuracy: 0.130\n",
      "Epoch: 1404 \tTraining Loss: 2.288 \tTraining Accuracy: 0.130\n",
      "Epoch: 1405 \tTraining Loss: 2.288 \tTraining Accuracy: 0.130\n",
      "Epoch: 1406 \tTraining Loss: 2.288 \tTraining Accuracy: 0.130\n",
      "Epoch: 1407 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1408 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1409 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1410 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1411 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1412 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1413 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1414 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1415 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1416 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1417 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1418 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1419 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1420 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1421 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1422 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1423 \tTraining Loss: 2.287 \tTraining Accuracy: 0.130\n",
      "Epoch: 1424 \tTraining Loss: 2.287 \tTraining Accuracy: 0.131\n",
      "Epoch: 1425 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1426 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1427 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1428 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1429 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1430 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1431 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1432 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1433 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1434 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1435 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1436 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1437 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1438 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1439 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1440 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1441 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1442 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1443 \tTraining Loss: 2.286 \tTraining Accuracy: 0.131\n",
      "Epoch: 1444 \tTraining Loss: 2.285 \tTraining Accuracy: 0.131\n",
      "Epoch: 1445 \tTraining Loss: 2.285 \tTraining Accuracy: 0.131\n",
      "Epoch: 1446 \tTraining Loss: 2.285 \tTraining Accuracy: 0.131\n",
      "Epoch: 1447 \tTraining Loss: 2.285 \tTraining Accuracy: 0.131\n",
      "Epoch: 1448 \tTraining Loss: 2.285 \tTraining Accuracy: 0.131\n",
      "Epoch: 1449 \tTraining Loss: 2.285 \tTraining Accuracy: 0.131\n",
      "Epoch: 1450 \tTraining Loss: 2.285 \tTraining Accuracy: 0.131\n",
      "Epoch: 1451 \tTraining Loss: 2.285 \tTraining Accuracy: 0.131\n",
      "Epoch: 1452 \tTraining Loss: 2.285 \tTraining Accuracy: 0.132\n",
      "Epoch: 1453 \tTraining Loss: 2.285 \tTraining Accuracy: 0.132\n",
      "Epoch: 1454 \tTraining Loss: 2.285 \tTraining Accuracy: 0.132\n",
      "Epoch: 1455 \tTraining Loss: 2.285 \tTraining Accuracy: 0.132\n",
      "Epoch: 1456 \tTraining Loss: 2.285 \tTraining Accuracy: 0.132\n",
      "Epoch: 1457 \tTraining Loss: 2.285 \tTraining Accuracy: 0.132\n",
      "Epoch: 1458 \tTraining Loss: 2.285 \tTraining Accuracy: 0.132\n",
      "Epoch: 1459 \tTraining Loss: 2.285 \tTraining Accuracy: 0.132\n",
      "Epoch: 1460 \tTraining Loss: 2.285 \tTraining Accuracy: 0.132\n",
      "Epoch: 1461 \tTraining Loss: 2.285 \tTraining Accuracy: 0.132\n",
      "Epoch: 1462 \tTraining Loss: 2.285 \tTraining Accuracy: 0.132\n",
      "Epoch: 1463 \tTraining Loss: 2.284 \tTraining Accuracy: 0.132\n",
      "Epoch: 1464 \tTraining Loss: 2.284 \tTraining Accuracy: 0.132\n",
      "Epoch: 1465 \tTraining Loss: 2.284 \tTraining Accuracy: 0.132\n",
      "Epoch: 1466 \tTraining Loss: 2.284 \tTraining Accuracy: 0.132\n",
      "Epoch: 1467 \tTraining Loss: 2.284 \tTraining Accuracy: 0.132\n",
      "Epoch: 1468 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1469 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1470 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1471 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1472 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1473 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1474 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1475 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1476 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1477 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1478 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1479 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1480 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1481 \tTraining Loss: 2.284 \tTraining Accuracy: 0.133\n",
      "Epoch: 1482 \tTraining Loss: 2.283 \tTraining Accuracy: 0.133\n",
      "Epoch: 1483 \tTraining Loss: 2.283 \tTraining Accuracy: 0.133\n",
      "Epoch: 1484 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1485 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1486 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1487 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1488 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1489 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1490 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1491 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1492 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1493 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1494 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1495 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1496 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1497 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1498 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1499 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1500 \tTraining Loss: 2.283 \tTraining Accuracy: 0.134\n",
      "Epoch: 1501 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1502 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1503 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1504 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1505 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1506 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1507 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1508 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1509 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1510 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1511 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1512 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1513 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1514 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1515 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1516 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1517 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1518 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1519 \tTraining Loss: 2.282 \tTraining Accuracy: 0.134\n",
      "Epoch: 1520 \tTraining Loss: 2.281 \tTraining Accuracy: 0.134\n",
      "Epoch: 1521 \tTraining Loss: 2.281 \tTraining Accuracy: 0.134\n",
      "Epoch: 1522 \tTraining Loss: 2.281 \tTraining Accuracy: 0.134\n",
      "Epoch: 1523 \tTraining Loss: 2.281 \tTraining Accuracy: 0.134\n",
      "Epoch: 1524 \tTraining Loss: 2.281 \tTraining Accuracy: 0.134\n",
      "Epoch: 1525 \tTraining Loss: 2.281 \tTraining Accuracy: 0.134\n",
      "Epoch: 1526 \tTraining Loss: 2.281 \tTraining Accuracy: 0.134\n",
      "Epoch: 1527 \tTraining Loss: 2.281 \tTraining Accuracy: 0.134\n",
      "Epoch: 1528 \tTraining Loss: 2.281 \tTraining Accuracy: 0.135\n",
      "Epoch: 1529 \tTraining Loss: 2.281 \tTraining Accuracy: 0.135\n",
      "Epoch: 1530 \tTraining Loss: 2.281 \tTraining Accuracy: 0.135\n",
      "Epoch: 1531 \tTraining Loss: 2.281 \tTraining Accuracy: 0.135\n",
      "Epoch: 1532 \tTraining Loss: 2.281 \tTraining Accuracy: 0.135\n",
      "Epoch: 1533 \tTraining Loss: 2.281 \tTraining Accuracy: 0.135\n",
      "Epoch: 1534 \tTraining Loss: 2.281 \tTraining Accuracy: 0.135\n",
      "Epoch: 1535 \tTraining Loss: 2.281 \tTraining Accuracy: 0.135\n",
      "Epoch: 1536 \tTraining Loss: 2.281 \tTraining Accuracy: 0.135\n",
      "Epoch: 1537 \tTraining Loss: 2.281 \tTraining Accuracy: 0.135\n",
      "Epoch: 1538 \tTraining Loss: 2.281 \tTraining Accuracy: 0.135\n",
      "Epoch: 1539 \tTraining Loss: 2.280 \tTraining Accuracy: 0.135\n",
      "Epoch: 1540 \tTraining Loss: 2.280 \tTraining Accuracy: 0.135\n",
      "Epoch: 1541 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1542 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1543 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1544 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1545 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1546 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1547 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1548 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1549 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1550 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1551 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1552 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1553 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1554 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1555 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1556 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1557 \tTraining Loss: 2.280 \tTraining Accuracy: 0.136\n",
      "Epoch: 1558 \tTraining Loss: 2.279 \tTraining Accuracy: 0.136\n",
      "Epoch: 1559 \tTraining Loss: 2.279 \tTraining Accuracy: 0.136\n",
      "Epoch: 1560 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1561 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1562 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1563 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1564 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1565 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1566 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1567 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1568 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1569 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1570 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1571 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1572 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1573 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1574 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1575 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1576 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1577 \tTraining Loss: 2.279 \tTraining Accuracy: 0.137\n",
      "Epoch: 1578 \tTraining Loss: 2.278 \tTraining Accuracy: 0.137\n",
      "Epoch: 1579 \tTraining Loss: 2.278 \tTraining Accuracy: 0.137\n",
      "Epoch: 1580 \tTraining Loss: 2.278 \tTraining Accuracy: 0.137\n",
      "Epoch: 1581 \tTraining Loss: 2.278 \tTraining Accuracy: 0.137\n",
      "Epoch: 1582 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1583 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1584 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1585 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1586 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1587 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1588 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1589 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1590 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1591 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1592 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1593 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1594 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1595 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1596 \tTraining Loss: 2.278 \tTraining Accuracy: 0.138\n",
      "Epoch: 1597 \tTraining Loss: 2.277 \tTraining Accuracy: 0.138\n",
      "Epoch: 1598 \tTraining Loss: 2.277 \tTraining Accuracy: 0.138\n",
      "Epoch: 1599 \tTraining Loss: 2.277 \tTraining Accuracy: 0.138\n",
      "Epoch: 1600 \tTraining Loss: 2.277 \tTraining Accuracy: 0.138\n",
      "Epoch: 1601 \tTraining Loss: 2.277 \tTraining Accuracy: 0.138\n",
      "Epoch: 1602 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1603 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1604 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1605 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1606 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1607 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1608 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1609 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1610 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1611 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1612 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1613 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1614 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1615 \tTraining Loss: 2.277 \tTraining Accuracy: 0.139\n",
      "Epoch: 1616 \tTraining Loss: 2.276 \tTraining Accuracy: 0.139\n",
      "Epoch: 1617 \tTraining Loss: 2.276 \tTraining Accuracy: 0.139\n",
      "Epoch: 1618 \tTraining Loss: 2.276 \tTraining Accuracy: 0.139\n",
      "Epoch: 1619 \tTraining Loss: 2.276 \tTraining Accuracy: 0.139\n",
      "Epoch: 1620 \tTraining Loss: 2.276 \tTraining Accuracy: 0.139\n",
      "Epoch: 1621 \tTraining Loss: 2.276 \tTraining Accuracy: 0.139\n",
      "Epoch: 1622 \tTraining Loss: 2.276 \tTraining Accuracy: 0.139\n",
      "Epoch: 1623 \tTraining Loss: 2.276 \tTraining Accuracy: 0.139\n",
      "Epoch: 1624 \tTraining Loss: 2.276 \tTraining Accuracy: 0.140\n",
      "Epoch: 1625 \tTraining Loss: 2.276 \tTraining Accuracy: 0.140\n",
      "Epoch: 1626 \tTraining Loss: 2.276 \tTraining Accuracy: 0.140\n",
      "Epoch: 1627 \tTraining Loss: 2.276 \tTraining Accuracy: 0.140\n",
      "Epoch: 1628 \tTraining Loss: 2.276 \tTraining Accuracy: 0.140\n",
      "Epoch: 1629 \tTraining Loss: 2.276 \tTraining Accuracy: 0.140\n",
      "Epoch: 1630 \tTraining Loss: 2.276 \tTraining Accuracy: 0.140\n",
      "Epoch: 1631 \tTraining Loss: 2.276 \tTraining Accuracy: 0.140\n",
      "Epoch: 1632 \tTraining Loss: 2.276 \tTraining Accuracy: 0.140\n",
      "Epoch: 1633 \tTraining Loss: 2.276 \tTraining Accuracy: 0.140\n",
      "Epoch: 1634 \tTraining Loss: 2.276 \tTraining Accuracy: 0.140\n",
      "Epoch: 1635 \tTraining Loss: 2.276 \tTraining Accuracy: 0.140\n",
      "Epoch: 1636 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1637 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1638 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1639 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1640 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1641 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1642 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1643 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1644 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1645 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1646 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1647 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1648 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1649 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1650 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1651 \tTraining Loss: 2.275 \tTraining Accuracy: 0.140\n",
      "Epoch: 1652 \tTraining Loss: 2.275 \tTraining Accuracy: 0.141\n",
      "Epoch: 1653 \tTraining Loss: 2.275 \tTraining Accuracy: 0.141\n",
      "Epoch: 1654 \tTraining Loss: 2.275 \tTraining Accuracy: 0.141\n",
      "Epoch: 1655 \tTraining Loss: 2.275 \tTraining Accuracy: 0.141\n",
      "Epoch: 1656 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1657 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1658 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1659 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1660 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1661 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1662 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1663 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1664 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1665 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1666 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1667 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1668 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1669 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1670 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1671 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1672 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1673 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1674 \tTraining Loss: 2.274 \tTraining Accuracy: 0.141\n",
      "Epoch: 1675 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1676 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1677 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1678 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1679 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1680 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1681 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1682 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1683 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1684 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1685 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1686 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1687 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1688 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1689 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1690 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1691 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1692 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1693 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1694 \tTraining Loss: 2.273 \tTraining Accuracy: 0.141\n",
      "Epoch: 1695 \tTraining Loss: 2.272 \tTraining Accuracy: 0.141\n",
      "Epoch: 1696 \tTraining Loss: 2.272 \tTraining Accuracy: 0.141\n",
      "Epoch: 1697 \tTraining Loss: 2.272 \tTraining Accuracy: 0.141\n",
      "Epoch: 1698 \tTraining Loss: 2.272 \tTraining Accuracy: 0.141\n",
      "Epoch: 1699 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1700 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1701 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1702 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1703 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1704 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1705 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1706 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1707 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1708 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1709 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1710 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1711 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1712 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1713 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1714 \tTraining Loss: 2.272 \tTraining Accuracy: 0.142\n",
      "Epoch: 1715 \tTraining Loss: 2.271 \tTraining Accuracy: 0.142\n",
      "Epoch: 1716 \tTraining Loss: 2.271 \tTraining Accuracy: 0.142\n",
      "Epoch: 1717 \tTraining Loss: 2.271 \tTraining Accuracy: 0.142\n",
      "Epoch: 1718 \tTraining Loss: 2.271 \tTraining Accuracy: 0.142\n",
      "Epoch: 1719 \tTraining Loss: 2.271 \tTraining Accuracy: 0.142\n",
      "Epoch: 1720 \tTraining Loss: 2.271 \tTraining Accuracy: 0.142\n",
      "Epoch: 1721 \tTraining Loss: 2.271 \tTraining Accuracy: 0.142\n",
      "Epoch: 1722 \tTraining Loss: 2.271 \tTraining Accuracy: 0.142\n",
      "Epoch: 1723 \tTraining Loss: 2.271 \tTraining Accuracy: 0.142\n",
      "Epoch: 1724 \tTraining Loss: 2.271 \tTraining Accuracy: 0.142\n",
      "Epoch: 1725 \tTraining Loss: 2.271 \tTraining Accuracy: 0.142\n",
      "Epoch: 1726 \tTraining Loss: 2.271 \tTraining Accuracy: 0.142\n",
      "Epoch: 1727 \tTraining Loss: 2.271 \tTraining Accuracy: 0.143\n",
      "Epoch: 1728 \tTraining Loss: 2.271 \tTraining Accuracy: 0.143\n",
      "Epoch: 1729 \tTraining Loss: 2.271 \tTraining Accuracy: 0.143\n",
      "Epoch: 1730 \tTraining Loss: 2.271 \tTraining Accuracy: 0.143\n",
      "Epoch: 1731 \tTraining Loss: 2.271 \tTraining Accuracy: 0.143\n",
      "Epoch: 1732 \tTraining Loss: 2.271 \tTraining Accuracy: 0.143\n",
      "Epoch: 1733 \tTraining Loss: 2.271 \tTraining Accuracy: 0.143\n",
      "Epoch: 1734 \tTraining Loss: 2.271 \tTraining Accuracy: 0.143\n",
      "Epoch: 1735 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1736 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1737 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1738 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1739 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1740 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1741 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1742 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1743 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1744 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1745 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1746 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1747 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1748 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1749 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1750 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1751 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1752 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1753 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1754 \tTraining Loss: 2.270 \tTraining Accuracy: 0.143\n",
      "Epoch: 1755 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1756 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1757 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1758 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1759 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1760 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1761 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1762 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1763 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1764 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1765 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1766 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1767 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1768 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1769 \tTraining Loss: 2.269 \tTraining Accuracy: 0.143\n",
      "Epoch: 1770 \tTraining Loss: 2.269 \tTraining Accuracy: 0.144\n",
      "Epoch: 1771 \tTraining Loss: 2.269 \tTraining Accuracy: 0.144\n",
      "Epoch: 1772 \tTraining Loss: 2.269 \tTraining Accuracy: 0.144\n",
      "Epoch: 1773 \tTraining Loss: 2.269 \tTraining Accuracy: 0.144\n",
      "Epoch: 1774 \tTraining Loss: 2.269 \tTraining Accuracy: 0.144\n",
      "Epoch: 1775 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1776 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1777 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1778 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1779 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1780 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1781 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1782 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1783 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1784 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1785 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1786 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1787 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1788 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1789 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1790 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1791 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1792 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1793 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1794 \tTraining Loss: 2.268 \tTraining Accuracy: 0.144\n",
      "Epoch: 1795 \tTraining Loss: 2.267 \tTraining Accuracy: 0.144\n",
      "Epoch: 1796 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1797 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1798 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1799 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1800 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1801 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1802 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1803 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1804 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1805 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1806 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1807 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1808 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1809 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1810 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1811 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1812 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1813 \tTraining Loss: 2.267 \tTraining Accuracy: 0.145\n",
      "Epoch: 1814 \tTraining Loss: 2.267 \tTraining Accuracy: 0.146\n",
      "Epoch: 1815 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1816 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1817 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1818 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1819 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1820 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1821 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1822 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1823 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1824 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1825 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1826 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1827 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1828 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1829 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1830 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1831 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1832 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1833 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1834 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1835 \tTraining Loss: 2.266 \tTraining Accuracy: 0.146\n",
      "Epoch: 1836 \tTraining Loss: 2.265 \tTraining Accuracy: 0.146\n",
      "Epoch: 1837 \tTraining Loss: 2.265 \tTraining Accuracy: 0.146\n",
      "Epoch: 1838 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1839 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1840 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1841 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1842 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1843 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1844 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1845 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1846 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1847 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1848 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1849 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1850 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1851 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1852 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1853 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1854 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1855 \tTraining Loss: 2.265 \tTraining Accuracy: 0.147\n",
      "Epoch: 1856 \tTraining Loss: 2.264 \tTraining Accuracy: 0.147\n",
      "Epoch: 1857 \tTraining Loss: 2.264 \tTraining Accuracy: 0.147\n",
      "Epoch: 1858 \tTraining Loss: 2.264 \tTraining Accuracy: 0.147\n",
      "Epoch: 1859 \tTraining Loss: 2.264 \tTraining Accuracy: 0.147\n",
      "Epoch: 1860 \tTraining Loss: 2.264 \tTraining Accuracy: 0.147\n",
      "Epoch: 1861 \tTraining Loss: 2.264 \tTraining Accuracy: 0.147\n",
      "Epoch: 1862 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1863 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1864 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1865 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1866 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1867 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1868 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1869 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1870 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1871 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1872 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1873 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1874 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1875 \tTraining Loss: 2.264 \tTraining Accuracy: 0.148\n",
      "Epoch: 1876 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1877 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1878 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1879 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1880 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1881 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1882 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1883 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1884 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1885 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1886 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1887 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1888 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1889 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1890 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1891 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1892 \tTraining Loss: 2.263 \tTraining Accuracy: 0.148\n",
      "Epoch: 1893 \tTraining Loss: 2.263 \tTraining Accuracy: 0.149\n",
      "Epoch: 1894 \tTraining Loss: 2.263 \tTraining Accuracy: 0.149\n",
      "Epoch: 1895 \tTraining Loss: 2.263 \tTraining Accuracy: 0.149\n",
      "Epoch: 1896 \tTraining Loss: 2.263 \tTraining Accuracy: 0.149\n",
      "Epoch: 1897 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1898 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1899 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1900 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1901 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1902 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1903 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1904 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1905 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1906 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1907 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1908 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1909 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1910 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1911 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1912 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1913 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1914 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1915 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1916 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1917 \tTraining Loss: 2.262 \tTraining Accuracy: 0.149\n",
      "Epoch: 1918 \tTraining Loss: 2.261 \tTraining Accuracy: 0.149\n",
      "Epoch: 1919 \tTraining Loss: 2.261 \tTraining Accuracy: 0.149\n",
      "Epoch: 1920 \tTraining Loss: 2.261 \tTraining Accuracy: 0.149\n",
      "Epoch: 1921 \tTraining Loss: 2.261 \tTraining Accuracy: 0.149\n",
      "Epoch: 1922 \tTraining Loss: 2.261 \tTraining Accuracy: 0.149\n",
      "Epoch: 1923 \tTraining Loss: 2.261 \tTraining Accuracy: 0.149\n",
      "Epoch: 1924 \tTraining Loss: 2.261 \tTraining Accuracy: 0.149\n",
      "Epoch: 1925 \tTraining Loss: 2.261 \tTraining Accuracy: 0.150\n",
      "Epoch: 1926 \tTraining Loss: 2.261 \tTraining Accuracy: 0.150\n",
      "Epoch: 1927 \tTraining Loss: 2.261 \tTraining Accuracy: 0.150\n",
      "Epoch: 1928 \tTraining Loss: 2.261 \tTraining Accuracy: 0.150\n",
      "Epoch: 1929 \tTraining Loss: 2.261 \tTraining Accuracy: 0.150\n",
      "Epoch: 1930 \tTraining Loss: 2.261 \tTraining Accuracy: 0.150\n",
      "Epoch: 1931 \tTraining Loss: 2.261 \tTraining Accuracy: 0.150\n",
      "Epoch: 1932 \tTraining Loss: 2.261 \tTraining Accuracy: 0.150\n",
      "Epoch: 1933 \tTraining Loss: 2.261 \tTraining Accuracy: 0.150\n",
      "Epoch: 1934 \tTraining Loss: 2.261 \tTraining Accuracy: 0.150\n",
      "Epoch: 1935 \tTraining Loss: 2.261 \tTraining Accuracy: 0.150\n",
      "Epoch: 1936 \tTraining Loss: 2.261 \tTraining Accuracy: 0.150\n",
      "Epoch: 1937 \tTraining Loss: 2.261 \tTraining Accuracy: 0.150\n",
      "Epoch: 1938 \tTraining Loss: 2.260 \tTraining Accuracy: 0.150\n",
      "Epoch: 1939 \tTraining Loss: 2.260 \tTraining Accuracy: 0.150\n",
      "Epoch: 1940 \tTraining Loss: 2.260 \tTraining Accuracy: 0.150\n",
      "Epoch: 1941 \tTraining Loss: 2.260 \tTraining Accuracy: 0.150\n",
      "Epoch: 1942 \tTraining Loss: 2.260 \tTraining Accuracy: 0.150\n",
      "Epoch: 1943 \tTraining Loss: 2.260 \tTraining Accuracy: 0.150\n",
      "Epoch: 1944 \tTraining Loss: 2.260 \tTraining Accuracy: 0.150\n",
      "Epoch: 1945 \tTraining Loss: 2.260 \tTraining Accuracy: 0.150\n",
      "Epoch: 1946 \tTraining Loss: 2.260 \tTraining Accuracy: 0.150\n",
      "Epoch: 1947 \tTraining Loss: 2.260 \tTraining Accuracy: 0.150\n",
      "Epoch: 1948 \tTraining Loss: 2.260 \tTraining Accuracy: 0.151\n",
      "Epoch: 1949 \tTraining Loss: 2.260 \tTraining Accuracy: 0.151\n",
      "Epoch: 1950 \tTraining Loss: 2.260 \tTraining Accuracy: 0.151\n",
      "Epoch: 1951 \tTraining Loss: 2.260 \tTraining Accuracy: 0.151\n",
      "Epoch: 1952 \tTraining Loss: 2.260 \tTraining Accuracy: 0.151\n",
      "Epoch: 1953 \tTraining Loss: 2.260 \tTraining Accuracy: 0.151\n",
      "Epoch: 1954 \tTraining Loss: 2.260 \tTraining Accuracy: 0.151\n",
      "Epoch: 1955 \tTraining Loss: 2.260 \tTraining Accuracy: 0.151\n",
      "Epoch: 1956 \tTraining Loss: 2.260 \tTraining Accuracy: 0.151\n",
      "Epoch: 1957 \tTraining Loss: 2.260 \tTraining Accuracy: 0.151\n",
      "Epoch: 1958 \tTraining Loss: 2.260 \tTraining Accuracy: 0.151\n",
      "Epoch: 1959 \tTraining Loss: 2.259 \tTraining Accuracy: 0.151\n",
      "Epoch: 1960 \tTraining Loss: 2.259 \tTraining Accuracy: 0.151\n",
      "Epoch: 1961 \tTraining Loss: 2.259 \tTraining Accuracy: 0.151\n",
      "Epoch: 1962 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1963 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1964 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1965 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1966 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1967 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1968 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1969 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1970 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1971 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1972 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1973 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1974 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1975 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1976 \tTraining Loss: 2.259 \tTraining Accuracy: 0.152\n",
      "Epoch: 1977 \tTraining Loss: 2.259 \tTraining Accuracy: 0.153\n",
      "Epoch: 1978 \tTraining Loss: 2.259 \tTraining Accuracy: 0.153\n",
      "Epoch: 1979 \tTraining Loss: 2.259 \tTraining Accuracy: 0.153\n",
      "Epoch: 1980 \tTraining Loss: 2.258 \tTraining Accuracy: 0.153\n",
      "Epoch: 1981 \tTraining Loss: 2.258 \tTraining Accuracy: 0.153\n",
      "Epoch: 1982 \tTraining Loss: 2.258 \tTraining Accuracy: 0.153\n",
      "Epoch: 1983 \tTraining Loss: 2.258 \tTraining Accuracy: 0.153\n",
      "Epoch: 1984 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1985 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1986 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1987 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1988 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1989 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1990 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1991 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1992 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1993 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1994 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1995 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1996 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1997 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1998 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 1999 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 2000 \tTraining Loss: 2.258 \tTraining Accuracy: 0.154\n",
      "Epoch: 2001 \tTraining Loss: 2.257 \tTraining Accuracy: 0.154\n",
      "Epoch: 2002 \tTraining Loss: 2.257 \tTraining Accuracy: 0.154\n",
      "Epoch: 2003 \tTraining Loss: 2.257 \tTraining Accuracy: 0.154\n",
      "Epoch: 2004 \tTraining Loss: 2.257 \tTraining Accuracy: 0.154\n",
      "Epoch: 2005 \tTraining Loss: 2.257 \tTraining Accuracy: 0.154\n",
      "Epoch: 2006 \tTraining Loss: 2.257 \tTraining Accuracy: 0.154\n",
      "Epoch: 2007 \tTraining Loss: 2.257 \tTraining Accuracy: 0.154\n",
      "Epoch: 2008 \tTraining Loss: 2.257 \tTraining Accuracy: 0.154\n",
      "Epoch: 2009 \tTraining Loss: 2.257 \tTraining Accuracy: 0.154\n",
      "Epoch: 2010 \tTraining Loss: 2.257 \tTraining Accuracy: 0.154\n",
      "Epoch: 2011 \tTraining Loss: 2.257 \tTraining Accuracy: 0.155\n",
      "Epoch: 2012 \tTraining Loss: 2.257 \tTraining Accuracy: 0.155\n",
      "Epoch: 2013 \tTraining Loss: 2.257 \tTraining Accuracy: 0.155\n",
      "Epoch: 2014 \tTraining Loss: 2.257 \tTraining Accuracy: 0.155\n",
      "Epoch: 2015 \tTraining Loss: 2.257 \tTraining Accuracy: 0.155\n",
      "Epoch: 2016 \tTraining Loss: 2.257 \tTraining Accuracy: 0.155\n",
      "Epoch: 2017 \tTraining Loss: 2.257 \tTraining Accuracy: 0.155\n",
      "Epoch: 2018 \tTraining Loss: 2.257 \tTraining Accuracy: 0.155\n",
      "Epoch: 2019 \tTraining Loss: 2.257 \tTraining Accuracy: 0.155\n",
      "Epoch: 2020 \tTraining Loss: 2.257 \tTraining Accuracy: 0.155\n",
      "Epoch: 2021 \tTraining Loss: 2.257 \tTraining Accuracy: 0.155\n",
      "Epoch: 2022 \tTraining Loss: 2.256 \tTraining Accuracy: 0.155\n",
      "Epoch: 2023 \tTraining Loss: 2.256 \tTraining Accuracy: 0.155\n",
      "Epoch: 2024 \tTraining Loss: 2.256 \tTraining Accuracy: 0.155\n",
      "Epoch: 2025 \tTraining Loss: 2.256 \tTraining Accuracy: 0.155\n",
      "Epoch: 2026 \tTraining Loss: 2.256 \tTraining Accuracy: 0.155\n",
      "Epoch: 2027 \tTraining Loss: 2.256 \tTraining Accuracy: 0.155\n",
      "Epoch: 2028 \tTraining Loss: 2.256 \tTraining Accuracy: 0.155\n",
      "Epoch: 2029 \tTraining Loss: 2.256 \tTraining Accuracy: 0.155\n",
      "Epoch: 2030 \tTraining Loss: 2.256 \tTraining Accuracy: 0.155\n",
      "Epoch: 2031 \tTraining Loss: 2.256 \tTraining Accuracy: 0.155\n",
      "Epoch: 2032 \tTraining Loss: 2.256 \tTraining Accuracy: 0.156\n",
      "Epoch: 2033 \tTraining Loss: 2.256 \tTraining Accuracy: 0.156\n",
      "Epoch: 2034 \tTraining Loss: 2.256 \tTraining Accuracy: 0.156\n",
      "Epoch: 2035 \tTraining Loss: 2.256 \tTraining Accuracy: 0.156\n",
      "Epoch: 2036 \tTraining Loss: 2.256 \tTraining Accuracy: 0.156\n",
      "Epoch: 2037 \tTraining Loss: 2.256 \tTraining Accuracy: 0.156\n",
      "Epoch: 2038 \tTraining Loss: 2.256 \tTraining Accuracy: 0.156\n",
      "Epoch: 2039 \tTraining Loss: 2.256 \tTraining Accuracy: 0.156\n",
      "Epoch: 2040 \tTraining Loss: 2.256 \tTraining Accuracy: 0.156\n",
      "Epoch: 2041 \tTraining Loss: 2.256 \tTraining Accuracy: 0.156\n",
      "Epoch: 2042 \tTraining Loss: 2.256 \tTraining Accuracy: 0.156\n",
      "Epoch: 2043 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2044 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2045 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2046 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2047 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2048 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2049 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2050 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2051 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2052 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2053 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2054 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2055 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2056 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2057 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2058 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2059 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2060 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2061 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2062 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2063 \tTraining Loss: 2.255 \tTraining Accuracy: 0.156\n",
      "Epoch: 2064 \tTraining Loss: 2.254 \tTraining Accuracy: 0.156\n",
      "Epoch: 2065 \tTraining Loss: 2.254 \tTraining Accuracy: 0.156\n",
      "Epoch: 2066 \tTraining Loss: 2.254 \tTraining Accuracy: 0.156\n",
      "Epoch: 2067 \tTraining Loss: 2.254 \tTraining Accuracy: 0.156\n",
      "Epoch: 2068 \tTraining Loss: 2.254 \tTraining Accuracy: 0.156\n",
      "Epoch: 2069 \tTraining Loss: 2.254 \tTraining Accuracy: 0.156\n",
      "Epoch: 2070 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2071 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2072 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2073 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2074 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2075 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2076 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2077 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2078 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2079 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2080 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2081 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2082 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2083 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2084 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2085 \tTraining Loss: 2.254 \tTraining Accuracy: 0.157\n",
      "Epoch: 2086 \tTraining Loss: 2.253 \tTraining Accuracy: 0.157\n",
      "Epoch: 2087 \tTraining Loss: 2.253 \tTraining Accuracy: 0.157\n",
      "Epoch: 2088 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2089 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2090 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2091 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2092 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2093 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2094 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2095 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2096 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2097 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2098 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2099 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2100 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2101 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2102 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2103 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2104 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2105 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2106 \tTraining Loss: 2.253 \tTraining Accuracy: 0.158\n",
      "Epoch: 2107 \tTraining Loss: 2.252 \tTraining Accuracy: 0.158\n",
      "Epoch: 2108 \tTraining Loss: 2.252 \tTraining Accuracy: 0.158\n",
      "Epoch: 2109 \tTraining Loss: 2.252 \tTraining Accuracy: 0.158\n",
      "Epoch: 2110 \tTraining Loss: 2.252 \tTraining Accuracy: 0.158\n",
      "Epoch: 2111 \tTraining Loss: 2.252 \tTraining Accuracy: 0.158\n",
      "Epoch: 2112 \tTraining Loss: 2.252 \tTraining Accuracy: 0.158\n",
      "Epoch: 2113 \tTraining Loss: 2.252 \tTraining Accuracy: 0.158\n",
      "Epoch: 2114 \tTraining Loss: 2.252 \tTraining Accuracy: 0.158\n",
      "Epoch: 2115 \tTraining Loss: 2.252 \tTraining Accuracy: 0.158\n",
      "Epoch: 2116 \tTraining Loss: 2.252 \tTraining Accuracy: 0.158\n",
      "Epoch: 2117 \tTraining Loss: 2.252 \tTraining Accuracy: 0.158\n",
      "Epoch: 2118 \tTraining Loss: 2.252 \tTraining Accuracy: 0.158\n",
      "Epoch: 2119 \tTraining Loss: 2.252 \tTraining Accuracy: 0.159\n",
      "Epoch: 2120 \tTraining Loss: 2.252 \tTraining Accuracy: 0.159\n",
      "Epoch: 2121 \tTraining Loss: 2.252 \tTraining Accuracy: 0.159\n",
      "Epoch: 2122 \tTraining Loss: 2.252 \tTraining Accuracy: 0.159\n",
      "Epoch: 2123 \tTraining Loss: 2.252 \tTraining Accuracy: 0.159\n",
      "Epoch: 2124 \tTraining Loss: 2.252 \tTraining Accuracy: 0.159\n",
      "Epoch: 2125 \tTraining Loss: 2.252 \tTraining Accuracy: 0.159\n",
      "Epoch: 2126 \tTraining Loss: 2.252 \tTraining Accuracy: 0.159\n",
      "Epoch: 2127 \tTraining Loss: 2.252 \tTraining Accuracy: 0.159\n",
      "Epoch: 2128 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2129 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2130 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2131 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2132 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2133 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2134 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2135 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2136 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2137 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2138 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2139 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2140 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2141 \tTraining Loss: 2.251 \tTraining Accuracy: 0.159\n",
      "Epoch: 2142 \tTraining Loss: 2.251 \tTraining Accuracy: 0.160\n",
      "Epoch: 2143 \tTraining Loss: 2.251 \tTraining Accuracy: 0.160\n",
      "Epoch: 2144 \tTraining Loss: 2.251 \tTraining Accuracy: 0.160\n",
      "Epoch: 2145 \tTraining Loss: 2.251 \tTraining Accuracy: 0.160\n",
      "Epoch: 2146 \tTraining Loss: 2.251 \tTraining Accuracy: 0.160\n",
      "Epoch: 2147 \tTraining Loss: 2.251 \tTraining Accuracy: 0.160\n",
      "Epoch: 2148 \tTraining Loss: 2.251 \tTraining Accuracy: 0.160\n",
      "Epoch: 2149 \tTraining Loss: 2.251 \tTraining Accuracy: 0.160\n",
      "Epoch: 2150 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2151 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2152 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2153 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2154 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2155 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2156 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2157 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2158 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2159 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2160 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2161 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2162 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2163 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2164 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2165 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2166 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2167 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2168 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2169 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2170 \tTraining Loss: 2.250 \tTraining Accuracy: 0.160\n",
      "Epoch: 2171 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2172 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2173 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2174 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2175 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2176 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2177 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2178 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2179 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2180 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2181 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2182 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2183 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2184 \tTraining Loss: 2.249 \tTraining Accuracy: 0.160\n",
      "Epoch: 2185 \tTraining Loss: 2.249 \tTraining Accuracy: 0.161\n",
      "Epoch: 2186 \tTraining Loss: 2.249 \tTraining Accuracy: 0.161\n",
      "Epoch: 2187 \tTraining Loss: 2.249 \tTraining Accuracy: 0.161\n",
      "Epoch: 2188 \tTraining Loss: 2.249 \tTraining Accuracy: 0.161\n",
      "Epoch: 2189 \tTraining Loss: 2.249 \tTraining Accuracy: 0.161\n",
      "Epoch: 2190 \tTraining Loss: 2.249 \tTraining Accuracy: 0.161\n",
      "Epoch: 2191 \tTraining Loss: 2.249 \tTraining Accuracy: 0.161\n",
      "Epoch: 2192 \tTraining Loss: 2.249 \tTraining Accuracy: 0.161\n",
      "Epoch: 2193 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2194 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2195 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2196 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2197 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2198 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2199 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2200 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2201 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2202 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2203 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2204 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2205 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2206 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2207 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2208 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2209 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2210 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2211 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2212 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2213 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2214 \tTraining Loss: 2.248 \tTraining Accuracy: 0.161\n",
      "Epoch: 2215 \tTraining Loss: 2.247 \tTraining Accuracy: 0.161\n",
      "Epoch: 2216 \tTraining Loss: 2.247 \tTraining Accuracy: 0.161\n",
      "Epoch: 2217 \tTraining Loss: 2.247 \tTraining Accuracy: 0.161\n",
      "Epoch: 2218 \tTraining Loss: 2.247 \tTraining Accuracy: 0.161\n",
      "Epoch: 2219 \tTraining Loss: 2.247 \tTraining Accuracy: 0.161\n",
      "Epoch: 2220 \tTraining Loss: 2.247 \tTraining Accuracy: 0.161\n",
      "Epoch: 2221 \tTraining Loss: 2.247 \tTraining Accuracy: 0.161\n",
      "Epoch: 2222 \tTraining Loss: 2.247 \tTraining Accuracy: 0.161\n",
      "Epoch: 2223 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2224 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2225 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2226 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2227 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2228 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2229 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2230 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2231 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2232 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2233 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2234 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2235 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2236 \tTraining Loss: 2.247 \tTraining Accuracy: 0.162\n",
      "Epoch: 2237 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2238 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2239 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2240 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2241 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2242 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2243 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2244 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2245 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2246 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2247 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2248 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2249 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2250 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2251 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2252 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2253 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2254 \tTraining Loss: 2.246 \tTraining Accuracy: 0.162\n",
      "Epoch: 2255 \tTraining Loss: 2.246 \tTraining Accuracy: 0.163\n",
      "Epoch: 2256 \tTraining Loss: 2.246 \tTraining Accuracy: 0.163\n",
      "Epoch: 2257 \tTraining Loss: 2.246 \tTraining Accuracy: 0.163\n",
      "Epoch: 2258 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2259 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2260 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2261 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2262 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2263 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2264 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2265 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2266 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2267 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2268 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2269 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2270 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2271 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2272 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2273 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2274 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2275 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2276 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2277 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2278 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2279 \tTraining Loss: 2.245 \tTraining Accuracy: 0.163\n",
      "Epoch: 2280 \tTraining Loss: 2.244 \tTraining Accuracy: 0.163\n",
      "Epoch: 2281 \tTraining Loss: 2.244 \tTraining Accuracy: 0.163\n",
      "Epoch: 2282 \tTraining Loss: 2.244 \tTraining Accuracy: 0.163\n",
      "Epoch: 2283 \tTraining Loss: 2.244 \tTraining Accuracy: 0.163\n",
      "Epoch: 2284 \tTraining Loss: 2.244 \tTraining Accuracy: 0.163\n",
      "Epoch: 2285 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2286 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2287 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2288 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2289 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2290 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2291 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2292 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2293 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2294 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2295 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2296 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2297 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2298 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2299 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2300 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2301 \tTraining Loss: 2.244 \tTraining Accuracy: 0.164\n",
      "Epoch: 2302 \tTraining Loss: 2.243 \tTraining Accuracy: 0.164\n",
      "Epoch: 2303 \tTraining Loss: 2.243 \tTraining Accuracy: 0.164\n",
      "Epoch: 2304 \tTraining Loss: 2.243 \tTraining Accuracy: 0.164\n",
      "Epoch: 2305 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2306 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2307 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2308 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2309 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2310 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2311 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2312 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2313 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2314 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2315 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2316 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2317 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2318 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2319 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2320 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2321 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2322 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2323 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2324 \tTraining Loss: 2.243 \tTraining Accuracy: 0.165\n",
      "Epoch: 2325 \tTraining Loss: 2.242 \tTraining Accuracy: 0.165\n",
      "Epoch: 2326 \tTraining Loss: 2.242 \tTraining Accuracy: 0.165\n",
      "Epoch: 2327 \tTraining Loss: 2.242 \tTraining Accuracy: 0.165\n",
      "Epoch: 2328 \tTraining Loss: 2.242 \tTraining Accuracy: 0.165\n",
      "Epoch: 2329 \tTraining Loss: 2.242 \tTraining Accuracy: 0.165\n",
      "Epoch: 2330 \tTraining Loss: 2.242 \tTraining Accuracy: 0.165\n",
      "Epoch: 2331 \tTraining Loss: 2.242 \tTraining Accuracy: 0.165\n",
      "Epoch: 2332 \tTraining Loss: 2.242 \tTraining Accuracy: 0.165\n",
      "Epoch: 2333 \tTraining Loss: 2.242 \tTraining Accuracy: 0.165\n",
      "Epoch: 2334 \tTraining Loss: 2.242 \tTraining Accuracy: 0.166\n",
      "Epoch: 2335 \tTraining Loss: 2.242 \tTraining Accuracy: 0.166\n",
      "Epoch: 2336 \tTraining Loss: 2.242 \tTraining Accuracy: 0.166\n",
      "Epoch: 2337 \tTraining Loss: 2.242 \tTraining Accuracy: 0.166\n",
      "Epoch: 2338 \tTraining Loss: 2.242 \tTraining Accuracy: 0.166\n",
      "Epoch: 2339 \tTraining Loss: 2.242 \tTraining Accuracy: 0.166\n",
      "Epoch: 2340 \tTraining Loss: 2.242 \tTraining Accuracy: 0.166\n",
      "Epoch: 2341 \tTraining Loss: 2.242 \tTraining Accuracy: 0.166\n",
      "Epoch: 2342 \tTraining Loss: 2.242 \tTraining Accuracy: 0.166\n",
      "Epoch: 2343 \tTraining Loss: 2.242 \tTraining Accuracy: 0.166\n",
      "Epoch: 2344 \tTraining Loss: 2.242 \tTraining Accuracy: 0.166\n",
      "Epoch: 2345 \tTraining Loss: 2.242 \tTraining Accuracy: 0.166\n",
      "Epoch: 2346 \tTraining Loss: 2.242 \tTraining Accuracy: 0.166\n",
      "Epoch: 2347 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2348 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2349 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2350 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2351 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2352 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2353 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2354 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2355 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2356 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2357 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2358 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2359 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2360 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2361 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2362 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2363 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2364 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2365 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2366 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2367 \tTraining Loss: 2.241 \tTraining Accuracy: 0.166\n",
      "Epoch: 2368 \tTraining Loss: 2.241 \tTraining Accuracy: 0.167\n",
      "Epoch: 2369 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2370 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2371 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2372 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2373 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2374 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2375 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2376 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2377 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2378 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2379 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2380 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2381 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2382 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2383 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2384 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2385 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2386 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2387 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2388 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2389 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2390 \tTraining Loss: 2.240 \tTraining Accuracy: 0.167\n",
      "Epoch: 2391 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2392 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2393 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2394 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2395 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2396 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2397 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2398 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2399 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2400 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2401 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2402 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2403 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2404 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2405 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2406 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2407 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2408 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2409 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2410 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2411 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2412 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2413 \tTraining Loss: 2.239 \tTraining Accuracy: 0.167\n",
      "Epoch: 2414 \tTraining Loss: 2.238 \tTraining Accuracy: 0.167\n",
      "Epoch: 2415 \tTraining Loss: 2.238 \tTraining Accuracy: 0.167\n",
      "Epoch: 2416 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2417 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2418 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2419 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2420 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2421 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2422 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2423 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2424 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2425 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2426 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2427 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2428 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2429 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2430 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2431 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2432 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2433 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2434 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2435 \tTraining Loss: 2.238 \tTraining Accuracy: 0.168\n",
      "Epoch: 2436 \tTraining Loss: 2.237 \tTraining Accuracy: 0.168\n",
      "Epoch: 2437 \tTraining Loss: 2.237 \tTraining Accuracy: 0.168\n",
      "Epoch: 2438 \tTraining Loss: 2.237 \tTraining Accuracy: 0.168\n",
      "Epoch: 2439 \tTraining Loss: 2.237 \tTraining Accuracy: 0.168\n",
      "Epoch: 2440 \tTraining Loss: 2.237 \tTraining Accuracy: 0.168\n",
      "Epoch: 2441 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2442 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2443 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2444 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2445 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2446 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2447 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2448 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2449 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2450 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2451 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2452 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2453 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2454 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2455 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2456 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2457 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2458 \tTraining Loss: 2.237 \tTraining Accuracy: 0.169\n",
      "Epoch: 2459 \tTraining Loss: 2.236 \tTraining Accuracy: 0.169\n",
      "Epoch: 2460 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2461 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2462 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2463 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2464 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2465 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2466 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2467 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2468 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2469 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2470 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2471 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2472 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2473 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2474 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2475 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2476 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2477 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2478 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2479 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2480 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2481 \tTraining Loss: 2.236 \tTraining Accuracy: 0.170\n",
      "Epoch: 2482 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2483 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2484 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2485 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2486 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2487 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2488 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2489 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2490 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2491 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2492 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2493 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2494 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2495 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2496 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2497 \tTraining Loss: 2.235 \tTraining Accuracy: 0.170\n",
      "Epoch: 2498 \tTraining Loss: 2.235 \tTraining Accuracy: 0.171\n",
      "Epoch: 2499 \tTraining Loss: 2.235 \tTraining Accuracy: 0.171\n",
      "Epoch: 2500 \tTraining Loss: 2.235 \tTraining Accuracy: 0.171\n",
      "Epoch: 2501 \tTraining Loss: 2.235 \tTraining Accuracy: 0.171\n",
      "Epoch: 2502 \tTraining Loss: 2.235 \tTraining Accuracy: 0.171\n",
      "Epoch: 2503 \tTraining Loss: 2.235 \tTraining Accuracy: 0.171\n",
      "Epoch: 2504 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2505 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2506 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2507 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2508 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2509 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2510 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2511 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2512 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2513 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2514 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2515 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2516 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2517 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2518 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2519 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2520 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2521 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2522 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2523 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2524 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2525 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2526 \tTraining Loss: 2.234 \tTraining Accuracy: 0.171\n",
      "Epoch: 2527 \tTraining Loss: 2.233 \tTraining Accuracy: 0.171\n",
      "Epoch: 2528 \tTraining Loss: 2.233 \tTraining Accuracy: 0.171\n",
      "Epoch: 2529 \tTraining Loss: 2.233 \tTraining Accuracy: 0.171\n",
      "Epoch: 2530 \tTraining Loss: 2.233 \tTraining Accuracy: 0.171\n",
      "Epoch: 2531 \tTraining Loss: 2.233 \tTraining Accuracy: 0.171\n",
      "Epoch: 2532 \tTraining Loss: 2.233 \tTraining Accuracy: 0.171\n",
      "Epoch: 2533 \tTraining Loss: 2.233 \tTraining Accuracy: 0.171\n",
      "Epoch: 2534 \tTraining Loss: 2.233 \tTraining Accuracy: 0.171\n",
      "Epoch: 2535 \tTraining Loss: 2.233 \tTraining Accuracy: 0.171\n",
      "Epoch: 2536 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2537 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2538 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2539 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2540 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2541 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2542 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2543 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2544 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2545 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2546 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2547 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2548 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2549 \tTraining Loss: 2.233 \tTraining Accuracy: 0.172\n",
      "Epoch: 2550 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2551 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2552 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2553 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2554 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2555 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2556 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2557 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2558 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2559 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2560 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2561 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2562 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2563 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2564 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2565 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2566 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2567 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2568 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2569 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2570 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2571 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2572 \tTraining Loss: 2.232 \tTraining Accuracy: 0.172\n",
      "Epoch: 2573 \tTraining Loss: 2.231 \tTraining Accuracy: 0.172\n",
      "Epoch: 2574 \tTraining Loss: 2.231 \tTraining Accuracy: 0.172\n",
      "Epoch: 2575 \tTraining Loss: 2.231 \tTraining Accuracy: 0.172\n",
      "Epoch: 2576 \tTraining Loss: 2.231 \tTraining Accuracy: 0.172\n",
      "Epoch: 2577 \tTraining Loss: 2.231 \tTraining Accuracy: 0.172\n",
      "Epoch: 2578 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2579 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2580 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2581 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2582 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2583 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2584 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2585 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2586 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2587 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2588 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2589 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2590 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2591 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2592 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2593 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2594 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2595 \tTraining Loss: 2.231 \tTraining Accuracy: 0.173\n",
      "Epoch: 2596 \tTraining Loss: 2.230 \tTraining Accuracy: 0.173\n",
      "Epoch: 2597 \tTraining Loss: 2.230 \tTraining Accuracy: 0.173\n",
      "Epoch: 2598 \tTraining Loss: 2.230 \tTraining Accuracy: 0.173\n",
      "Epoch: 2599 \tTraining Loss: 2.230 \tTraining Accuracy: 0.173\n",
      "Epoch: 2600 \tTraining Loss: 2.230 \tTraining Accuracy: 0.173\n",
      "Epoch: 2601 \tTraining Loss: 2.230 \tTraining Accuracy: 0.173\n",
      "Epoch: 2602 \tTraining Loss: 2.230 \tTraining Accuracy: 0.173\n",
      "Epoch: 2603 \tTraining Loss: 2.230 \tTraining Accuracy: 0.173\n",
      "Epoch: 2604 \tTraining Loss: 2.230 \tTraining Accuracy: 0.173\n",
      "Epoch: 2605 \tTraining Loss: 2.230 \tTraining Accuracy: 0.173\n",
      "Epoch: 2606 \tTraining Loss: 2.230 \tTraining Accuracy: 0.173\n",
      "Epoch: 2607 \tTraining Loss: 2.230 \tTraining Accuracy: 0.173\n",
      "Epoch: 2608 \tTraining Loss: 2.230 \tTraining Accuracy: 0.173\n",
      "Epoch: 2609 \tTraining Loss: 2.230 \tTraining Accuracy: 0.174\n",
      "Epoch: 2610 \tTraining Loss: 2.230 \tTraining Accuracy: 0.174\n",
      "Epoch: 2611 \tTraining Loss: 2.230 \tTraining Accuracy: 0.174\n",
      "Epoch: 2612 \tTraining Loss: 2.230 \tTraining Accuracy: 0.174\n",
      "Epoch: 2613 \tTraining Loss: 2.230 \tTraining Accuracy: 0.174\n",
      "Epoch: 2614 \tTraining Loss: 2.230 \tTraining Accuracy: 0.174\n",
      "Epoch: 2615 \tTraining Loss: 2.230 \tTraining Accuracy: 0.174\n",
      "Epoch: 2616 \tTraining Loss: 2.230 \tTraining Accuracy: 0.174\n",
      "Epoch: 2617 \tTraining Loss: 2.230 \tTraining Accuracy: 0.174\n",
      "Epoch: 2618 \tTraining Loss: 2.230 \tTraining Accuracy: 0.174\n",
      "Epoch: 2619 \tTraining Loss: 2.229 \tTraining Accuracy: 0.174\n",
      "Epoch: 2620 \tTraining Loss: 2.229 \tTraining Accuracy: 0.174\n",
      "Epoch: 2621 \tTraining Loss: 2.229 \tTraining Accuracy: 0.174\n",
      "Epoch: 2622 \tTraining Loss: 2.229 \tTraining Accuracy: 0.174\n",
      "Epoch: 2623 \tTraining Loss: 2.229 \tTraining Accuracy: 0.174\n",
      "Epoch: 2624 \tTraining Loss: 2.229 \tTraining Accuracy: 0.174\n",
      "Epoch: 2625 \tTraining Loss: 2.229 \tTraining Accuracy: 0.174\n",
      "Epoch: 2626 \tTraining Loss: 2.229 \tTraining Accuracy: 0.174\n",
      "Epoch: 2627 \tTraining Loss: 2.229 \tTraining Accuracy: 0.174\n",
      "Epoch: 2628 \tTraining Loss: 2.229 \tTraining Accuracy: 0.174\n",
      "Epoch: 2629 \tTraining Loss: 2.229 \tTraining Accuracy: 0.174\n",
      "Epoch: 2630 \tTraining Loss: 2.229 \tTraining Accuracy: 0.175\n",
      "Epoch: 2631 \tTraining Loss: 2.229 \tTraining Accuracy: 0.175\n",
      "Epoch: 2632 \tTraining Loss: 2.229 \tTraining Accuracy: 0.175\n",
      "Epoch: 2633 \tTraining Loss: 2.229 \tTraining Accuracy: 0.175\n",
      "Epoch: 2634 \tTraining Loss: 2.229 \tTraining Accuracy: 0.175\n",
      "Epoch: 2635 \tTraining Loss: 2.229 \tTraining Accuracy: 0.175\n",
      "Epoch: 2636 \tTraining Loss: 2.229 \tTraining Accuracy: 0.175\n",
      "Epoch: 2637 \tTraining Loss: 2.229 \tTraining Accuracy: 0.175\n",
      "Epoch: 2638 \tTraining Loss: 2.229 \tTraining Accuracy: 0.175\n",
      "Epoch: 2639 \tTraining Loss: 2.229 \tTraining Accuracy: 0.175\n",
      "Epoch: 2640 \tTraining Loss: 2.229 \tTraining Accuracy: 0.175\n",
      "Epoch: 2641 \tTraining Loss: 2.229 \tTraining Accuracy: 0.175\n",
      "Epoch: 2642 \tTraining Loss: 2.229 \tTraining Accuracy: 0.175\n",
      "Epoch: 2643 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2644 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2645 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2646 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2647 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2648 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2649 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2650 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2651 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2652 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2653 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2654 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2655 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2656 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2657 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2658 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2659 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2660 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2661 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2662 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2663 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2664 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2665 \tTraining Loss: 2.228 \tTraining Accuracy: 0.175\n",
      "Epoch: 2666 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2667 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2668 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2669 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2670 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2671 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2672 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2673 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2674 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2675 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2676 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2677 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2678 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2679 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2680 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2681 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2682 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2683 \tTraining Loss: 2.227 \tTraining Accuracy: 0.175\n",
      "Epoch: 2684 \tTraining Loss: 2.227 \tTraining Accuracy: 0.176\n",
      "Epoch: 2685 \tTraining Loss: 2.227 \tTraining Accuracy: 0.176\n",
      "Epoch: 2686 \tTraining Loss: 2.227 \tTraining Accuracy: 0.176\n",
      "Epoch: 2687 \tTraining Loss: 2.227 \tTraining Accuracy: 0.176\n",
      "Epoch: 2688 \tTraining Loss: 2.227 \tTraining Accuracy: 0.176\n",
      "Epoch: 2689 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2690 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2691 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2692 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2693 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2694 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2695 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2696 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2697 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2698 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2699 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2700 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2701 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2702 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2703 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2704 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2705 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2706 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2707 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2708 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2709 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2710 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2711 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2712 \tTraining Loss: 2.226 \tTraining Accuracy: 0.176\n",
      "Epoch: 2713 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2714 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2715 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2716 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2717 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2718 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2719 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2720 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2721 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2722 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2723 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2724 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2725 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2726 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2727 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2728 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2729 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2730 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2731 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2732 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2733 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2734 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2735 \tTraining Loss: 2.225 \tTraining Accuracy: 0.176\n",
      "Epoch: 2736 \tTraining Loss: 2.224 \tTraining Accuracy: 0.176\n",
      "Epoch: 2737 \tTraining Loss: 2.224 \tTraining Accuracy: 0.176\n",
      "Epoch: 2738 \tTraining Loss: 2.224 \tTraining Accuracy: 0.176\n",
      "Epoch: 2739 \tTraining Loss: 2.224 \tTraining Accuracy: 0.176\n",
      "Epoch: 2740 \tTraining Loss: 2.224 \tTraining Accuracy: 0.176\n",
      "Epoch: 2741 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2742 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2743 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2744 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2745 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2746 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2747 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2748 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2749 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2750 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2751 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2752 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2753 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2754 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2755 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2756 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2757 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2758 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2759 \tTraining Loss: 2.224 \tTraining Accuracy: 0.177\n",
      "Epoch: 2760 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2761 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2762 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2763 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2764 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2765 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2766 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2767 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2768 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2769 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2770 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2771 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2772 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2773 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2774 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2775 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2776 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2777 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2778 \tTraining Loss: 2.223 \tTraining Accuracy: 0.177\n",
      "Epoch: 2779 \tTraining Loss: 2.223 \tTraining Accuracy: 0.178\n",
      "Epoch: 2780 \tTraining Loss: 2.223 \tTraining Accuracy: 0.178\n",
      "Epoch: 2781 \tTraining Loss: 2.223 \tTraining Accuracy: 0.178\n",
      "Epoch: 2782 \tTraining Loss: 2.223 \tTraining Accuracy: 0.178\n",
      "Epoch: 2783 \tTraining Loss: 2.223 \tTraining Accuracy: 0.178\n",
      "Epoch: 2784 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2785 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2786 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2787 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2788 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2789 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2790 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2791 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2792 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2793 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2794 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2795 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2796 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2797 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2798 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2799 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2800 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2801 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2802 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2803 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2804 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2805 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2806 \tTraining Loss: 2.222 \tTraining Accuracy: 0.178\n",
      "Epoch: 2807 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2808 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2809 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2810 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2811 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2812 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2813 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2814 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2815 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2816 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2817 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2818 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2819 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2820 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2821 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2822 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2823 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2824 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2825 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2826 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2827 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2828 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2829 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2830 \tTraining Loss: 2.221 \tTraining Accuracy: 0.178\n",
      "Epoch: 2831 \tTraining Loss: 2.220 \tTraining Accuracy: 0.178\n",
      "Epoch: 2832 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2833 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2834 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2835 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2836 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2837 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2838 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2839 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2840 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2841 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2842 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2843 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2844 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2845 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2846 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2847 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2848 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2849 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2850 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2851 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2852 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2853 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2854 \tTraining Loss: 2.220 \tTraining Accuracy: 0.179\n",
      "Epoch: 2855 \tTraining Loss: 2.219 \tTraining Accuracy: 0.179\n",
      "Epoch: 2856 \tTraining Loss: 2.219 \tTraining Accuracy: 0.179\n",
      "Epoch: 2857 \tTraining Loss: 2.219 \tTraining Accuracy: 0.179\n",
      "Epoch: 2858 \tTraining Loss: 2.219 \tTraining Accuracy: 0.179\n",
      "Epoch: 2859 \tTraining Loss: 2.219 \tTraining Accuracy: 0.179\n",
      "Epoch: 2860 \tTraining Loss: 2.219 \tTraining Accuracy: 0.179\n",
      "Epoch: 2861 \tTraining Loss: 2.219 \tTraining Accuracy: 0.179\n",
      "Epoch: 2862 \tTraining Loss: 2.219 \tTraining Accuracy: 0.179\n",
      "Epoch: 2863 \tTraining Loss: 2.219 \tTraining Accuracy: 0.179\n",
      "Epoch: 2864 \tTraining Loss: 2.219 \tTraining Accuracy: 0.179\n",
      "Epoch: 2865 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2866 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2867 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2868 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2869 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2870 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2871 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2872 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2873 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2874 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2875 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2876 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2877 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2878 \tTraining Loss: 2.219 \tTraining Accuracy: 0.180\n",
      "Epoch: 2879 \tTraining Loss: 2.218 \tTraining Accuracy: 0.180\n",
      "Epoch: 2880 \tTraining Loss: 2.218 \tTraining Accuracy: 0.180\n",
      "Epoch: 2881 \tTraining Loss: 2.218 \tTraining Accuracy: 0.180\n",
      "Epoch: 2882 \tTraining Loss: 2.218 \tTraining Accuracy: 0.180\n",
      "Epoch: 2883 \tTraining Loss: 2.218 \tTraining Accuracy: 0.180\n",
      "Epoch: 2884 \tTraining Loss: 2.218 \tTraining Accuracy: 0.180\n",
      "Epoch: 2885 \tTraining Loss: 2.218 \tTraining Accuracy: 0.180\n",
      "Epoch: 2886 \tTraining Loss: 2.218 \tTraining Accuracy: 0.180\n",
      "Epoch: 2887 \tTraining Loss: 2.218 \tTraining Accuracy: 0.180\n",
      "Epoch: 2888 \tTraining Loss: 2.218 \tTraining Accuracy: 0.180\n",
      "Epoch: 2889 \tTraining Loss: 2.218 \tTraining Accuracy: 0.180\n",
      "Epoch: 2890 \tTraining Loss: 2.218 \tTraining Accuracy: 0.181\n",
      "Epoch: 2891 \tTraining Loss: 2.218 \tTraining Accuracy: 0.181\n",
      "Epoch: 2892 \tTraining Loss: 2.218 \tTraining Accuracy: 0.181\n",
      "Epoch: 2893 \tTraining Loss: 2.218 \tTraining Accuracy: 0.181\n",
      "Epoch: 2894 \tTraining Loss: 2.218 \tTraining Accuracy: 0.181\n",
      "Epoch: 2895 \tTraining Loss: 2.218 \tTraining Accuracy: 0.181\n",
      "Epoch: 2896 \tTraining Loss: 2.218 \tTraining Accuracy: 0.181\n",
      "Epoch: 2897 \tTraining Loss: 2.218 \tTraining Accuracy: 0.181\n",
      "Epoch: 2898 \tTraining Loss: 2.218 \tTraining Accuracy: 0.181\n",
      "Epoch: 2899 \tTraining Loss: 2.218 \tTraining Accuracy: 0.181\n",
      "Epoch: 2900 \tTraining Loss: 2.218 \tTraining Accuracy: 0.181\n",
      "Epoch: 2901 \tTraining Loss: 2.218 \tTraining Accuracy: 0.181\n",
      "Epoch: 2902 \tTraining Loss: 2.218 \tTraining Accuracy: 0.181\n",
      "Epoch: 2903 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2904 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2905 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2906 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2907 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2908 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2909 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2910 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2911 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2912 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2913 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2914 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2915 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2916 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2917 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2918 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2919 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2920 \tTraining Loss: 2.217 \tTraining Accuracy: 0.181\n",
      "Epoch: 2921 \tTraining Loss: 2.217 \tTraining Accuracy: 0.182\n",
      "Epoch: 2922 \tTraining Loss: 2.217 \tTraining Accuracy: 0.182\n",
      "Epoch: 2923 \tTraining Loss: 2.217 \tTraining Accuracy: 0.182\n",
      "Epoch: 2924 \tTraining Loss: 2.217 \tTraining Accuracy: 0.182\n",
      "Epoch: 2925 \tTraining Loss: 2.217 \tTraining Accuracy: 0.182\n",
      "Epoch: 2926 \tTraining Loss: 2.217 \tTraining Accuracy: 0.182\n",
      "Epoch: 2927 \tTraining Loss: 2.217 \tTraining Accuracy: 0.182\n",
      "Epoch: 2928 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2929 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2930 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2931 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2932 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2933 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2934 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2935 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2936 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2937 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2938 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2939 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2940 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2941 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2942 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2943 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2944 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2945 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2946 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2947 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2948 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2949 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2950 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2951 \tTraining Loss: 2.216 \tTraining Accuracy: 0.182\n",
      "Epoch: 2952 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2953 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2954 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2955 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2956 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2957 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2958 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2959 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2960 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2961 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2962 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2963 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2964 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2965 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2966 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2967 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2968 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2969 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2970 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2971 \tTraining Loss: 2.215 \tTraining Accuracy: 0.182\n",
      "Epoch: 2972 \tTraining Loss: 2.215 \tTraining Accuracy: 0.183\n",
      "Epoch: 2973 \tTraining Loss: 2.215 \tTraining Accuracy: 0.183\n",
      "Epoch: 2974 \tTraining Loss: 2.215 \tTraining Accuracy: 0.183\n",
      "Epoch: 2975 \tTraining Loss: 2.215 \tTraining Accuracy: 0.183\n",
      "Epoch: 2976 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2977 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2978 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2979 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2980 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2981 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2982 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2983 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2984 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2985 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2986 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2987 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2988 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2989 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2990 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2991 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2992 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2993 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2994 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2995 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2996 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2997 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2998 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 2999 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 3000 \tTraining Loss: 2.214 \tTraining Accuracy: 0.183\n",
      "Epoch: 3001 \tTraining Loss: 2.213 \tTraining Accuracy: 0.183\n",
      "Epoch: 3002 \tTraining Loss: 2.213 \tTraining Accuracy: 0.183\n",
      "Epoch: 3003 \tTraining Loss: 2.213 \tTraining Accuracy: 0.183\n",
      "Epoch: 3004 \tTraining Loss: 2.213 \tTraining Accuracy: 0.183\n",
      "Epoch: 3005 \tTraining Loss: 2.213 \tTraining Accuracy: 0.183\n",
      "Epoch: 3006 \tTraining Loss: 2.213 \tTraining Accuracy: 0.183\n",
      "Epoch: 3007 \tTraining Loss: 2.213 \tTraining Accuracy: 0.183\n",
      "Epoch: 3008 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3009 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3010 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3011 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3012 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3013 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3014 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3015 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3016 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3017 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3018 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3019 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3020 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3021 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3022 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3023 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3024 \tTraining Loss: 2.213 \tTraining Accuracy: 0.184\n",
      "Epoch: 3025 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3026 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3027 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3028 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3029 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3030 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3031 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3032 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3033 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3034 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3035 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3036 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3037 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3038 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3039 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3040 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3041 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3042 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3043 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3044 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3045 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3046 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3047 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3048 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3049 \tTraining Loss: 2.212 \tTraining Accuracy: 0.184\n",
      "Epoch: 3050 \tTraining Loss: 2.211 \tTraining Accuracy: 0.184\n",
      "Epoch: 3051 \tTraining Loss: 2.211 \tTraining Accuracy: 0.184\n",
      "Epoch: 3052 \tTraining Loss: 2.211 \tTraining Accuracy: 0.184\n",
      "Epoch: 3053 \tTraining Loss: 2.211 \tTraining Accuracy: 0.184\n",
      "Epoch: 3054 \tTraining Loss: 2.211 \tTraining Accuracy: 0.184\n",
      "Epoch: 3055 \tTraining Loss: 2.211 \tTraining Accuracy: 0.184\n",
      "Epoch: 3056 \tTraining Loss: 2.211 \tTraining Accuracy: 0.184\n",
      "Epoch: 3057 \tTraining Loss: 2.211 \tTraining Accuracy: 0.184\n",
      "Epoch: 3058 \tTraining Loss: 2.211 \tTraining Accuracy: 0.184\n",
      "Epoch: 3059 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3060 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3061 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3062 \tTraining Loss: 2.211 \tTraining Accuracy: 0.184\n",
      "Epoch: 3063 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3064 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3065 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3066 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3067 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3068 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3069 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3070 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3071 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3072 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3073 \tTraining Loss: 2.211 \tTraining Accuracy: 0.185\n",
      "Epoch: 3074 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3075 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3076 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3077 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3078 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3079 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3080 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3081 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3082 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3083 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3084 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3085 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3086 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3087 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3088 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3089 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3090 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3091 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3092 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3093 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3094 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3095 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3096 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3097 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3098 \tTraining Loss: 2.210 \tTraining Accuracy: 0.185\n",
      "Epoch: 3099 \tTraining Loss: 2.209 \tTraining Accuracy: 0.185\n",
      "Epoch: 3100 \tTraining Loss: 2.209 \tTraining Accuracy: 0.185\n",
      "Epoch: 3101 \tTraining Loss: 2.209 \tTraining Accuracy: 0.185\n",
      "Epoch: 3102 \tTraining Loss: 2.209 \tTraining Accuracy: 0.185\n",
      "Epoch: 3103 \tTraining Loss: 2.209 \tTraining Accuracy: 0.185\n",
      "Epoch: 3104 \tTraining Loss: 2.209 \tTraining Accuracy: 0.185\n",
      "Epoch: 3105 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3106 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3107 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3108 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3109 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3110 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3111 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3112 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3113 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3114 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3115 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3116 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3117 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3118 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3119 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3120 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3121 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3122 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3123 \tTraining Loss: 2.209 \tTraining Accuracy: 0.186\n",
      "Epoch: 3124 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3125 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3126 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3127 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3128 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3129 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3130 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3131 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3132 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3133 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3134 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3135 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3136 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3137 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3138 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3139 \tTraining Loss: 2.208 \tTraining Accuracy: 0.186\n",
      "Epoch: 3140 \tTraining Loss: 2.208 \tTraining Accuracy: 0.187\n",
      "Epoch: 3141 \tTraining Loss: 2.208 \tTraining Accuracy: 0.187\n",
      "Epoch: 3142 \tTraining Loss: 2.208 \tTraining Accuracy: 0.187\n",
      "Epoch: 3143 \tTraining Loss: 2.208 \tTraining Accuracy: 0.187\n",
      "Epoch: 3144 \tTraining Loss: 2.208 \tTraining Accuracy: 0.187\n",
      "Epoch: 3145 \tTraining Loss: 2.208 \tTraining Accuracy: 0.187\n",
      "Epoch: 3146 \tTraining Loss: 2.208 \tTraining Accuracy: 0.187\n",
      "Epoch: 3147 \tTraining Loss: 2.208 \tTraining Accuracy: 0.187\n",
      "Epoch: 3148 \tTraining Loss: 2.208 \tTraining Accuracy: 0.187\n",
      "Epoch: 3149 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3150 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3151 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3152 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3153 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3154 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3155 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3156 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3157 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3158 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3159 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3160 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3161 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3162 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3163 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3164 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3165 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3166 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3167 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3168 \tTraining Loss: 2.207 \tTraining Accuracy: 0.187\n",
      "Epoch: 3169 \tTraining Loss: 2.207 \tTraining Accuracy: 0.188\n",
      "Epoch: 3170 \tTraining Loss: 2.207 \tTraining Accuracy: 0.188\n",
      "Epoch: 3171 \tTraining Loss: 2.207 \tTraining Accuracy: 0.188\n",
      "Epoch: 3172 \tTraining Loss: 2.207 \tTraining Accuracy: 0.188\n",
      "Epoch: 3173 \tTraining Loss: 2.207 \tTraining Accuracy: 0.188\n",
      "Epoch: 3174 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3175 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3176 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3177 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3178 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3179 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3180 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3181 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3182 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3183 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3184 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3185 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3186 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3187 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3188 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3189 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3190 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3191 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3192 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3193 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3194 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3195 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3196 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3197 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3198 \tTraining Loss: 2.206 \tTraining Accuracy: 0.188\n",
      "Epoch: 3199 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3200 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3201 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3202 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3203 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3204 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3205 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3206 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3207 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3208 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3209 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3210 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3211 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3212 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3213 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3214 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3215 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3216 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3217 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3218 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3219 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3220 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3221 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3222 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3223 \tTraining Loss: 2.205 \tTraining Accuracy: 0.188\n",
      "Epoch: 3224 \tTraining Loss: 2.204 \tTraining Accuracy: 0.188\n",
      "Epoch: 3225 \tTraining Loss: 2.204 \tTraining Accuracy: 0.188\n",
      "Epoch: 3226 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3227 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3228 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3229 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3230 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3231 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3232 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3233 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3234 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3235 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3236 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3237 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3238 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3239 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3240 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3241 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3242 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3243 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3244 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3245 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3246 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3247 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3248 \tTraining Loss: 2.204 \tTraining Accuracy: 0.189\n",
      "Epoch: 3249 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3250 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3251 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3252 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3253 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3254 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3255 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3256 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3257 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3258 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3259 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3260 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3261 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3262 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3263 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3264 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3265 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3266 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3267 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3268 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3269 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3270 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3271 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3272 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3273 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3274 \tTraining Loss: 2.203 \tTraining Accuracy: 0.189\n",
      "Epoch: 3275 \tTraining Loss: 2.202 \tTraining Accuracy: 0.189\n",
      "Epoch: 3276 \tTraining Loss: 2.202 \tTraining Accuracy: 0.189\n",
      "Epoch: 3277 \tTraining Loss: 2.202 \tTraining Accuracy: 0.189\n",
      "Epoch: 3278 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3279 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3280 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3281 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3282 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3283 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3284 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3285 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3286 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3287 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3288 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3289 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3290 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3291 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3292 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3293 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3294 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3295 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3296 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3297 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3298 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3299 \tTraining Loss: 2.202 \tTraining Accuracy: 0.190\n",
      "Epoch: 3300 \tTraining Loss: 2.201 \tTraining Accuracy: 0.190\n",
      "Epoch: 3301 \tTraining Loss: 2.201 \tTraining Accuracy: 0.190\n",
      "Epoch: 3302 \tTraining Loss: 2.201 \tTraining Accuracy: 0.190\n",
      "Epoch: 3303 \tTraining Loss: 2.201 \tTraining Accuracy: 0.190\n",
      "Epoch: 3304 \tTraining Loss: 2.201 \tTraining Accuracy: 0.190\n",
      "Epoch: 3305 \tTraining Loss: 2.201 \tTraining Accuracy: 0.190\n",
      "Epoch: 3306 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3307 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3308 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3309 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3310 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3311 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3312 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3313 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3314 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3315 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3316 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3317 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3318 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3319 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3320 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3321 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3322 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3323 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3324 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3325 \tTraining Loss: 2.201 \tTraining Accuracy: 0.191\n",
      "Epoch: 3326 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3327 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3328 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3329 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3330 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3331 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3332 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3333 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3334 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3335 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3336 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3337 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3338 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3339 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3340 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3341 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3342 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3343 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3344 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3345 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3346 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3347 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3348 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3349 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3350 \tTraining Loss: 2.200 \tTraining Accuracy: 0.191\n",
      "Epoch: 3351 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3352 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3353 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3354 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3355 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3356 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3357 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3358 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3359 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3360 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3361 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3362 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3363 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3364 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3365 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3366 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3367 \tTraining Loss: 2.199 \tTraining Accuracy: 0.191\n",
      "Epoch: 3368 \tTraining Loss: 2.199 \tTraining Accuracy: 0.192\n",
      "Epoch: 3369 \tTraining Loss: 2.199 \tTraining Accuracy: 0.192\n",
      "Epoch: 3370 \tTraining Loss: 2.199 \tTraining Accuracy: 0.192\n",
      "Epoch: 3371 \tTraining Loss: 2.199 \tTraining Accuracy: 0.192\n",
      "Epoch: 3372 \tTraining Loss: 2.199 \tTraining Accuracy: 0.192\n",
      "Epoch: 3373 \tTraining Loss: 2.199 \tTraining Accuracy: 0.192\n",
      "Epoch: 3374 \tTraining Loss: 2.199 \tTraining Accuracy: 0.192\n",
      "Epoch: 3375 \tTraining Loss: 2.199 \tTraining Accuracy: 0.192\n",
      "Epoch: 3376 \tTraining Loss: 2.199 \tTraining Accuracy: 0.192\n",
      "Epoch: 3377 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3378 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3379 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3380 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3381 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3382 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3383 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3384 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3385 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3386 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3387 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3388 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3389 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3390 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3391 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3392 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3393 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3394 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3395 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3396 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3397 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3398 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3399 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3400 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3401 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3402 \tTraining Loss: 2.198 \tTraining Accuracy: 0.192\n",
      "Epoch: 3403 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3404 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3405 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3406 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3407 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3408 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3409 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3410 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3411 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3412 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3413 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3414 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3415 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3416 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3417 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3418 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3419 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3420 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3421 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3422 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3423 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3424 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3425 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3426 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3427 \tTraining Loss: 2.197 \tTraining Accuracy: 0.193\n",
      "Epoch: 3428 \tTraining Loss: 2.196 \tTraining Accuracy: 0.193\n",
      "Epoch: 3429 \tTraining Loss: 2.196 \tTraining Accuracy: 0.193\n",
      "Epoch: 3430 \tTraining Loss: 2.196 \tTraining Accuracy: 0.193\n",
      "Epoch: 3431 \tTraining Loss: 2.196 \tTraining Accuracy: 0.193\n",
      "Epoch: 3432 \tTraining Loss: 2.196 \tTraining Accuracy: 0.193\n",
      "Epoch: 3433 \tTraining Loss: 2.196 \tTraining Accuracy: 0.193\n",
      "Epoch: 3434 \tTraining Loss: 2.196 \tTraining Accuracy: 0.193\n",
      "Epoch: 3435 \tTraining Loss: 2.196 \tTraining Accuracy: 0.193\n",
      "Epoch: 3436 \tTraining Loss: 2.196 \tTraining Accuracy: 0.193\n",
      "Epoch: 3437 \tTraining Loss: 2.196 \tTraining Accuracy: 0.193\n",
      "Epoch: 3438 \tTraining Loss: 2.196 \tTraining Accuracy: 0.193\n",
      "Epoch: 3439 \tTraining Loss: 2.196 \tTraining Accuracy: 0.193\n",
      "Epoch: 3440 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3441 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3442 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3443 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3444 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3445 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3446 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3447 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3448 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3449 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3450 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3451 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3452 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3453 \tTraining Loss: 2.196 \tTraining Accuracy: 0.194\n",
      "Epoch: 3454 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3455 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3456 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3457 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3458 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3459 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3460 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3461 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3462 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3463 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3464 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3465 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3466 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3467 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3468 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3469 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3470 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3471 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3472 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3473 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3474 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3475 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3476 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3477 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3478 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3479 \tTraining Loss: 2.195 \tTraining Accuracy: 0.194\n",
      "Epoch: 3480 \tTraining Loss: 2.194 \tTraining Accuracy: 0.194\n",
      "Epoch: 3481 \tTraining Loss: 2.194 \tTraining Accuracy: 0.194\n",
      "Epoch: 3482 \tTraining Loss: 2.194 \tTraining Accuracy: 0.194\n",
      "Epoch: 3483 \tTraining Loss: 2.194 \tTraining Accuracy: 0.194\n",
      "Epoch: 3484 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3485 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3486 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3487 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3488 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3489 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3490 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3491 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3492 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3493 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3494 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3495 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3496 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3497 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3498 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3499 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3500 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3501 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3502 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3503 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3504 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3505 \tTraining Loss: 2.194 \tTraining Accuracy: 0.195\n",
      "Epoch: 3506 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3507 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3508 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3509 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3510 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3511 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3512 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3513 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3514 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3515 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3516 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3517 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3518 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3519 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3520 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3521 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3522 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3523 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3524 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3525 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3526 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3527 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3528 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3529 \tTraining Loss: 2.193 \tTraining Accuracy: 0.195\n",
      "Epoch: 3530 \tTraining Loss: 2.193 \tTraining Accuracy: 0.196\n",
      "Epoch: 3531 \tTraining Loss: 2.193 \tTraining Accuracy: 0.196\n",
      "Epoch: 3532 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3533 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3534 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3535 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3536 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3537 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3538 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3539 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3540 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3541 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3542 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3543 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3544 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3545 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3546 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3547 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3548 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3549 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3550 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3551 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3552 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3553 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3554 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3555 \tTraining Loss: 2.192 \tTraining Accuracy: 0.196\n",
      "Epoch: 3556 \tTraining Loss: 2.192 \tTraining Accuracy: 0.197\n",
      "Epoch: 3557 \tTraining Loss: 2.192 \tTraining Accuracy: 0.197\n",
      "Epoch: 3558 \tTraining Loss: 2.192 \tTraining Accuracy: 0.197\n",
      "Epoch: 3559 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3560 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3561 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3562 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3563 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3564 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3565 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3566 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3567 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3568 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3569 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3570 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3571 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3572 \tTraining Loss: 2.191 \tTraining Accuracy: 0.197\n",
      "Epoch: 3573 \tTraining Loss: 2.191 \tTraining Accuracy: 0.198\n",
      "Epoch: 3574 \tTraining Loss: 2.191 \tTraining Accuracy: 0.198\n",
      "Epoch: 3575 \tTraining Loss: 2.191 \tTraining Accuracy: 0.198\n",
      "Epoch: 3576 \tTraining Loss: 2.191 \tTraining Accuracy: 0.198\n",
      "Epoch: 3577 \tTraining Loss: 2.191 \tTraining Accuracy: 0.198\n",
      "Epoch: 3578 \tTraining Loss: 2.191 \tTraining Accuracy: 0.198\n",
      "Epoch: 3579 \tTraining Loss: 2.191 \tTraining Accuracy: 0.198\n",
      "Epoch: 3580 \tTraining Loss: 2.191 \tTraining Accuracy: 0.198\n",
      "Epoch: 3581 \tTraining Loss: 2.191 \tTraining Accuracy: 0.198\n",
      "Epoch: 3582 \tTraining Loss: 2.191 \tTraining Accuracy: 0.198\n",
      "Epoch: 3583 \tTraining Loss: 2.191 \tTraining Accuracy: 0.198\n",
      "Epoch: 3584 \tTraining Loss: 2.191 \tTraining Accuracy: 0.198\n",
      "Epoch: 3585 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3586 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3587 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3588 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3589 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3590 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3591 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3592 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3593 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3594 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3595 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3596 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3597 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3598 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3599 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3600 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3601 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3602 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3603 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3604 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3605 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3606 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3607 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3608 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3609 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3610 \tTraining Loss: 2.190 \tTraining Accuracy: 0.198\n",
      "Epoch: 3611 \tTraining Loss: 2.189 \tTraining Accuracy: 0.198\n",
      "Epoch: 3612 \tTraining Loss: 2.189 \tTraining Accuracy: 0.198\n",
      "Epoch: 3613 \tTraining Loss: 2.189 \tTraining Accuracy: 0.198\n",
      "Epoch: 3614 \tTraining Loss: 2.189 \tTraining Accuracy: 0.198\n",
      "Epoch: 3615 \tTraining Loss: 2.189 \tTraining Accuracy: 0.198\n",
      "Epoch: 3616 \tTraining Loss: 2.189 \tTraining Accuracy: 0.198\n",
      "Epoch: 3617 \tTraining Loss: 2.189 \tTraining Accuracy: 0.198\n",
      "Epoch: 3618 \tTraining Loss: 2.189 \tTraining Accuracy: 0.198\n",
      "Epoch: 3619 \tTraining Loss: 2.189 \tTraining Accuracy: 0.198\n",
      "Epoch: 3620 \tTraining Loss: 2.189 \tTraining Accuracy: 0.198\n",
      "Epoch: 3621 \tTraining Loss: 2.189 \tTraining Accuracy: 0.198\n",
      "Epoch: 3622 \tTraining Loss: 2.189 \tTraining Accuracy: 0.198\n",
      "Epoch: 3623 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3624 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3625 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3626 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3627 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3628 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3629 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3630 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3631 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3632 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3633 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3634 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3635 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3636 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3637 \tTraining Loss: 2.189 \tTraining Accuracy: 0.199\n",
      "Epoch: 3638 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3639 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3640 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3641 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3642 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3643 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3644 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3645 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3646 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3647 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3648 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3649 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3650 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3651 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3652 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3653 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3654 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3655 \tTraining Loss: 2.188 \tTraining Accuracy: 0.199\n",
      "Epoch: 3656 \tTraining Loss: 2.188 \tTraining Accuracy: 0.200\n",
      "Epoch: 3657 \tTraining Loss: 2.188 \tTraining Accuracy: 0.200\n",
      "Epoch: 3658 \tTraining Loss: 2.188 \tTraining Accuracy: 0.200\n",
      "Epoch: 3659 \tTraining Loss: 2.188 \tTraining Accuracy: 0.200\n",
      "Epoch: 3660 \tTraining Loss: 2.188 \tTraining Accuracy: 0.200\n",
      "Epoch: 3661 \tTraining Loss: 2.188 \tTraining Accuracy: 0.200\n",
      "Epoch: 3662 \tTraining Loss: 2.188 \tTraining Accuracy: 0.200\n",
      "Epoch: 3663 \tTraining Loss: 2.188 \tTraining Accuracy: 0.200\n",
      "Epoch: 3664 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3665 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3666 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3667 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3668 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3669 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3670 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3671 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3672 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3673 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3674 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3675 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3676 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3677 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3678 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3679 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3680 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3681 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3682 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3683 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3684 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3685 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3686 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3687 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3688 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3689 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3690 \tTraining Loss: 2.187 \tTraining Accuracy: 0.200\n",
      "Epoch: 3691 \tTraining Loss: 2.186 \tTraining Accuracy: 0.200\n",
      "Epoch: 3692 \tTraining Loss: 2.186 \tTraining Accuracy: 0.200\n",
      "Epoch: 3693 \tTraining Loss: 2.186 \tTraining Accuracy: 0.200\n",
      "Epoch: 3694 \tTraining Loss: 2.186 \tTraining Accuracy: 0.200\n",
      "Epoch: 3695 \tTraining Loss: 2.186 \tTraining Accuracy: 0.200\n",
      "Epoch: 3696 \tTraining Loss: 2.186 \tTraining Accuracy: 0.200\n",
      "Epoch: 3697 \tTraining Loss: 2.186 \tTraining Accuracy: 0.200\n",
      "Epoch: 3698 \tTraining Loss: 2.186 \tTraining Accuracy: 0.200\n",
      "Epoch: 3699 \tTraining Loss: 2.186 \tTraining Accuracy: 0.200\n",
      "Epoch: 3700 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3701 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3702 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3703 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3704 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3705 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3706 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3707 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3708 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3709 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3710 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3711 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3712 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3713 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3714 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3715 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3716 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3717 \tTraining Loss: 2.186 \tTraining Accuracy: 0.201\n",
      "Epoch: 3718 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3719 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3720 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3721 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3722 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3723 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3724 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3725 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3726 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3727 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3728 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3729 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3730 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3731 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3732 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3733 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3734 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3735 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3736 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3737 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3738 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3739 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3740 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3741 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3742 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3743 \tTraining Loss: 2.185 \tTraining Accuracy: 0.201\n",
      "Epoch: 3744 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3745 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3746 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3747 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3748 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3749 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3750 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3751 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3752 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3753 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3754 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3755 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3756 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3757 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3758 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3759 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3760 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3761 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3762 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3763 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3764 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3765 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3766 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3767 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3768 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3769 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3770 \tTraining Loss: 2.184 \tTraining Accuracy: 0.202\n",
      "Epoch: 3771 \tTraining Loss: 2.183 \tTraining Accuracy: 0.202\n",
      "Epoch: 3772 \tTraining Loss: 2.183 \tTraining Accuracy: 0.202\n",
      "Epoch: 3773 \tTraining Loss: 2.183 \tTraining Accuracy: 0.202\n",
      "Epoch: 3774 \tTraining Loss: 2.183 \tTraining Accuracy: 0.202\n",
      "Epoch: 3775 \tTraining Loss: 2.183 \tTraining Accuracy: 0.202\n",
      "Epoch: 3776 \tTraining Loss: 2.183 \tTraining Accuracy: 0.202\n",
      "Epoch: 3777 \tTraining Loss: 2.183 \tTraining Accuracy: 0.202\n",
      "Epoch: 3778 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3779 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3780 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3781 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3782 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3783 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3784 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3785 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3786 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3787 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3788 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3789 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3790 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3791 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3792 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3793 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3794 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3795 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3796 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3797 \tTraining Loss: 2.183 \tTraining Accuracy: 0.203\n",
      "Epoch: 3798 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3799 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3800 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3801 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3802 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3803 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3804 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3805 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3806 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3807 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3808 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3809 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3810 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3811 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3812 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3813 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3814 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3815 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3816 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3817 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3818 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3819 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3820 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3821 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3822 \tTraining Loss: 2.182 \tTraining Accuracy: 0.203\n",
      "Epoch: 3823 \tTraining Loss: 2.182 \tTraining Accuracy: 0.204\n",
      "Epoch: 3824 \tTraining Loss: 2.182 \tTraining Accuracy: 0.204\n",
      "Epoch: 3825 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3826 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3827 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3828 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3829 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3830 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3831 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3832 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3833 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3834 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3835 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3836 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3837 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3838 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3839 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3840 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3841 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3842 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3843 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3844 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3845 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3846 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3847 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3848 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3849 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3850 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3851 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3852 \tTraining Loss: 2.181 \tTraining Accuracy: 0.204\n",
      "Epoch: 3853 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3854 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3855 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3856 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3857 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3858 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3859 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3860 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3861 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3862 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3863 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3864 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3865 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3866 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3867 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3868 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3869 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3870 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3871 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3872 \tTraining Loss: 2.180 \tTraining Accuracy: 0.204\n",
      "Epoch: 3873 \tTraining Loss: 2.180 \tTraining Accuracy: 0.205\n",
      "Epoch: 3874 \tTraining Loss: 2.180 \tTraining Accuracy: 0.205\n",
      "Epoch: 3875 \tTraining Loss: 2.180 \tTraining Accuracy: 0.205\n",
      "Epoch: 3876 \tTraining Loss: 2.180 \tTraining Accuracy: 0.205\n",
      "Epoch: 3877 \tTraining Loss: 2.180 \tTraining Accuracy: 0.205\n",
      "Epoch: 3878 \tTraining Loss: 2.180 \tTraining Accuracy: 0.205\n",
      "Epoch: 3879 \tTraining Loss: 2.180 \tTraining Accuracy: 0.205\n",
      "Epoch: 3880 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3881 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3882 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3883 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3884 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3885 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3886 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3887 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3888 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3889 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3890 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3891 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3892 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3893 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3894 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3895 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3896 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3897 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3898 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3899 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3900 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3901 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3902 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3903 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3904 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3905 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3906 \tTraining Loss: 2.179 \tTraining Accuracy: 0.205\n",
      "Epoch: 3907 \tTraining Loss: 2.178 \tTraining Accuracy: 0.205\n",
      "Epoch: 3908 \tTraining Loss: 2.178 \tTraining Accuracy: 0.205\n",
      "Epoch: 3909 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3910 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3911 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3912 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3913 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3914 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3915 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3916 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3917 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3918 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3919 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3920 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3921 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3922 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3923 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3924 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3925 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3926 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3927 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3928 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3929 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3930 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3931 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3932 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3933 \tTraining Loss: 2.178 \tTraining Accuracy: 0.206\n",
      "Epoch: 3934 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3935 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3936 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3937 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3938 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3939 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3940 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3941 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3942 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3943 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3944 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3945 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3946 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3947 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3948 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3949 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3950 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3951 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3952 \tTraining Loss: 2.177 \tTraining Accuracy: 0.206\n",
      "Epoch: 3953 \tTraining Loss: 2.177 \tTraining Accuracy: 0.207\n",
      "Epoch: 3954 \tTraining Loss: 2.177 \tTraining Accuracy: 0.207\n",
      "Epoch: 3955 \tTraining Loss: 2.177 \tTraining Accuracy: 0.207\n",
      "Epoch: 3956 \tTraining Loss: 2.177 \tTraining Accuracy: 0.207\n",
      "Epoch: 3957 \tTraining Loss: 2.177 \tTraining Accuracy: 0.207\n",
      "Epoch: 3958 \tTraining Loss: 2.177 \tTraining Accuracy: 0.207\n",
      "Epoch: 3959 \tTraining Loss: 2.177 \tTraining Accuracy: 0.207\n",
      "Epoch: 3960 \tTraining Loss: 2.177 \tTraining Accuracy: 0.207\n",
      "Epoch: 3961 \tTraining Loss: 2.177 \tTraining Accuracy: 0.207\n",
      "Epoch: 3962 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3963 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3964 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3965 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3966 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3967 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3968 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3969 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3970 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3971 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3972 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3973 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3974 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3975 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3976 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3977 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3978 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3979 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3980 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3981 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3982 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3983 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3984 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3985 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3986 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3987 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3988 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3989 \tTraining Loss: 2.176 \tTraining Accuracy: 0.207\n",
      "Epoch: 3990 \tTraining Loss: 2.175 \tTraining Accuracy: 0.207\n",
      "Epoch: 3991 \tTraining Loss: 2.175 \tTraining Accuracy: 0.207\n",
      "Epoch: 3992 \tTraining Loss: 2.175 \tTraining Accuracy: 0.207\n",
      "Epoch: 3993 \tTraining Loss: 2.175 \tTraining Accuracy: 0.207\n",
      "Epoch: 3994 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 3995 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 3996 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 3997 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 3998 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 3999 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4000 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4001 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4002 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4003 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4004 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4005 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4006 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4007 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4008 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4009 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4010 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4011 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4012 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4013 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4014 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4015 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4016 \tTraining Loss: 2.175 \tTraining Accuracy: 0.208\n",
      "Epoch: 4017 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4018 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4019 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4020 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4021 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4022 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4023 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4024 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4025 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4026 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4027 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4028 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4029 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4030 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4031 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4032 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4033 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4034 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4035 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4036 \tTraining Loss: 2.174 \tTraining Accuracy: 0.208\n",
      "Epoch: 4037 \tTraining Loss: 2.174 \tTraining Accuracy: 0.209\n",
      "Epoch: 4038 \tTraining Loss: 2.174 \tTraining Accuracy: 0.209\n",
      "Epoch: 4039 \tTraining Loss: 2.174 \tTraining Accuracy: 0.209\n",
      "Epoch: 4040 \tTraining Loss: 2.174 \tTraining Accuracy: 0.209\n",
      "Epoch: 4041 \tTraining Loss: 2.174 \tTraining Accuracy: 0.209\n",
      "Epoch: 4042 \tTraining Loss: 2.174 \tTraining Accuracy: 0.209\n",
      "Epoch: 4043 \tTraining Loss: 2.174 \tTraining Accuracy: 0.209\n",
      "Epoch: 4044 \tTraining Loss: 2.174 \tTraining Accuracy: 0.209\n",
      "Epoch: 4045 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4046 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4047 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4048 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4049 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4050 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4051 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4052 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4053 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4054 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4055 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4056 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4057 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4058 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4059 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4060 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4061 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4062 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4063 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4064 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4065 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4066 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4067 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4068 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4069 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4070 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4071 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4072 \tTraining Loss: 2.173 \tTraining Accuracy: 0.209\n",
      "Epoch: 4073 \tTraining Loss: 2.172 \tTraining Accuracy: 0.209\n",
      "Epoch: 4074 \tTraining Loss: 2.172 \tTraining Accuracy: 0.209\n",
      "Epoch: 4075 \tTraining Loss: 2.172 \tTraining Accuracy: 0.209\n",
      "Epoch: 4076 \tTraining Loss: 2.172 \tTraining Accuracy: 0.209\n",
      "Epoch: 4077 \tTraining Loss: 2.172 \tTraining Accuracy: 0.209\n",
      "Epoch: 4078 \tTraining Loss: 2.172 \tTraining Accuracy: 0.209\n",
      "Epoch: 4079 \tTraining Loss: 2.172 \tTraining Accuracy: 0.209\n",
      "Epoch: 4080 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4081 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4082 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4083 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4084 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4085 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4086 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4087 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4088 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4089 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4090 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4091 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4092 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4093 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4094 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4095 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4096 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4097 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4098 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4099 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4100 \tTraining Loss: 2.172 \tTraining Accuracy: 0.210\n",
      "Epoch: 4101 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4102 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4103 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4104 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4105 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4106 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4107 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4108 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4109 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4110 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4111 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4112 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4113 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4114 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4115 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4116 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4117 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4118 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4119 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4120 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4121 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4122 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4123 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4124 \tTraining Loss: 2.171 \tTraining Accuracy: 0.210\n",
      "Epoch: 4125 \tTraining Loss: 2.171 \tTraining Accuracy: 0.211\n",
      "Epoch: 4126 \tTraining Loss: 2.171 \tTraining Accuracy: 0.211\n",
      "Epoch: 4127 \tTraining Loss: 2.171 \tTraining Accuracy: 0.211\n",
      "Epoch: 4128 \tTraining Loss: 2.171 \tTraining Accuracy: 0.211\n",
      "Epoch: 4129 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4130 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4131 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4132 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4133 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4134 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4135 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4136 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4137 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4138 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4139 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4140 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4141 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4142 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4143 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4144 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4145 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4146 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4147 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4148 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4149 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4150 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4151 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4152 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4153 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4154 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4155 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4156 \tTraining Loss: 2.170 \tTraining Accuracy: 0.211\n",
      "Epoch: 4157 \tTraining Loss: 2.169 \tTraining Accuracy: 0.211\n",
      "Epoch: 4158 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4159 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4160 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4161 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4162 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4163 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4164 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4165 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4166 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4167 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4168 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4169 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4170 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4171 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4172 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4173 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4174 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4175 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4176 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4177 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4178 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4179 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4180 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4181 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4182 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4183 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4184 \tTraining Loss: 2.169 \tTraining Accuracy: 0.212\n",
      "Epoch: 4185 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4186 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4187 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4188 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4189 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4190 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4191 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4192 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4193 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4194 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4195 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4196 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4197 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4198 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4199 \tTraining Loss: 2.168 \tTraining Accuracy: 0.212\n",
      "Epoch: 4200 \tTraining Loss: 2.168 \tTraining Accuracy: 0.213\n",
      "Epoch: 4201 \tTraining Loss: 2.168 \tTraining Accuracy: 0.213\n",
      "Epoch: 4202 \tTraining Loss: 2.168 \tTraining Accuracy: 0.213\n",
      "Epoch: 4203 \tTraining Loss: 2.168 \tTraining Accuracy: 0.213\n",
      "Epoch: 4204 \tTraining Loss: 2.168 \tTraining Accuracy: 0.213\n",
      "Epoch: 4205 \tTraining Loss: 2.168 \tTraining Accuracy: 0.213\n",
      "Epoch: 4206 \tTraining Loss: 2.168 \tTraining Accuracy: 0.213\n",
      "Epoch: 4207 \tTraining Loss: 2.168 \tTraining Accuracy: 0.213\n",
      "Epoch: 4208 \tTraining Loss: 2.168 \tTraining Accuracy: 0.213\n",
      "Epoch: 4209 \tTraining Loss: 2.168 \tTraining Accuracy: 0.213\n",
      "Epoch: 4210 \tTraining Loss: 2.168 \tTraining Accuracy: 0.213\n",
      "Epoch: 4211 \tTraining Loss: 2.168 \tTraining Accuracy: 0.213\n",
      "Epoch: 4212 \tTraining Loss: 2.168 \tTraining Accuracy: 0.213\n",
      "Epoch: 4213 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4214 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4215 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4216 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4217 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4218 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4219 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4220 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4221 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4222 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4223 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4224 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4225 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4226 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4227 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4228 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4229 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4230 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4231 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4232 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4233 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4234 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4235 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4236 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4237 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4238 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4239 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4240 \tTraining Loss: 2.167 \tTraining Accuracy: 0.213\n",
      "Epoch: 4241 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4242 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4243 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4244 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4245 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4246 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4247 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4248 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4249 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4250 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4251 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4252 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4253 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4254 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4255 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4256 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4257 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4258 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4259 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4260 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4261 \tTraining Loss: 2.166 \tTraining Accuracy: 0.213\n",
      "Epoch: 4262 \tTraining Loss: 2.166 \tTraining Accuracy: 0.214\n",
      "Epoch: 4263 \tTraining Loss: 2.166 \tTraining Accuracy: 0.214\n",
      "Epoch: 4264 \tTraining Loss: 2.166 \tTraining Accuracy: 0.214\n",
      "Epoch: 4265 \tTraining Loss: 2.166 \tTraining Accuracy: 0.214\n",
      "Epoch: 4266 \tTraining Loss: 2.166 \tTraining Accuracy: 0.214\n",
      "Epoch: 4267 \tTraining Loss: 2.166 \tTraining Accuracy: 0.214\n",
      "Epoch: 4268 \tTraining Loss: 2.166 \tTraining Accuracy: 0.214\n",
      "Epoch: 4269 \tTraining Loss: 2.166 \tTraining Accuracy: 0.214\n",
      "Epoch: 4270 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4271 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4272 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4273 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4274 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4275 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4276 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4277 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4278 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4279 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4280 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4281 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4282 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4283 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4284 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4285 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4286 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4287 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4288 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4289 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4290 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4291 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4292 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4293 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4294 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4295 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4296 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4297 \tTraining Loss: 2.165 \tTraining Accuracy: 0.214\n",
      "Epoch: 4298 \tTraining Loss: 2.164 \tTraining Accuracy: 0.214\n",
      "Epoch: 4299 \tTraining Loss: 2.164 \tTraining Accuracy: 0.214\n",
      "Epoch: 4300 \tTraining Loss: 2.164 \tTraining Accuracy: 0.214\n",
      "Epoch: 4301 \tTraining Loss: 2.164 \tTraining Accuracy: 0.214\n",
      "Epoch: 4302 \tTraining Loss: 2.164 \tTraining Accuracy: 0.214\n",
      "Epoch: 4303 \tTraining Loss: 2.164 \tTraining Accuracy: 0.214\n",
      "Epoch: 4304 \tTraining Loss: 2.164 \tTraining Accuracy: 0.214\n",
      "Epoch: 4305 \tTraining Loss: 2.164 \tTraining Accuracy: 0.214\n",
      "Epoch: 4306 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4307 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4308 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4309 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4310 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4311 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4312 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4313 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4314 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4315 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4316 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4317 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4318 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4319 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4320 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4321 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4322 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4323 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4324 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4325 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4326 \tTraining Loss: 2.164 \tTraining Accuracy: 0.215\n",
      "Epoch: 4327 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4328 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4329 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4330 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4331 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4332 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4333 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4334 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4335 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4336 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4337 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4338 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4339 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4340 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4341 \tTraining Loss: 2.163 \tTraining Accuracy: 0.215\n",
      "Epoch: 4342 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4343 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4344 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4345 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4346 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4347 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4348 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4349 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4350 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4351 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4352 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4353 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4354 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4355 \tTraining Loss: 2.163 \tTraining Accuracy: 0.216\n",
      "Epoch: 4356 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4357 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4358 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4359 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4360 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4361 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4362 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4363 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4364 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4365 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4366 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4367 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4368 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4369 \tTraining Loss: 2.162 \tTraining Accuracy: 0.216\n",
      "Epoch: 4370 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4371 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4372 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4373 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4374 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4375 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4376 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4377 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4378 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4379 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4380 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4381 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4382 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4383 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4384 \tTraining Loss: 2.162 \tTraining Accuracy: 0.217\n",
      "Epoch: 4385 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4386 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4387 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4388 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4389 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4390 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4391 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4392 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4393 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4394 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4395 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4396 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4397 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4398 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4399 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4400 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4401 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4402 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4403 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4404 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4405 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4406 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4407 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4408 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4409 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4410 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4411 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4412 \tTraining Loss: 2.161 \tTraining Accuracy: 0.217\n",
      "Epoch: 4413 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4414 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4415 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4416 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4417 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4418 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4419 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4420 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4421 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4422 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4423 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4424 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4425 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4426 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4427 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4428 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4429 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4430 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4431 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4432 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4433 \tTraining Loss: 2.160 \tTraining Accuracy: 0.217\n",
      "Epoch: 4434 \tTraining Loss: 2.160 \tTraining Accuracy: 0.218\n",
      "Epoch: 4435 \tTraining Loss: 2.160 \tTraining Accuracy: 0.218\n",
      "Epoch: 4436 \tTraining Loss: 2.160 \tTraining Accuracy: 0.218\n",
      "Epoch: 4437 \tTraining Loss: 2.160 \tTraining Accuracy: 0.218\n",
      "Epoch: 4438 \tTraining Loss: 2.160 \tTraining Accuracy: 0.218\n",
      "Epoch: 4439 \tTraining Loss: 2.160 \tTraining Accuracy: 0.218\n",
      "Epoch: 4440 \tTraining Loss: 2.160 \tTraining Accuracy: 0.218\n",
      "Epoch: 4441 \tTraining Loss: 2.160 \tTraining Accuracy: 0.218\n",
      "Epoch: 4442 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4443 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4444 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4445 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4446 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4447 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4448 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4449 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4450 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4451 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4452 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4453 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4454 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4455 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4456 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4457 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4458 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4459 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4460 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4461 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4462 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4463 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4464 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4465 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4466 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4467 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4468 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4469 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4470 \tTraining Loss: 2.159 \tTraining Accuracy: 0.218\n",
      "Epoch: 4471 \tTraining Loss: 2.158 \tTraining Accuracy: 0.218\n",
      "Epoch: 4472 \tTraining Loss: 2.158 \tTraining Accuracy: 0.218\n",
      "Epoch: 4473 \tTraining Loss: 2.158 \tTraining Accuracy: 0.218\n",
      "Epoch: 4474 \tTraining Loss: 2.158 \tTraining Accuracy: 0.218\n",
      "Epoch: 4475 \tTraining Loss: 2.158 \tTraining Accuracy: 0.218\n",
      "Epoch: 4476 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4477 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4478 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4479 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4480 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4481 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4482 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4483 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4484 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4485 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4486 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4487 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4488 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4489 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4490 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4491 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4492 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4493 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4494 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4495 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4496 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4497 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4498 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4499 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4500 \tTraining Loss: 2.158 \tTraining Accuracy: 0.219\n",
      "Epoch: 4501 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4502 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4503 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4504 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4505 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4506 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4507 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4508 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4509 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4510 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4511 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4512 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4513 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4514 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4515 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4516 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4517 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4518 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4519 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4520 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4521 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4522 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4523 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4524 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4525 \tTraining Loss: 2.157 \tTraining Accuracy: 0.219\n",
      "Epoch: 4526 \tTraining Loss: 2.157 \tTraining Accuracy: 0.220\n",
      "Epoch: 4527 \tTraining Loss: 2.157 \tTraining Accuracy: 0.220\n",
      "Epoch: 4528 \tTraining Loss: 2.157 \tTraining Accuracy: 0.220\n",
      "Epoch: 4529 \tTraining Loss: 2.157 \tTraining Accuracy: 0.220\n",
      "Epoch: 4530 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4531 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4532 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4533 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4534 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4535 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4536 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4537 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4538 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4539 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4540 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4541 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4542 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4543 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4544 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4545 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4546 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4547 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4548 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4549 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4550 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4551 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4552 \tTraining Loss: 2.156 \tTraining Accuracy: 0.220\n",
      "Epoch: 4553 \tTraining Loss: 2.156 \tTraining Accuracy: 0.221\n",
      "Epoch: 4554 \tTraining Loss: 2.156 \tTraining Accuracy: 0.221\n",
      "Epoch: 4555 \tTraining Loss: 2.156 \tTraining Accuracy: 0.221\n",
      "Epoch: 4556 \tTraining Loss: 2.156 \tTraining Accuracy: 0.221\n",
      "Epoch: 4557 \tTraining Loss: 2.156 \tTraining Accuracy: 0.221\n",
      "Epoch: 4558 \tTraining Loss: 2.156 \tTraining Accuracy: 0.221\n",
      "Epoch: 4559 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4560 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4561 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4562 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4563 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4564 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4565 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4566 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4567 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4568 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4569 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4570 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4571 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4572 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4573 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4574 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4575 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4576 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4577 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4578 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4579 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4580 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4581 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4582 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4583 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4584 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4585 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4586 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4587 \tTraining Loss: 2.155 \tTraining Accuracy: 0.221\n",
      "Epoch: 4588 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4589 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4590 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4591 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4592 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4593 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4594 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4595 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4596 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4597 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4598 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4599 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4600 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4601 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4602 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4603 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4604 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4605 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4606 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4607 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4608 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4609 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4610 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4611 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4612 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4613 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4614 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4615 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4616 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4617 \tTraining Loss: 2.154 \tTraining Accuracy: 0.221\n",
      "Epoch: 4618 \tTraining Loss: 2.153 \tTraining Accuracy: 0.221\n",
      "Epoch: 4619 \tTraining Loss: 2.153 \tTraining Accuracy: 0.221\n",
      "Epoch: 4620 \tTraining Loss: 2.153 \tTraining Accuracy: 0.221\n",
      "Epoch: 4621 \tTraining Loss: 2.153 \tTraining Accuracy: 0.221\n",
      "Epoch: 4622 \tTraining Loss: 2.153 \tTraining Accuracy: 0.221\n",
      "Epoch: 4623 \tTraining Loss: 2.153 \tTraining Accuracy: 0.221\n",
      "Epoch: 4624 \tTraining Loss: 2.153 \tTraining Accuracy: 0.221\n",
      "Epoch: 4625 \tTraining Loss: 2.153 \tTraining Accuracy: 0.221\n",
      "Epoch: 4626 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4627 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4628 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4629 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4630 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4631 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4632 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4633 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4634 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4635 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4636 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4637 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4638 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4639 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4640 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4641 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4642 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4643 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4644 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4645 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4646 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4647 \tTraining Loss: 2.153 \tTraining Accuracy: 0.222\n",
      "Epoch: 4648 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4649 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4650 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4651 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4652 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4653 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4654 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4655 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4656 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4657 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4658 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4659 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4660 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4661 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4662 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4663 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4664 \tTraining Loss: 2.152 \tTraining Accuracy: 0.222\n",
      "Epoch: 4665 \tTraining Loss: 2.152 \tTraining Accuracy: 0.223\n",
      "Epoch: 4666 \tTraining Loss: 2.152 \tTraining Accuracy: 0.223\n",
      "Epoch: 4667 \tTraining Loss: 2.152 \tTraining Accuracy: 0.223\n",
      "Epoch: 4668 \tTraining Loss: 2.152 \tTraining Accuracy: 0.223\n",
      "Epoch: 4669 \tTraining Loss: 2.152 \tTraining Accuracy: 0.223\n",
      "Epoch: 4670 \tTraining Loss: 2.152 \tTraining Accuracy: 0.223\n",
      "Epoch: 4671 \tTraining Loss: 2.152 \tTraining Accuracy: 0.223\n",
      "Epoch: 4672 \tTraining Loss: 2.152 \tTraining Accuracy: 0.223\n",
      "Epoch: 4673 \tTraining Loss: 2.152 \tTraining Accuracy: 0.223\n",
      "Epoch: 4674 \tTraining Loss: 2.152 \tTraining Accuracy: 0.223\n",
      "Epoch: 4675 \tTraining Loss: 2.152 \tTraining Accuracy: 0.223\n",
      "Epoch: 4676 \tTraining Loss: 2.152 \tTraining Accuracy: 0.223\n",
      "Epoch: 4677 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4678 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4679 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4680 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4681 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4682 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4683 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4684 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4685 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4686 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4687 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4688 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4689 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4690 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4691 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4692 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4693 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4694 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4695 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4696 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4697 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4698 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4699 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4700 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4701 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4702 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4703 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4704 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4705 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4706 \tTraining Loss: 2.151 \tTraining Accuracy: 0.223\n",
      "Epoch: 4707 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4708 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4709 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4710 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4711 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4712 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4713 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4714 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4715 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4716 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4717 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4718 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4719 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4720 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4721 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4722 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4723 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4724 \tTraining Loss: 2.150 \tTraining Accuracy: 0.223\n",
      "Epoch: 4725 \tTraining Loss: 2.150 \tTraining Accuracy: 0.224\n",
      "Epoch: 4726 \tTraining Loss: 2.150 \tTraining Accuracy: 0.224\n",
      "Epoch: 4727 \tTraining Loss: 2.150 \tTraining Accuracy: 0.224\n",
      "Epoch: 4728 \tTraining Loss: 2.150 \tTraining Accuracy: 0.224\n",
      "Epoch: 4729 \tTraining Loss: 2.150 \tTraining Accuracy: 0.224\n",
      "Epoch: 4730 \tTraining Loss: 2.150 \tTraining Accuracy: 0.224\n",
      "Epoch: 4731 \tTraining Loss: 2.150 \tTraining Accuracy: 0.224\n",
      "Epoch: 4732 \tTraining Loss: 2.150 \tTraining Accuracy: 0.224\n",
      "Epoch: 4733 \tTraining Loss: 2.150 \tTraining Accuracy: 0.224\n",
      "Epoch: 4734 \tTraining Loss: 2.150 \tTraining Accuracy: 0.224\n",
      "Epoch: 4735 \tTraining Loss: 2.150 \tTraining Accuracy: 0.224\n",
      "Epoch: 4736 \tTraining Loss: 2.150 \tTraining Accuracy: 0.224\n",
      "Epoch: 4737 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4738 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4739 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4740 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4741 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4742 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4743 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4744 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4745 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4746 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4747 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4748 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4749 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4750 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4751 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4752 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4753 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4754 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4755 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4756 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4757 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4758 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4759 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4760 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4761 \tTraining Loss: 2.149 \tTraining Accuracy: 0.224\n",
      "Epoch: 4762 \tTraining Loss: 2.149 \tTraining Accuracy: 0.225\n",
      "Epoch: 4763 \tTraining Loss: 2.149 \tTraining Accuracy: 0.225\n",
      "Epoch: 4764 \tTraining Loss: 2.149 \tTraining Accuracy: 0.225\n",
      "Epoch: 4765 \tTraining Loss: 2.149 \tTraining Accuracy: 0.225\n",
      "Epoch: 4766 \tTraining Loss: 2.149 \tTraining Accuracy: 0.225\n",
      "Epoch: 4767 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4768 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4769 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4770 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4771 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4772 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4773 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4774 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4775 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4776 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4777 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4778 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4779 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4780 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4781 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4782 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4783 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4784 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4785 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4786 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4787 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4788 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4789 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4790 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4791 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4792 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4793 \tTraining Loss: 2.148 \tTraining Accuracy: 0.225\n",
      "Epoch: 4794 \tTraining Loss: 2.148 \tTraining Accuracy: 0.226\n",
      "Epoch: 4795 \tTraining Loss: 2.148 \tTraining Accuracy: 0.226\n",
      "Epoch: 4796 \tTraining Loss: 2.148 \tTraining Accuracy: 0.226\n",
      "Epoch: 4797 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4798 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4799 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4800 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4801 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4802 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4803 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4804 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4805 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4806 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4807 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4808 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4809 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4810 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4811 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4812 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4813 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4814 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4815 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4816 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4817 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4818 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4819 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4820 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4821 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4822 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4823 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4824 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4825 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4826 \tTraining Loss: 2.147 \tTraining Accuracy: 0.226\n",
      "Epoch: 4827 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4828 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4829 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4830 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4831 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4832 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4833 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4834 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4835 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4836 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4837 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4838 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4839 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4840 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4841 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4842 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4843 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4844 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4845 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4846 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4847 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4848 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4849 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4850 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4851 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4852 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4853 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4854 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4855 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4856 \tTraining Loss: 2.146 \tTraining Accuracy: 0.226\n",
      "Epoch: 4857 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4858 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4859 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4860 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4861 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4862 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4863 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4864 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4865 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4866 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4867 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4868 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4869 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4870 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4871 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4872 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4873 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4874 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4875 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4876 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4877 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4878 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4879 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4880 \tTraining Loss: 2.145 \tTraining Accuracy: 0.226\n",
      "Epoch: 4881 \tTraining Loss: 2.145 \tTraining Accuracy: 0.227\n",
      "Epoch: 4882 \tTraining Loss: 2.145 \tTraining Accuracy: 0.227\n",
      "Epoch: 4883 \tTraining Loss: 2.145 \tTraining Accuracy: 0.227\n",
      "Epoch: 4884 \tTraining Loss: 2.145 \tTraining Accuracy: 0.227\n",
      "Epoch: 4885 \tTraining Loss: 2.145 \tTraining Accuracy: 0.227\n",
      "Epoch: 4886 \tTraining Loss: 2.145 \tTraining Accuracy: 0.227\n",
      "Epoch: 4887 \tTraining Loss: 2.145 \tTraining Accuracy: 0.227\n",
      "Epoch: 4888 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4889 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4890 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4891 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4892 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4893 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4894 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4895 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4896 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4897 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4898 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4899 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4900 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4901 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4902 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4903 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4904 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4905 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4906 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4907 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4908 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4909 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4910 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4911 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4912 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4913 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4914 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4915 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4916 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4917 \tTraining Loss: 2.144 \tTraining Accuracy: 0.227\n",
      "Epoch: 4918 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4919 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4920 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4921 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4922 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4923 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4924 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4925 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4926 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4927 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4928 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4929 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4930 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4931 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4932 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4933 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4934 \tTraining Loss: 2.143 \tTraining Accuracy: 0.227\n",
      "Epoch: 4935 \tTraining Loss: 2.143 \tTraining Accuracy: 0.228\n",
      "Epoch: 4936 \tTraining Loss: 2.143 \tTraining Accuracy: 0.228\n",
      "Epoch: 4937 \tTraining Loss: 2.143 \tTraining Accuracy: 0.228\n",
      "Epoch: 4938 \tTraining Loss: 2.143 \tTraining Accuracy: 0.228\n",
      "Epoch: 4939 \tTraining Loss: 2.143 \tTraining Accuracy: 0.228\n",
      "Epoch: 4940 \tTraining Loss: 2.143 \tTraining Accuracy: 0.228\n",
      "Epoch: 4941 \tTraining Loss: 2.143 \tTraining Accuracy: 0.228\n",
      "Epoch: 4942 \tTraining Loss: 2.143 \tTraining Accuracy: 0.228\n",
      "Epoch: 4943 \tTraining Loss: 2.143 \tTraining Accuracy: 0.228\n",
      "Epoch: 4944 \tTraining Loss: 2.143 \tTraining Accuracy: 0.228\n",
      "Epoch: 4945 \tTraining Loss: 2.143 \tTraining Accuracy: 0.228\n",
      "Epoch: 4946 \tTraining Loss: 2.143 \tTraining Accuracy: 0.228\n",
      "Epoch: 4947 \tTraining Loss: 2.143 \tTraining Accuracy: 0.228\n",
      "Epoch: 4948 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4949 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4950 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4951 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4952 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4953 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4954 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4955 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4956 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4957 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4958 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4959 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4960 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4961 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4962 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4963 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4964 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4965 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4966 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4967 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4968 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4969 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4970 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4971 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4972 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4973 \tTraining Loss: 2.142 \tTraining Accuracy: 0.228\n",
      "Epoch: 4974 \tTraining Loss: 2.142 \tTraining Accuracy: 0.229\n",
      "Epoch: 4975 \tTraining Loss: 2.142 \tTraining Accuracy: 0.229\n",
      "Epoch: 4976 \tTraining Loss: 2.142 \tTraining Accuracy: 0.229\n",
      "Epoch: 4977 \tTraining Loss: 2.142 \tTraining Accuracy: 0.229\n",
      "Epoch: 4978 \tTraining Loss: 2.142 \tTraining Accuracy: 0.229\n",
      "Epoch: 4979 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4980 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4981 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4982 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4983 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4984 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4985 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4986 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4987 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4988 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4989 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4990 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4991 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4992 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4993 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4994 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4995 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4996 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4997 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4998 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n",
      "Epoch: 4999 \tTraining Loss: 2.141 \tTraining Accuracy: 0.229\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(1, n_epochs+1):\n",
    "for epoch in range(1, 5000):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    correct_train = 0.0\n",
    "    correct_valid = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    # g.train()\n",
    "    model.train()\n",
    "\n",
    "    for k, v in fd_d.items():\n",
    "\n",
    "        for i in range(0, len(v), 1000):\n",
    "            data, target = v[i:i+1000].type(torch.float).cuda(), torch.full((1000,), k).cuda()\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            # output = g(data.reshape((1000, 2048, 1, 1)))\n",
    "            output = model.classifier(data.reshape((1000, 2048, 1, 1)))\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += (loss.data.item() * data.shape[0])\n",
    "            # print('outputs on which to apply torch.max ', prediction)\n",
    "            # find the maximum along the rows, use dim=1 to torch.max()\n",
    "            _, predicted_outputs = torch.max(output.data, 1)\n",
    "            # Update the running corrects\n",
    "            correct_train += (predicted_outputs == target).float().sum().item()\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    # calculate accuracies\n",
    "    train_acc =  correct_train / len(train_loader.sampler)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.3f} \\tTraining Accuracy: {:.3f}'.format(\n",
    "        epoch, train_loss, train_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def test_inference(model, test_dataset, gpu=1, local_batch_size=10, loss_function=\"CrossEntropyLoss\"):\n",
    "    \"\"\"\n",
    "    Returns the test accuracy and loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # f.eval()\n",
    "    # g.eval()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "\n",
    "    device = 'cuda' if gpu else 'cpu'\n",
    "    if loss_function == \"NLLLoss\":\n",
    "        criterion = nn.NLLLoss()\n",
    "    if loss_function == \"CrossEntropyLoss\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    testloader = DataLoader(test_dataset, batch_size=10,\n",
    "                            shuffle=False, generator=generator)\n",
    "\n",
    "    for images, labels in testloader:\n",
    "    # for k, v in fd_d.items():\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # images, labels = v.type(torch.float).cuda().reshape(1000, 2048, 1, 1), torch.full((1000,), k).cuda()\n",
    "\n",
    "        # Inference\n",
    "        # output_f = f(images)[\"avg_pool2d\"]\n",
    "        # output_f = f(images.cuda())\n",
    "        # output_g = g(output_f)\n",
    "        output_g = model(images.cuda())\n",
    "\n",
    "        loss = criterion(output_g, labels)\n",
    "        test_loss += (loss.data.item() * images.shape[0])\n",
    "\n",
    "        # Prediction\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output_g, 1)\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            label = labels.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    # average test loss\n",
    "    test_loss = test_loss / len(testloader.dataset)\n",
    "\n",
    "    accuracy = np.sum(class_correct) / np.sum(class_total)\n",
    "\n",
    "    return accuracy, test_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results after 100 global rounds of training:\n",
      "|---- Test Accuracy: 60.61%\n"
     ]
    }
   ],
   "source": [
    "# test the trained model\n",
    "\n",
    "test_acc, test_loss = test_inference(model, test_dataset=test_dataset, gpu=gpu,\n",
    "                                     loss_function=loss_function)\n",
    "\n",
    "print(f'\\nResults after {n_epochs} global rounds of training:')\n",
    "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "filename_pt = \"CCVR_results/iid[{}]_unbalanced[{}]_lr[{}]_Mc[{}].pt\".format(\n",
    "    iid, unbalanced, lr, n_virtual_samples)\n",
    "torch.save(model.state_dict(), filename_pt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}