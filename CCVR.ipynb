{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from models import ResNet50\n",
    "from models_CCVR import feature_extractor, classifier\n",
    "from update import LocalUpdate, test_inference"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# parameters\n",
    "iid = 1 # if the data is i.i.d or not\n",
    "unbalanced = 0 # in non i.i.d. setting split the data between clients equally or not\n",
    "num_users = 100 # number of client\n",
    "frac = 0.1 # fraction of the clients to be used for federated updates\n",
    "n_epochs = 100\n",
    "gpu = 0\n",
    "optimizer = \"sgd\" #sgd or adam\n",
    "local_batch_size = 10 # batch size of local updates in each user\n",
    "lr = 0.001 # learning rate\n",
    "local_epochs = 10\n",
    "loss_function = \"CrossEntropyLoss\"\n",
    "\n",
    "# percentage = 90  # percentage of strugglers\n",
    "\n",
    "num_groups = 0  # 0 for BatchNorm, > 0 for GroupNorm\n",
    "if num_groups == 0:\n",
    "    normalization_type = \"BatchNorm\"\n",
    "else:\n",
    "    normalization_type = \"GroupNorm\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if iid:\n",
    "    from utils_v2 import get_dataset, average_weights, weighted_average_weights, exp_details\n",
    "else:\n",
    "    from utils import get_dataset, average_weights, weighted_average_weights, exp_details"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# for REPRODUCIBILITY https://pytorch.org/docs/stable/notes/randomness.html\n",
    "torch.manual_seed(0)\n",
    "\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(0)\n",
    "\n",
    "np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"\n",
    "    An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, idxs):\n",
    "    trainloader = DataLoader(DatasetSplit(dataset, idxs),\n",
    "                             batch_size=None, shuffle=True, generator=generator,\n",
    "                             worker_init_fn=seed_worker)\n",
    "\n",
    "    return trainloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, user_groups = get_dataset(iid=iid, unbalanced=unbalanced,\n",
    "                                                       num_users=num_users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    },
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (linear): Linear(in_features=2048, out_features=10, bias=True)\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50(n_type=normalization_type)\n",
    "# model = CNNCifar()\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "    device = torch.device(\"cpu\")\n",
    "    gpu = 0\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpu = 1\n",
    "\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filename_pt = \"fedAVG_results/weighted_average/ResNet50_100_sgd_lr_[0.001]_C[0.1]_iid[0]_unbalanced[1]_E[1]_B[10]_BatchNorm_numGroups[0].pt\"\n",
    "\n",
    "# filename_pt = \"fedAVG_results/ResNet50_100_sgd_lr_[0.001]_C[0.1]_iid[0]_unbalanced[0]_E[1]_B[10]_BatchNorm_numGroups[0].pt\"\n",
    "\n",
    "filename_pt = \"fedAVG_results/ResNet50_100_sgd_lr_[0.001]_C[0.1]_iid[1]_unbalanced[0]_E[1]_B[10]_BatchNorm_numGroups[0].pt\"\n",
    "\n",
    "# load saved model (i.e. the one with the smallest validation loss)\n",
    "model.load_state_dict(torch.load(filename_pt))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = feature_extractor()\n",
    "f.to(device)\n",
    "\n",
    "f.load_state_dict(torch.load(filename_pt))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# def test_inference(model, test_dataset, gpu=1, local_batch_size=10, loss_function=\"NLLLoss\"):\n",
    "#     \"\"\"\n",
    "#     Returns the test accuracy and loss.\n",
    "#     \"\"\"\n",
    "#\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     class_correct = list(0. for i in range(10))\n",
    "#     class_total = list(0. for i in range(10))\n",
    "#\n",
    "#     device = 'cuda' if gpu else 'cpu'\n",
    "#     if loss_function == \"NLLLoss\":\n",
    "#         criterion = nn.NLLLoss()\n",
    "#     if loss_function == \"CrossEntropyLoss\":\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#\n",
    "#     testloader = DataLoader(test_dataset, batch_size=local_batch_size,\n",
    "#                             shuffle=False, generator=generator)\n",
    "#\n",
    "#     for images, labels in testloader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#\n",
    "#         # Inference\n",
    "#         output = model(images)\n",
    "#         loss = criterion(output, labels)\n",
    "#         test_loss += (loss.data.item() * images.shape[0])\n",
    "#\n",
    "#         # Prediction\n",
    "#         # convert output probabilities to predicted class\n",
    "#         _, pred = torch.max(output, 1)\n",
    "#         # compare predictions to true label\n",
    "#         correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "#         correct = np.squeeze(correct_tensor.numpy()) if not gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "#\n",
    "#         for i in range(len(images)):\n",
    "#             label = labels.data[i]\n",
    "#             class_correct[label] += correct[i].item()\n",
    "#             class_total[label] += 1\n",
    "#\n",
    "#     # average test loss\n",
    "#     test_loss = test_loss / len(testloader.dataset)\n",
    "#\n",
    "#     accuracy = np.sum(class_correct) / np.sum(class_total)\n",
    "#\n",
    "#     return accuracy, test_loss\n",
    "#\n",
    "#\n",
    "# # test the trained model\n",
    "#\n",
    "# test_acc, test_loss = test_inference(model=model, test_dataset=test_dataset, gpu=gpu,\n",
    "#                                      loss_function=loss_function)\n",
    "#\n",
    "# print(f'\\nResults after {n_epochs} global rounds of training:')\n",
    "# print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "#\n",
    "# f = nn.Sequential(\n",
    "#     # stop at conv4\n",
    "#     *list(model.children())[:-1]\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]           4,096\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "            Conv2d-7          [-1, 256, 32, 32]          16,384\n",
      "       BatchNorm2d-8          [-1, 256, 32, 32]             512\n",
      "            Conv2d-9          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
      "       Bottleneck-11          [-1, 256, 32, 32]               0\n",
      "           Conv2d-12           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-13           [-1, 64, 32, 32]             128\n",
      "           Conv2d-14           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
      "           Conv2d-16          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-17          [-1, 256, 32, 32]             512\n",
      "       Bottleneck-18          [-1, 256, 32, 32]               0\n",
      "           Conv2d-19           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
      "           Conv2d-21           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 32, 32]             128\n",
      "           Conv2d-23          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 32, 32]             512\n",
      "       Bottleneck-25          [-1, 256, 32, 32]               0\n",
      "           Conv2d-26          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-27          [-1, 128, 32, 32]             256\n",
      "           Conv2d-28          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 16, 16]             256\n",
      "           Conv2d-30          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-31          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-32          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-34          [-1, 512, 16, 16]               0\n",
      "           Conv2d-35          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-36          [-1, 128, 16, 16]             256\n",
      "           Conv2d-37          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-38          [-1, 128, 16, 16]             256\n",
      "           Conv2d-39          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-40          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-41          [-1, 512, 16, 16]               0\n",
      "           Conv2d-42          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-43          [-1, 128, 16, 16]             256\n",
      "           Conv2d-44          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-45          [-1, 128, 16, 16]             256\n",
      "           Conv2d-46          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-47          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-48          [-1, 512, 16, 16]               0\n",
      "           Conv2d-49          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 16, 16]             256\n",
      "           Conv2d-51          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-52          [-1, 128, 16, 16]             256\n",
      "           Conv2d-53          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-54          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-55          [-1, 512, 16, 16]               0\n",
      "           Conv2d-56          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-57          [-1, 256, 16, 16]             512\n",
      "           Conv2d-58            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-59            [-1, 256, 8, 8]             512\n",
      "           Conv2d-60           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-61           [-1, 1024, 8, 8]           2,048\n",
      "           Conv2d-62           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-63           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-64           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-65            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-66            [-1, 256, 8, 8]             512\n",
      "           Conv2d-67            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-68            [-1, 256, 8, 8]             512\n",
      "           Conv2d-69           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-70           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-71           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-72            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-73            [-1, 256, 8, 8]             512\n",
      "           Conv2d-74            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-75            [-1, 256, 8, 8]             512\n",
      "           Conv2d-76           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-77           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-78           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-79            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-80            [-1, 256, 8, 8]             512\n",
      "           Conv2d-81            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-82            [-1, 256, 8, 8]             512\n",
      "           Conv2d-83           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-84           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-85           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-86            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-87            [-1, 256, 8, 8]             512\n",
      "           Conv2d-88            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-89            [-1, 256, 8, 8]             512\n",
      "           Conv2d-90           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-91           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-92           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-93            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-94            [-1, 256, 8, 8]             512\n",
      "           Conv2d-95            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-96            [-1, 256, 8, 8]             512\n",
      "           Conv2d-97           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-99           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-100            [-1, 512, 8, 8]         524,288\n",
      "     BatchNorm2d-101            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-102            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-103            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-104           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-105           [-1, 2048, 4, 4]           4,096\n",
      "          Conv2d-106           [-1, 2048, 4, 4]       2,097,152\n",
      "     BatchNorm2d-107           [-1, 2048, 4, 4]           4,096\n",
      "      Bottleneck-108           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-109            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-110            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-111            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-112            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-113           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-114           [-1, 2048, 4, 4]           4,096\n",
      "      Bottleneck-115           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-116            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-117            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-118            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-119            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-120           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-121           [-1, 2048, 4, 4]           4,096\n",
      "      Bottleneck-122           [-1, 2048, 4, 4]               0\n",
      "================================================================\n",
      "Total params: 23,500,352\n",
      "Trainable params: 23,500,352\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 66.12\n",
      "Params size (MB): 89.65\n",
      "Estimated Total Size (MB): 155.78\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(f, (3, 32, 32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]           4,096\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "            Conv2d-7          [-1, 256, 32, 32]          16,384\n",
      "       BatchNorm2d-8          [-1, 256, 32, 32]             512\n",
      "            Conv2d-9          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
      "       Bottleneck-11          [-1, 256, 32, 32]               0\n",
      "           Conv2d-12           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-13           [-1, 64, 32, 32]             128\n",
      "           Conv2d-14           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
      "           Conv2d-16          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-17          [-1, 256, 32, 32]             512\n",
      "       Bottleneck-18          [-1, 256, 32, 32]               0\n",
      "           Conv2d-19           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
      "           Conv2d-21           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 32, 32]             128\n",
      "           Conv2d-23          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 32, 32]             512\n",
      "       Bottleneck-25          [-1, 256, 32, 32]               0\n",
      "           Conv2d-26          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-27          [-1, 128, 32, 32]             256\n",
      "           Conv2d-28          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 16, 16]             256\n",
      "           Conv2d-30          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-31          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-32          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-34          [-1, 512, 16, 16]               0\n",
      "           Conv2d-35          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-36          [-1, 128, 16, 16]             256\n",
      "           Conv2d-37          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-38          [-1, 128, 16, 16]             256\n",
      "           Conv2d-39          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-40          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-41          [-1, 512, 16, 16]               0\n",
      "           Conv2d-42          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-43          [-1, 128, 16, 16]             256\n",
      "           Conv2d-44          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-45          [-1, 128, 16, 16]             256\n",
      "           Conv2d-46          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-47          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-48          [-1, 512, 16, 16]               0\n",
      "           Conv2d-49          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 16, 16]             256\n",
      "           Conv2d-51          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-52          [-1, 128, 16, 16]             256\n",
      "           Conv2d-53          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-54          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-55          [-1, 512, 16, 16]               0\n",
      "           Conv2d-56          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-57          [-1, 256, 16, 16]             512\n",
      "           Conv2d-58            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-59            [-1, 256, 8, 8]             512\n",
      "           Conv2d-60           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-61           [-1, 1024, 8, 8]           2,048\n",
      "           Conv2d-62           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-63           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-64           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-65            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-66            [-1, 256, 8, 8]             512\n",
      "           Conv2d-67            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-68            [-1, 256, 8, 8]             512\n",
      "           Conv2d-69           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-70           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-71           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-72            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-73            [-1, 256, 8, 8]             512\n",
      "           Conv2d-74            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-75            [-1, 256, 8, 8]             512\n",
      "           Conv2d-76           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-77           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-78           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-79            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-80            [-1, 256, 8, 8]             512\n",
      "           Conv2d-81            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-82            [-1, 256, 8, 8]             512\n",
      "           Conv2d-83           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-84           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-85           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-86            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-87            [-1, 256, 8, 8]             512\n",
      "           Conv2d-88            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-89            [-1, 256, 8, 8]             512\n",
      "           Conv2d-90           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-91           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-92           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-93            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-94            [-1, 256, 8, 8]             512\n",
      "           Conv2d-95            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-96            [-1, 256, 8, 8]             512\n",
      "           Conv2d-97           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-99           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-100            [-1, 512, 8, 8]         524,288\n",
      "     BatchNorm2d-101            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-102            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-103            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-104           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-105           [-1, 2048, 4, 4]           4,096\n",
      "          Conv2d-106           [-1, 2048, 4, 4]       2,097,152\n",
      "     BatchNorm2d-107           [-1, 2048, 4, 4]           4,096\n",
      "      Bottleneck-108           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-109            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-110            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-111            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-112            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-113           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-114           [-1, 2048, 4, 4]           4,096\n",
      "      Bottleneck-115           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-116            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-117            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-118            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-119            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-120           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-121           [-1, 2048, 4, 4]           4,096\n",
      "      Bottleneck-122           [-1, 2048, 4, 4]               0\n",
      "          Linear-123                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 23,520,842\n",
      "Trainable params: 23,520,842\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 66.13\n",
      "Params size (MB): 89.72\n",
      "Estimated Total Size (MB): 155.86\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 32, 32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from torchvision.models.feature_extraction import get_graph_node_names\n",
    "#\n",
    "# train_nodes, eval_nodes = get_graph_node_names(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# return_nodes_featureExtractor = {\n",
    "#     # node_name: user-specified key for output dict\n",
    "#     'layer1.2.relu_2': 'layer1',\n",
    "#     'layer2.3.relu_2': 'layer2',\n",
    "#     'layer3.5.relu_2': 'layer3',\n",
    "#     'layer4.2.relu_2': 'layer4'\n",
    "# }\n",
    "#\n",
    "# return_nodes_Classifier = {\n",
    "#     'avg_pool2d' : 'avg_pool',\n",
    "#     'size' : 'size',\n",
    "#     'view' : 'view',\n",
    "#     'linear' : 'linear'\n",
    "# }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from torchvision.models.feature_extraction import create_feature_extractor\n",
    "#\n",
    "# f = create_feature_extractor(model, train_return_nodes=train_nodes[:-3], eval_return_nodes=eval_nodes[:-3])\n",
    "# # f = create_feature_extractor(model, return_nodes_featureExtractor)\n",
    "# g = create_feature_extractor(model, return_nodes_Classifier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# f(torch.rand((1, 3, 32, 32)).to(device))[\"avg_pool2d\"].reshape(-1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# g(torch.rand((1, 3, 32, 32)).to(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def clientUpdate(f, dataset, idxs, device):\n",
    "#\n",
    "#     trainloader = get_dataloader(dataset, idxs)\n",
    "#\n",
    "#     d = {}\n",
    "#     sum_ = 0\n",
    "#\n",
    "#     # extract features by category\n",
    "#     for batch_idx, (image, label) in enumerate(trainloader):\n",
    "#         sum_ += 1\n",
    "#\n",
    "#         image = image.to(device)\n",
    "#         label = int(label.cpu())\n",
    "#\n",
    "#         # feature = (f(image.reshape(1, 3, 32, 32))[\"avg_pool2d\"].reshape(-1)).cpu().detach()\n",
    "#         # feature = (f(image.reshape(1, 3, 32, 32))[\"layer4.2.relu_2\"].reshape(-1)).cpu().detach()\n",
    "#         feature = (f(image.reshape(1, 3, 32, 32)).reshape(-1)).cpu().detach()\n",
    "#\n",
    "#         if label in d.keys():\n",
    "#             d[label].append(feature)\n",
    "#         else:\n",
    "#             d[label] = [feature]\n",
    "#\n",
    "#     # mu, sigma\n",
    "#     upload_d = {}\n",
    "#\n",
    "#     # for k, v in tqdm.tqdm(d.items()):\n",
    "#     #     v_item = torch.stack(v).detach().cpu()\n",
    "#     #\n",
    "#     #     # consider the case where the sample size is too small to upload\n",
    "#     #     if len(v_item) < 10:\n",
    "#     #         continue\n",
    "#     #\n",
    "#     #     mu, sigma = v_item.mean(dim=0), v_item.var(dim=0)\n",
    "#     #     upload_d[k] = {\"mu\": mu, \"sigma\": sigma, \"N\": len(v)}\n",
    "#\n",
    "#     for k, v in tqdm.tqdm(d.items()):\n",
    "#         v_item = torch.stack(v).detach().cpu()\n",
    "#         # consider the case where the sample size is too small to upload\n",
    "#         if len(v_item) < 10:\n",
    "#             continue\n",
    "#\n",
    "#         N = len(v)\n",
    "#\n",
    "#         mu = v_item.mean(dim=0)\n",
    "#\n",
    "#         # sigma = torch.zeros((2048, 2048))\n",
    "#         sigma = torch.zeros((8192, 8192))\n",
    "#         for t in v_item:\n",
    "#             # x = (t - mu).reshape(1, 2048)\n",
    "#             x = (t - mu).reshape(1, 8192)\n",
    "#             sigma = torch.add(torch.mul(x, torch.transpose(x, 1, 0)), sigma)\n",
    "#         sigma *= 1/(N-1)\n",
    "#\n",
    "#         upload_d[k] = {\"mu\": mu, \"sigma\": sigma, \"N\": N}\n",
    "#\n",
    "#     return upload_d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clientUpdate(f, dataset, idxs, device):\n",
    "\n",
    "    trainloader = get_dataloader(dataset, idxs)\n",
    "\n",
    "    d = {}\n",
    "    sum_ = 0\n",
    "\n",
    "    # extract features by category\n",
    "    for batch_idx, (image, label) in enumerate(trainloader):\n",
    "        sum_ += 1\n",
    "\n",
    "        image = image.to(device)\n",
    "        label = int(label.cpu())\n",
    "\n",
    "        # feature = (f(image.reshape(1, 3, 32, 32))[\"avg_pool2d\"].reshape(-1)).cpu().detach()\n",
    "        # feature = (f(image.reshape(1, 3, 32, 32))[\"layer4.2.relu_2\"].reshape(-1)).cpu().detach()\n",
    "        # feature = (f(image.reshape(1, 3, 32, 32)).reshape(-1)).cpu().detach()\n",
    "        feature = (f(image.reshape(1, 3, 32, 32)).reshape(-1)).detach().cpu()\n",
    "\n",
    "        if label in d.keys():\n",
    "            d[label].append(feature)\n",
    "        else:\n",
    "            d[label] = [feature]\n",
    "\n",
    "    # mu, sigma\n",
    "    upload_d = {}\n",
    "\n",
    "    # for k, v in tqdm.tqdm(d.items()):\n",
    "    #     v_item = torch.stack(v).detach().cpu()\n",
    "    #\n",
    "    #     # consider the case where the sample size is too small to upload\n",
    "    #     if len(v_item) < 10:\n",
    "    #         continue\n",
    "    #\n",
    "    #     mu, sigma = v_item.mean(dim=0), v_item.var(dim=0)\n",
    "    #     upload_d[k] = {\"mu\": mu, \"sigma\": sigma, \"N\": len(v)}\n",
    "\n",
    "    for k, v in tqdm.tqdm(d.items()):\n",
    "        v_item = torch.stack(v).detach().cpu()\n",
    "        # consider the case where the sample size is too small to upload\n",
    "        if len(v_item) < 10:\n",
    "            continue\n",
    "\n",
    "        N = len(v)\n",
    "\n",
    "        mu = v_item.mean(dim=0)\n",
    "\n",
    "        # sigma = torch.zeros((2048, 2048))\n",
    "        # sigma = v_item.reshape(50, 8192).transpose(1, 0).cov()\n",
    "        sigma = v_item.reshape(50, 2048).transpose(1, 0).cov()\n",
    "\n",
    "        # sigma = torch.zeros((8192, 8192))\n",
    "        # for t in v_item:\n",
    "        #     # x = (t - mu).reshape(1, 2048)\n",
    "        #     x = (t - mu).reshape(1, 8192)\n",
    "        #     sigma = torch.add(torch.mul(x, torch.transpose(x, 1, 0)), sigma)\n",
    "        # sigma *= 1/(N-1)\n",
    "        #\n",
    "        upload_d[k] = {\"mu\": mu, \"sigma\": sigma, \"N\": N}\n",
    "\n",
    "\n",
    "    return upload_d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "upload_d = clientUpdate(f, train_dataset, user_groups[0], device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "upload_d.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "upload_d[0][\"mu\"].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "upload_d[0][\"sigma\"].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "upload_d[0][\"sigma\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m = max(int(frac * num_users), 1) # number of users to be used for federated updates, at least 1\n",
    "# idxs_users = np.random.choice(range(num_users), m, replace=False) # choose randomly m users\n",
    "\n",
    "idxs_users = range(num_users)\n",
    "\n",
    "upload_d_list = [ clientUpdate(f, train_dataset, user_groups[idx], device) for idx in idxs_users ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(upload_d_list))\n",
    "upload_d_list[5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def server_aggregate_stat(upload_d_list):\n",
    "    # statistical feature distribution for each label\n",
    "    fd_d = {}\n",
    "\n",
    "    for l in range(10):  # for each label\n",
    "\n",
    "        print(\"label : \", l)\n",
    "\n",
    "        # clients do not necessarily have all tags, heterogeneous\n",
    "        labeled_fd_lst = [ x for x in upload_d_list if l in x.keys() ]\n",
    "        sum_n = sum(x[l][\"N\"] for x in labeled_fd_lst)\n",
    "\n",
    "        mu_lst = [fd[l][\"mu\"] * fd[l][\"N\"] / sum_n for fd in labeled_fd_lst]\n",
    "        mu = torch.stack(mu_lst).sum(dim=0)\n",
    "        # print(mu.shape)\n",
    "        #\n",
    "        # sigma1 = torch.stack(\n",
    "        #     [fd[l][\"mu\"] * (fd[l][\"N\"] - 1) / (sum_n - 1) for fd in labeled_fd_lst]\n",
    "        # ).sum(dim=0)\n",
    "        #\n",
    "        # sigma2 = torch.stack(\n",
    "        #     [\n",
    "        #         fd[l][\"mu\"] * fd[l][\"mu\"].T * fd[l][\"N\"] / (sum_n - 1)\n",
    "        #         for fd in labeled_fd_lst\n",
    "        #     ]\n",
    "        # ).sum(dim=0)\n",
    "        #\n",
    "        # sigma = sigma1 + sigma2 - sum_n / (sum_n - 1) * mu * mu.T\n",
    "\n",
    "        sigma1 = torch.stack(\n",
    "            [ (fd[l][\"N\"] - 1) / (sum_n - 1) * fd[l][\"sigma\"] for fd in labeled_fd_lst ]\n",
    "        ).sum(dim=0)\n",
    "        # print(sigma1.shape)\n",
    "\n",
    "        sigma2 = torch.stack(\n",
    "            [\n",
    "                fd[l][\"mu\"].reshape(1, 2048) * torch.transpose(fd[l][\"mu\"].reshape(1, 2048), 1, 0) * fd[l][\"N\"] / (sum_n - 1)\n",
    "                for fd in labeled_fd_lst\n",
    "            ]\n",
    "        ).sum(dim=0)\n",
    "        # sigma2 = torch.stack(\n",
    "        #     [\n",
    "        #         fd[l][\"mu\"].reshape(1, 8192) * torch.transpose(fd[l][\"mu\"].reshape(1, 8192), 1, 0) * fd[l][\"N\"] / (sum_n - 1)\n",
    "        #         for fd in labeled_fd_lst\n",
    "        #     ]\n",
    "        # ).sum(dim=0)\n",
    "        # print(sigma2.shape)\n",
    "\n",
    "        sigma3 = sum_n / (sum_n - 1) * mu.reshape(1, 2048) * torch.transpose(mu.reshape(1, 2048), 1, 0)\n",
    "        # sigma3 = sum_n / (sum_n - 1) * mu.reshape(1, 8192) * torch.transpose(mu.reshape(1, 8192), 1, 0)\n",
    "        # print(sigma3.shape)\n",
    "\n",
    "        sigma = sigma1 + sigma2 - sigma3\n",
    "        # print(sigma.shape)\n",
    "\n",
    "\n",
    "        virtual_samples = np.random.default_rng().multivariate_normal(mu, sigma, check_valid='ignore', size=1000, tol=1e-6, method='eigh')\n",
    "        fd_d[l] = torch.tensor(virtual_samples)\n",
    "\n",
    "\n",
    "        # generate data samples with batchsize of 1k, there are 10 categories in total, so it is 10k samples\n",
    "        # dist_c = np.random.normal(mu, sigma, size=(1000, mu.size()[0]))\n",
    "        # fd_d[l] = torch.tensor(dist_c)\n",
    "\n",
    "    return fd_d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fd_d = {}\n",
    "# for l in range(10):  # for each label\n",
    "#     print(\"label : \", l)\n",
    "#\n",
    "#     # clients do not necessarily have all tags, heterogeneous\n",
    "#     labeled_fd_lst = [ x for x in upload_d_list if l in x.keys() ]\n",
    "#     sum_n = sum(x[l][\"N\"] for x in labeled_fd_lst)\n",
    "#\n",
    "#     mu_lst = [fd[l][\"mu\"] * fd[l][\"N\"] / sum_n for fd in labeled_fd_lst]\n",
    "#     mu = torch.stack(mu_lst).sum(dim=0)\n",
    "#     # print(mu.shape)\n",
    "#\n",
    "#     sigma1 = torch.stack(\n",
    "#         [ (fd[l][\"N\"] - 1) / (sum_n - 1) * fd[l][\"sigma\"] for fd in labeled_fd_lst ]\n",
    "#     ).sum(dim=0)\n",
    "#     # print(sigma1.shape)\n",
    "#\n",
    "#     sigma2 = torch.stack(\n",
    "#         [\n",
    "#             fd[l][\"mu\"].reshape(1, 2048) * torch.transpose(fd[l][\"mu\"].reshape(1, 2048), 1, 0) * fd[l][\"N\"] / (sum_n - 1)\n",
    "#             for fd in labeled_fd_lst\n",
    "#         ]\n",
    "#     ).sum(dim=0)\n",
    "#     # print(sigma2.shape)\n",
    "#\n",
    "#     sigma3 = sum_n / (sum_n - 1) * mu.reshape(1, 2048) * torch.transpose(mu.reshape(1, 2048), 1, 0)\n",
    "#     # print(sigma3.shape)\n",
    "#\n",
    "#     sigma = sigma1 + sigma2 - sum_n / (sum_n - 1) * mu * mu.T\n",
    "#     # print(sigma.shape)\n",
    "#\n",
    "#\n",
    "#     virtual_samples = np.random.default_rng().multivariate_normal(mu, sigma, check_valid='ignore', size=100, tol=1e-6, method='eigh')\n",
    "#     fd_d[l] = torch.tensor(virtual_samples)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fd_d = server_aggregate_stat(upload_d_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for k, v in fd_d.items():\n",
    "#     print(\"label\", k)\n",
    "#     print(v.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "filename_pikle = \"CCVR_results/extracted_features_iid[{}]_unbalanced[{}]_new.pickle\".format(iid, unbalanced)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(filename_pikle, 'wb') as handle:\n",
    "    pickle.dump(fd_d, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "with open(filename_pikle, 'rb') as handle:\n",
    "    fd_d = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class DictDataset(Dataset):\n",
    "    def __init__(self, label_data_d):\n",
    "        \"Initialization\"\n",
    "        self.data, self.labels = [], []\n",
    "        for label, data in label_data_d.items():\n",
    "            self.data.append(data)\n",
    "            self.labels.append(torch.tensor([label] * len(data)))\n",
    "\n",
    "        self.data = torch.cat(self.data).type(torch.float32)\n",
    "        self.labels = torch.cat(self.labels).type(torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        return self.data[index], self.labels[index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def get_dataloader(trainset, testset, batch_size, num_workers=0, pin_memory=False):\n",
    "    trainloader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    testloader = DataLoader(\n",
    "        testset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "#\n",
    "# class classifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(classifier, self).__init__()\n",
    "#\n",
    "#         # network ends a 10-way fully-connected layer\n",
    "#         expansion = 4\n",
    "#         num_classes = 10\n",
    "#\n",
    "#         self.linear = nn.Linear(512 * expansion, num_classes)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # out = F.avg_pool2d(x, 4)  # average pooling before fully connected layer\n",
    "#         out = x.view(x.size(0), -1)\n",
    "#         out = self.linear(out)\n",
    "#         return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "relu = torch.nn.ReLU()\n",
    "beta = 0.5\n",
    "for k, v in fd_d.items():\n",
    "    x = relu(v)\n",
    "    fd_d[k] = torch.tensor(np.power(x.cpu().numpy(), beta)).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 20,490\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "g = classifier()\n",
    "g.to(device)\n",
    "summary(g, (2048, 1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# avg = torch.nn.functional.avg_pool2d(torch.rand((1,2048,4,4)), 2)\n",
    "# avg = avg.reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# avg.view(avg.size(0), -1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fd_d[0][0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fd_d[0][0].reshape((2048, 1, 1)).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# x = fd_d[0].reshape((1000, 2048, 1, 1)).type(torch.float).to(device)\n",
    "# x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# g(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer = torch.optim.SGD(g.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "# prepare datasets, models, optimizers, and more\n",
    "trainset = DictDataset(fd_d)\n",
    "train_loader, _ = get_dataloader(trainset, trainset, batch_size=local_batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 332.946 \tTraining Accuracy: 0.000\n",
      "Epoch: 2 \tTraining Loss: 792.373 \tTraining Accuracy: 0.000\n",
      "Epoch: 3 \tTraining Loss: 737.292 \tTraining Accuracy: 0.000\n",
      "Epoch: 4 \tTraining Loss: 608.546 \tTraining Accuracy: 0.000\n",
      "Epoch: 5 \tTraining Loss: 680.207 \tTraining Accuracy: 0.000\n",
      "Epoch: 6 \tTraining Loss: 712.422 \tTraining Accuracy: 0.000\n",
      "Epoch: 7 \tTraining Loss: 604.989 \tTraining Accuracy: 0.000\n",
      "Epoch: 8 \tTraining Loss: 524.490 \tTraining Accuracy: 0.000\n",
      "Epoch: 9 \tTraining Loss: 606.068 \tTraining Accuracy: 0.000\n",
      "Epoch: 10 \tTraining Loss: 752.801 \tTraining Accuracy: 0.000\n",
      "Epoch: 11 \tTraining Loss: 492.749 \tTraining Accuracy: 0.000\n",
      "Epoch: 12 \tTraining Loss: 791.937 \tTraining Accuracy: 0.000\n",
      "Epoch: 13 \tTraining Loss: 596.404 \tTraining Accuracy: 0.000\n",
      "Epoch: 14 \tTraining Loss: 722.416 \tTraining Accuracy: 0.000\n",
      "Epoch: 15 \tTraining Loss: 662.545 \tTraining Accuracy: 0.000\n",
      "Epoch: 16 \tTraining Loss: 735.718 \tTraining Accuracy: 0.000\n",
      "Epoch: 17 \tTraining Loss: 437.989 \tTraining Accuracy: 0.000\n",
      "Epoch: 18 \tTraining Loss: 486.894 \tTraining Accuracy: 0.000\n",
      "Epoch: 19 \tTraining Loss: 468.941 \tTraining Accuracy: 0.000\n",
      "Epoch: 20 \tTraining Loss: 457.963 \tTraining Accuracy: 0.000\n",
      "Epoch: 21 \tTraining Loss: 587.090 \tTraining Accuracy: 0.000\n",
      "Epoch: 22 \tTraining Loss: 566.937 \tTraining Accuracy: 0.000\n",
      "Epoch: 23 \tTraining Loss: 533.725 \tTraining Accuracy: 0.000\n",
      "Epoch: 24 \tTraining Loss: 813.616 \tTraining Accuracy: 0.000\n",
      "Epoch: 25 \tTraining Loss: 530.386 \tTraining Accuracy: 0.000\n",
      "Epoch: 26 \tTraining Loss: 835.938 \tTraining Accuracy: 0.000\n",
      "Epoch: 27 \tTraining Loss: 484.963 \tTraining Accuracy: 0.000\n",
      "Epoch: 28 \tTraining Loss: 860.529 \tTraining Accuracy: 0.000\n",
      "Epoch: 29 \tTraining Loss: 570.018 \tTraining Accuracy: 0.000\n",
      "Epoch: 30 \tTraining Loss: 780.245 \tTraining Accuracy: 0.000\n",
      "Epoch: 31 \tTraining Loss: 701.526 \tTraining Accuracy: 0.000\n",
      "Epoch: 32 \tTraining Loss: 660.463 \tTraining Accuracy: 0.100\n",
      "Epoch: 33 \tTraining Loss: 671.480 \tTraining Accuracy: 0.000\n",
      "Epoch: 34 \tTraining Loss: 691.524 \tTraining Accuracy: 0.000\n",
      "Epoch: 35 \tTraining Loss: 543.477 \tTraining Accuracy: 0.100\n",
      "Epoch: 36 \tTraining Loss: 626.100 \tTraining Accuracy: 0.000\n",
      "Epoch: 37 \tTraining Loss: 706.910 \tTraining Accuracy: 0.000\n",
      "Epoch: 38 \tTraining Loss: 492.318 \tTraining Accuracy: 0.077\n",
      "Epoch: 39 \tTraining Loss: 465.445 \tTraining Accuracy: 0.077\n",
      "Epoch: 40 \tTraining Loss: 615.296 \tTraining Accuracy: 0.000\n",
      "Epoch: 41 \tTraining Loss: 463.102 \tTraining Accuracy: 0.100\n",
      "Epoch: 42 \tTraining Loss: 604.506 \tTraining Accuracy: 0.090\n",
      "Epoch: 43 \tTraining Loss: 402.279 \tTraining Accuracy: 0.070\n",
      "Epoch: 44 \tTraining Loss: 772.534 \tTraining Accuracy: 0.002\n",
      "Epoch: 45 \tTraining Loss: 482.565 \tTraining Accuracy: 0.000\n",
      "Epoch: 46 \tTraining Loss: 959.886 \tTraining Accuracy: 0.000\n",
      "Epoch: 47 \tTraining Loss: 624.140 \tTraining Accuracy: 0.034\n",
      "Epoch: 48 \tTraining Loss: 692.822 \tTraining Accuracy: 0.073\n",
      "Epoch: 49 \tTraining Loss: 391.843 \tTraining Accuracy: 0.196\n",
      "Epoch: 50 \tTraining Loss: 737.121 \tTraining Accuracy: 0.000\n",
      "Epoch: 51 \tTraining Loss: 126.433 \tTraining Accuracy: 0.000\n",
      "Epoch: 52 \tTraining Loss: 58.687 \tTraining Accuracy: 0.097\n",
      "Epoch: 53 \tTraining Loss: 63.926 \tTraining Accuracy: 0.006\n",
      "Epoch: 54 \tTraining Loss: 70.559 \tTraining Accuracy: 0.006\n",
      "Epoch: 55 \tTraining Loss: 66.891 \tTraining Accuracy: 0.042\n",
      "Epoch: 56 \tTraining Loss: 69.683 \tTraining Accuracy: 0.000\n",
      "Epoch: 57 \tTraining Loss: 43.378 \tTraining Accuracy: 0.008\n",
      "Epoch: 58 \tTraining Loss: 32.950 \tTraining Accuracy: 0.068\n",
      "Epoch: 59 \tTraining Loss: 34.661 \tTraining Accuracy: 0.016\n",
      "Epoch: 60 \tTraining Loss: 43.335 \tTraining Accuracy: 0.000\n",
      "Epoch: 61 \tTraining Loss: 30.140 \tTraining Accuracy: 0.017\n",
      "Epoch: 62 \tTraining Loss: 40.702 \tTraining Accuracy: 0.001\n",
      "Epoch: 63 \tTraining Loss: 31.210 \tTraining Accuracy: 0.095\n",
      "Epoch: 64 \tTraining Loss: 34.576 \tTraining Accuracy: 0.008\n",
      "Epoch: 65 \tTraining Loss: 36.667 \tTraining Accuracy: 0.018\n",
      "Epoch: 66 \tTraining Loss: 36.320 \tTraining Accuracy: 0.035\n",
      "Epoch: 67 \tTraining Loss: 36.853 \tTraining Accuracy: 0.014\n",
      "Epoch: 68 \tTraining Loss: 39.945 \tTraining Accuracy: 0.068\n",
      "Epoch: 69 \tTraining Loss: 36.089 \tTraining Accuracy: 0.093\n",
      "Epoch: 70 \tTraining Loss: 39.020 \tTraining Accuracy: 0.033\n",
      "Epoch: 71 \tTraining Loss: 36.696 \tTraining Accuracy: 0.098\n",
      "Epoch: 72 \tTraining Loss: 44.675 \tTraining Accuracy: 0.051\n",
      "Epoch: 73 \tTraining Loss: 31.129 \tTraining Accuracy: 0.074\n",
      "Epoch: 74 \tTraining Loss: 31.672 \tTraining Accuracy: 0.001\n",
      "Epoch: 75 \tTraining Loss: 39.138 \tTraining Accuracy: 0.016\n",
      "Epoch: 76 \tTraining Loss: 38.324 \tTraining Accuracy: 0.065\n",
      "Epoch: 77 \tTraining Loss: 41.364 \tTraining Accuracy: 0.005\n",
      "Epoch: 78 \tTraining Loss: 39.158 \tTraining Accuracy: 0.014\n",
      "Epoch: 79 \tTraining Loss: 45.047 \tTraining Accuracy: 0.098\n",
      "Epoch: 80 \tTraining Loss: 38.815 \tTraining Accuracy: 0.064\n",
      "Epoch: 81 \tTraining Loss: 35.058 \tTraining Accuracy: 0.017\n",
      "Epoch: 82 \tTraining Loss: 47.012 \tTraining Accuracy: 0.015\n",
      "Epoch: 83 \tTraining Loss: 33.057 \tTraining Accuracy: 0.137\n",
      "Epoch: 84 \tTraining Loss: 32.613 \tTraining Accuracy: 0.103\n",
      "Epoch: 85 \tTraining Loss: 32.580 \tTraining Accuracy: 0.021\n",
      "Epoch: 86 \tTraining Loss: 31.536 \tTraining Accuracy: 0.084\n",
      "Epoch: 87 \tTraining Loss: 42.579 \tTraining Accuracy: 0.004\n",
      "Epoch: 88 \tTraining Loss: 39.525 \tTraining Accuracy: 0.014\n",
      "Epoch: 89 \tTraining Loss: 51.108 \tTraining Accuracy: 0.007\n",
      "Epoch: 90 \tTraining Loss: 44.723 \tTraining Accuracy: 0.061\n",
      "Epoch: 91 \tTraining Loss: 36.996 \tTraining Accuracy: 0.059\n",
      "Epoch: 92 \tTraining Loss: 35.052 \tTraining Accuracy: 0.037\n",
      "Epoch: 93 \tTraining Loss: 43.295 \tTraining Accuracy: 0.022\n",
      "Epoch: 94 \tTraining Loss: 39.411 \tTraining Accuracy: 0.005\n",
      "Epoch: 95 \tTraining Loss: 38.590 \tTraining Accuracy: 0.003\n",
      "Epoch: 96 \tTraining Loss: 33.175 \tTraining Accuracy: 0.003\n",
      "Epoch: 97 \tTraining Loss: 45.864 \tTraining Accuracy: 0.028\n",
      "Epoch: 98 \tTraining Loss: 38.016 \tTraining Accuracy: 0.107\n",
      "Epoch: 99 \tTraining Loss: 39.750 \tTraining Accuracy: 0.002\n",
      "Epoch: 100 \tTraining Loss: 44.688 \tTraining Accuracy: 0.061\n",
      "Epoch: 101 \tTraining Loss: 20.387 \tTraining Accuracy: 0.116\n",
      "Epoch: 102 \tTraining Loss: 10.833 \tTraining Accuracy: 0.169\n",
      "Epoch: 103 \tTraining Loss: 7.636 \tTraining Accuracy: 0.190\n",
      "Epoch: 104 \tTraining Loss: 6.398 \tTraining Accuracy: 0.179\n",
      "Epoch: 105 \tTraining Loss: 5.494 \tTraining Accuracy: 0.178\n",
      "Epoch: 106 \tTraining Loss: 5.041 \tTraining Accuracy: 0.213\n",
      "Epoch: 107 \tTraining Loss: 5.380 \tTraining Accuracy: 0.158\n",
      "Epoch: 108 \tTraining Loss: 4.960 \tTraining Accuracy: 0.212\n",
      "Epoch: 109 \tTraining Loss: 5.211 \tTraining Accuracy: 0.179\n",
      "Epoch: 110 \tTraining Loss: 4.923 \tTraining Accuracy: 0.202\n",
      "Epoch: 111 \tTraining Loss: 5.179 \tTraining Accuracy: 0.185\n",
      "Epoch: 112 \tTraining Loss: 4.930 \tTraining Accuracy: 0.197\n",
      "Epoch: 113 \tTraining Loss: 5.164 \tTraining Accuracy: 0.188\n",
      "Epoch: 114 \tTraining Loss: 4.939 \tTraining Accuracy: 0.195\n",
      "Epoch: 115 \tTraining Loss: 5.143 \tTraining Accuracy: 0.189\n",
      "Epoch: 116 \tTraining Loss: 4.943 \tTraining Accuracy: 0.194\n",
      "Epoch: 117 \tTraining Loss: 5.128 \tTraining Accuracy: 0.190\n",
      "Epoch: 118 \tTraining Loss: 4.944 \tTraining Accuracy: 0.192\n",
      "Epoch: 119 \tTraining Loss: 5.111 \tTraining Accuracy: 0.192\n",
      "Epoch: 120 \tTraining Loss: 4.945 \tTraining Accuracy: 0.192\n",
      "Epoch: 121 \tTraining Loss: 5.097 \tTraining Accuracy: 0.192\n",
      "Epoch: 122 \tTraining Loss: 4.944 \tTraining Accuracy: 0.192\n",
      "Epoch: 123 \tTraining Loss: 5.083 \tTraining Accuracy: 0.193\n",
      "Epoch: 124 \tTraining Loss: 4.943 \tTraining Accuracy: 0.192\n",
      "Epoch: 125 \tTraining Loss: 5.072 \tTraining Accuracy: 0.194\n",
      "Epoch: 126 \tTraining Loss: 4.941 \tTraining Accuracy: 0.192\n",
      "Epoch: 127 \tTraining Loss: 5.063 \tTraining Accuracy: 0.194\n",
      "Epoch: 128 \tTraining Loss: 4.938 \tTraining Accuracy: 0.192\n",
      "Epoch: 129 \tTraining Loss: 5.055 \tTraining Accuracy: 0.194\n",
      "Epoch: 130 \tTraining Loss: 4.936 \tTraining Accuracy: 0.192\n",
      "Epoch: 131 \tTraining Loss: 5.048 \tTraining Accuracy: 0.194\n",
      "Epoch: 132 \tTraining Loss: 4.934 \tTraining Accuracy: 0.192\n",
      "Epoch: 133 \tTraining Loss: 5.043 \tTraining Accuracy: 0.194\n",
      "Epoch: 134 \tTraining Loss: 4.931 \tTraining Accuracy: 0.192\n",
      "Epoch: 135 \tTraining Loss: 5.038 \tTraining Accuracy: 0.194\n",
      "Epoch: 136 \tTraining Loss: 4.929 \tTraining Accuracy: 0.191\n",
      "Epoch: 137 \tTraining Loss: 5.035 \tTraining Accuracy: 0.193\n",
      "Epoch: 138 \tTraining Loss: 4.926 \tTraining Accuracy: 0.191\n",
      "Epoch: 139 \tTraining Loss: 5.032 \tTraining Accuracy: 0.194\n",
      "Epoch: 140 \tTraining Loss: 4.924 \tTraining Accuracy: 0.191\n",
      "Epoch: 141 \tTraining Loss: 5.030 \tTraining Accuracy: 0.194\n",
      "Epoch: 142 \tTraining Loss: 4.921 \tTraining Accuracy: 0.191\n",
      "Epoch: 143 \tTraining Loss: 5.029 \tTraining Accuracy: 0.194\n",
      "Epoch: 144 \tTraining Loss: 4.919 \tTraining Accuracy: 0.190\n",
      "Epoch: 145 \tTraining Loss: 5.028 \tTraining Accuracy: 0.194\n",
      "Epoch: 146 \tTraining Loss: 4.916 \tTraining Accuracy: 0.190\n",
      "Epoch: 147 \tTraining Loss: 5.027 \tTraining Accuracy: 0.193\n",
      "Epoch: 148 \tTraining Loss: 4.914 \tTraining Accuracy: 0.190\n",
      "Epoch: 149 \tTraining Loss: 5.026 \tTraining Accuracy: 0.194\n",
      "Epoch: 150 \tTraining Loss: 4.912 \tTraining Accuracy: 0.189\n",
      "Epoch: 151 \tTraining Loss: 3.947 \tTraining Accuracy: 0.308\n",
      "Epoch: 152 \tTraining Loss: 3.854 \tTraining Accuracy: 0.313\n",
      "Epoch: 153 \tTraining Loss: 3.794 \tTraining Accuracy: 0.315\n",
      "Epoch: 154 \tTraining Loss: 3.769 \tTraining Accuracy: 0.317\n",
      "Epoch: 155 \tTraining Loss: 3.753 \tTraining Accuracy: 0.320\n",
      "Epoch: 156 \tTraining Loss: 3.749 \tTraining Accuracy: 0.321\n",
      "Epoch: 157 \tTraining Loss: 3.750 \tTraining Accuracy: 0.322\n",
      "Epoch: 158 \tTraining Loss: 3.752 \tTraining Accuracy: 0.321\n",
      "Epoch: 159 \tTraining Loss: 3.753 \tTraining Accuracy: 0.321\n",
      "Epoch: 160 \tTraining Loss: 3.753 \tTraining Accuracy: 0.321\n",
      "Epoch: 161 \tTraining Loss: 3.752 \tTraining Accuracy: 0.321\n",
      "Epoch: 162 \tTraining Loss: 3.752 \tTraining Accuracy: 0.321\n",
      "Epoch: 163 \tTraining Loss: 3.751 \tTraining Accuracy: 0.321\n",
      "Epoch: 164 \tTraining Loss: 3.751 \tTraining Accuracy: 0.321\n",
      "Epoch: 165 \tTraining Loss: 3.751 \tTraining Accuracy: 0.321\n",
      "Epoch: 166 \tTraining Loss: 3.751 \tTraining Accuracy: 0.321\n",
      "Epoch: 167 \tTraining Loss: 3.751 \tTraining Accuracy: 0.321\n",
      "Epoch: 168 \tTraining Loss: 3.750 \tTraining Accuracy: 0.321\n",
      "Epoch: 169 \tTraining Loss: 3.750 \tTraining Accuracy: 0.321\n",
      "Epoch: 170 \tTraining Loss: 3.750 \tTraining Accuracy: 0.321\n",
      "Epoch: 171 \tTraining Loss: 3.750 \tTraining Accuracy: 0.321\n",
      "Epoch: 172 \tTraining Loss: 3.749 \tTraining Accuracy: 0.321\n",
      "Epoch: 173 \tTraining Loss: 3.749 \tTraining Accuracy: 0.321\n",
      "Epoch: 174 \tTraining Loss: 3.749 \tTraining Accuracy: 0.321\n",
      "Epoch: 175 \tTraining Loss: 3.749 \tTraining Accuracy: 0.321\n",
      "Epoch: 176 \tTraining Loss: 3.748 \tTraining Accuracy: 0.321\n",
      "Epoch: 177 \tTraining Loss: 3.748 \tTraining Accuracy: 0.321\n",
      "Epoch: 178 \tTraining Loss: 3.748 \tTraining Accuracy: 0.321\n",
      "Epoch: 179 \tTraining Loss: 3.748 \tTraining Accuracy: 0.321\n",
      "Epoch: 180 \tTraining Loss: 3.747 \tTraining Accuracy: 0.321\n",
      "Epoch: 181 \tTraining Loss: 3.747 \tTraining Accuracy: 0.321\n",
      "Epoch: 182 \tTraining Loss: 3.747 \tTraining Accuracy: 0.321\n",
      "Epoch: 183 \tTraining Loss: 3.747 \tTraining Accuracy: 0.321\n",
      "Epoch: 184 \tTraining Loss: 3.746 \tTraining Accuracy: 0.321\n",
      "Epoch: 185 \tTraining Loss: 3.746 \tTraining Accuracy: 0.321\n",
      "Epoch: 186 \tTraining Loss: 3.746 \tTraining Accuracy: 0.321\n",
      "Epoch: 187 \tTraining Loss: 3.746 \tTraining Accuracy: 0.321\n",
      "Epoch: 188 \tTraining Loss: 3.745 \tTraining Accuracy: 0.321\n",
      "Epoch: 189 \tTraining Loss: 3.745 \tTraining Accuracy: 0.321\n",
      "Epoch: 190 \tTraining Loss: 3.745 \tTraining Accuracy: 0.321\n",
      "Epoch: 191 \tTraining Loss: 3.745 \tTraining Accuracy: 0.321\n",
      "Epoch: 192 \tTraining Loss: 3.744 \tTraining Accuracy: 0.321\n",
      "Epoch: 193 \tTraining Loss: 3.744 \tTraining Accuracy: 0.321\n",
      "Epoch: 194 \tTraining Loss: 3.744 \tTraining Accuracy: 0.321\n",
      "Epoch: 195 \tTraining Loss: 3.744 \tTraining Accuracy: 0.321\n",
      "Epoch: 196 \tTraining Loss: 3.743 \tTraining Accuracy: 0.321\n",
      "Epoch: 197 \tTraining Loss: 3.743 \tTraining Accuracy: 0.321\n",
      "Epoch: 198 \tTraining Loss: 3.743 \tTraining Accuracy: 0.321\n",
      "Epoch: 199 \tTraining Loss: 3.743 \tTraining Accuracy: 0.321\n",
      "Epoch: 200 \tTraining Loss: 3.743 \tTraining Accuracy: 0.321\n",
      "Epoch: 201 \tTraining Loss: 3.663 \tTraining Accuracy: 0.335\n",
      "Epoch: 202 \tTraining Loss: 3.663 \tTraining Accuracy: 0.335\n",
      "Epoch: 203 \tTraining Loss: 3.663 \tTraining Accuracy: 0.335\n",
      "Epoch: 204 \tTraining Loss: 3.663 \tTraining Accuracy: 0.335\n",
      "Epoch: 205 \tTraining Loss: 3.663 \tTraining Accuracy: 0.335\n",
      "Epoch: 206 \tTraining Loss: 3.663 \tTraining Accuracy: 0.335\n",
      "Epoch: 207 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 208 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 209 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 210 \tTraining Loss: 3.662 \tTraining Accuracy: 0.334\n",
      "Epoch: 211 \tTraining Loss: 3.662 \tTraining Accuracy: 0.334\n",
      "Epoch: 212 \tTraining Loss: 3.662 \tTraining Accuracy: 0.334\n",
      "Epoch: 213 \tTraining Loss: 3.662 \tTraining Accuracy: 0.334\n",
      "Epoch: 214 \tTraining Loss: 3.662 \tTraining Accuracy: 0.334\n",
      "Epoch: 215 \tTraining Loss: 3.662 \tTraining Accuracy: 0.334\n",
      "Epoch: 216 \tTraining Loss: 3.662 \tTraining Accuracy: 0.334\n",
      "Epoch: 217 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 218 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 219 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 220 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 221 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 222 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 223 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 224 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 225 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 226 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 227 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 228 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 229 \tTraining Loss: 3.662 \tTraining Accuracy: 0.335\n",
      "Epoch: 230 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 231 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 232 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 233 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 234 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 235 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 236 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 237 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 238 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 239 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 240 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 241 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 242 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 243 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 244 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 245 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 246 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 247 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 248 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 249 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 250 \tTraining Loss: 3.661 \tTraining Accuracy: 0.335\n",
      "Epoch: 251 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 252 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 253 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 254 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 255 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 256 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 257 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 258 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 259 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 260 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 261 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 262 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 263 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 264 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 265 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 266 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 267 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 268 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 269 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 270 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 271 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 272 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 273 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 274 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 275 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 276 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 277 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 278 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 279 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 280 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 281 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 282 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 283 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 284 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 285 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 286 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 287 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 288 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 289 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 290 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 291 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 292 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 293 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 294 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 295 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 296 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 297 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 298 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 299 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 300 \tTraining Loss: 3.653 \tTraining Accuracy: 0.337\n",
      "Epoch: 301 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 302 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 303 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 304 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 305 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 306 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 307 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 308 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 309 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 310 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 311 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 312 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 313 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 314 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 315 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 316 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 317 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 318 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 319 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 320 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 321 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 322 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 323 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 324 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 325 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 326 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 327 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 328 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 329 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 330 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 331 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 332 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 333 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 334 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 335 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 336 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 337 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 338 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 339 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 340 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 341 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 342 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 343 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 344 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 345 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 346 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 347 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 348 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 349 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 350 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 351 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 352 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 353 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 354 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 355 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 356 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 357 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 358 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 359 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 360 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 361 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 362 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 363 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 364 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 365 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 366 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 367 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 368 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 369 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 370 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 371 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 372 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 373 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 374 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 375 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 376 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 377 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 378 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 379 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 380 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 381 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 382 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 383 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 384 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 385 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 386 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 387 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 388 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 389 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 390 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 391 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 392 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 393 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 394 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 395 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 396 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 397 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 398 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 399 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 400 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 401 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 402 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 403 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 404 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 405 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 406 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 407 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 408 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 409 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 410 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 411 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 412 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 413 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 414 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 415 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 416 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 417 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 418 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 419 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 420 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 421 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 422 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 423 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 424 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 425 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 426 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 427 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 428 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 429 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 430 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 431 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 432 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 433 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 434 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 435 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 436 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 437 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 438 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 439 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 440 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 441 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 442 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 443 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 444 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 445 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 446 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 447 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 448 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 449 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 450 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 451 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 452 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 453 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 454 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 455 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 456 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 457 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 458 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 459 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 460 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 461 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 462 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 463 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 464 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 465 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 466 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 467 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 468 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 469 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 470 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 471 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 472 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 473 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 474 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 475 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 476 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 477 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 478 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 479 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 480 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 481 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 482 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 483 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 484 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 485 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 486 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 487 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 488 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 489 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 490 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 491 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 492 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 493 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 494 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 495 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 496 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 497 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 498 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n",
      "Epoch: 499 \tTraining Loss: 3.652 \tTraining Accuracy: 0.337\n"
     ]
    }
   ],
   "source": [
    "# for epoch in range(1, n_epochs+1):\n",
    "for epoch in range(1, 500):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    correct_train = 0.0\n",
    "    correct_valid = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    g.train()\n",
    "\n",
    "    for k, v in fd_d.items():\n",
    "\n",
    "        data, target = v.type(torch.float).cuda(), torch.full((1000,), k).cuda()\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = g(data.reshape((1000, 2048, 1, 1)))\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += (loss.data.item() * data.shape[0])\n",
    "        # print('outputs on which to apply torch.max ', prediction)\n",
    "        # find the maximum along the rows, use dim=1 to torch.max()\n",
    "        _, predicted_outputs = torch.max(output.data, 1)\n",
    "        # Update the running corrects\n",
    "        correct_train += (predicted_outputs == target).float().sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    # calculate accuracies\n",
    "    train_acc =  correct_train / len(train_loader.sampler)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.3f} \\tTraining Accuracy: {:.3f}'.format(\n",
    "        epoch, train_loss, train_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def test_inference(f, g, test_dataset, gpu=1, local_batch_size=10, loss_function=\"CrossEntropyLoss\"):\n",
    "    \"\"\"\n",
    "    Returns the test accuracy and loss.\n",
    "    \"\"\"\n",
    "\n",
    "    f.eval()\n",
    "    g.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "\n",
    "    device = 'cuda' if gpu else 'cpu'\n",
    "    if loss_function == \"NLLLoss\":\n",
    "        criterion = nn.NLLLoss()\n",
    "    if loss_function == \"CrossEntropyLoss\":\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    testloader = DataLoader(test_dataset, batch_size=10,\n",
    "                            shuffle=False, generator=generator)\n",
    "\n",
    "    for images, labels in testloader:\n",
    "    # for k, v in fd_d.items():\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # images, labels = v.type(torch.float).cuda().reshape(1000, 2048, 1, 1), torch.full((1000,), k).cuda()\n",
    "\n",
    "        # Inference\n",
    "        # output_f = f(images)[\"avg_pool2d\"]\n",
    "        output_f = f(images.cuda())\n",
    "        output_g = g(output_f)\n",
    "\n",
    "        loss = criterion(output_g, labels)\n",
    "        test_loss += (loss.data.item() * images.shape[0])\n",
    "\n",
    "        # Prediction\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output_g, 1)\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            label = labels.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    # average test loss\n",
    "    test_loss = test_loss / len(testloader.dataset)\n",
    "\n",
    "    accuracy = np.sum(class_correct) / np.sum(class_total)\n",
    "\n",
    "    return accuracy, test_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results after 100 global rounds of training:\n",
      "|---- Test Accuracy: 38.58%\n"
     ]
    }
   ],
   "source": [
    "# test the trained model\n",
    "\n",
    "test_acc, test_loss = test_inference(f, g, test_dataset=test_dataset, gpu=gpu,\n",
    "                                     loss_function=loss_function)\n",
    "\n",
    "print(f'\\nResults after {n_epochs} global rounds of training:')\n",
    "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}