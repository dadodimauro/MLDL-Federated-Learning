{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/AshwinRJ/Federated-Learning-PyTorch\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from models import ResNet50\n",
    "# from utils import get_dataset, average_weights, exp_details\n",
    "# from utils_v2 import get_dataset, average_weights, exp_details\n",
    "from update import LocalUpdate, test_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# parameters\n",
    "iid = 1 # if the data is i.i.d or not\n",
    "unbalanced = 0 # in non i.i.d. setting split the data between clients equally or not\n",
    "num_users = 100 # number of client\n",
    "frac = 0.1 # fraction of the clients to be used for federated updates\n",
    "n_epochs = 100\n",
    "gpu = 0\n",
    "optimizer = \"sgd\" #sgd or adam\n",
    "local_batch_size = 10 # batch size of local updates in each user\n",
    "lr = 0.001 # learning rate\n",
    "local_epochs = 1\n",
    "loss_function = \"CrossEntropyLoss\"\n",
    "\n",
    "num_groups = 0  # 0 for BatchNorm, > 0 for GroupNorm\n",
    "if num_groups == 0:\n",
    "    normalization_type = \"BatchNorm\"\n",
    "else:\n",
    "    normalization_type = \"GroupNorm\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if iid:\n",
    "    from utils_v2 import get_dataset, average_weights, exp_details\n",
    "else:\n",
    "    from utils import get_dataset, average_weights, exp_details"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimental details:\n",
      "    Model     : ResNet50\n",
      "    Optimizer : sgd\n",
      "    Learning  : 0.001\n",
      "    Normalization  : BatchNorm\n",
      "    Global Rounds   : 100\n",
      "\n",
      "    Federated parameters:\n",
      "    IID\n",
      "    NUmber of users  : 100\n",
      "    Fraction of users  : 0.1\n",
      "    Local Batch size   : 128\n",
      "    Local Epochs       : 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_details(\"ResNet50\", optimizer, lr, normalization_type, n_epochs, iid, frac,\n",
    "            local_batch_size, local_epochs, unbalanced, num_users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# for REPRODUCIBILITY https://pytorch.org/docs/stable/notes/randomness.html\n",
    "torch.manual_seed(0)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, user_groups = get_dataset(iid=iid, unbalanced=unbalanced,\n",
    "                                                       num_users=num_users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    },
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (shortcut): Sequential()\n    )\n  )\n  (linear): Linear(in_features=2048, out_features=10, bias=True)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50(n_type=normalization_type)\n",
    "# model = CNNCifar()\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "    device = torch.device(\"cpu\")\n",
    "    gpu = 0\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpu = 1\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# set the model to train\n",
    "model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]           4,096\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "            Conv2d-7          [-1, 256, 32, 32]          16,384\n",
      "       BatchNorm2d-8          [-1, 256, 32, 32]             512\n",
      "            Conv2d-9          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
      "       Bottleneck-11          [-1, 256, 32, 32]               0\n",
      "           Conv2d-12           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-13           [-1, 64, 32, 32]             128\n",
      "           Conv2d-14           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
      "           Conv2d-16          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-17          [-1, 256, 32, 32]             512\n",
      "       Bottleneck-18          [-1, 256, 32, 32]               0\n",
      "           Conv2d-19           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
      "           Conv2d-21           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 32, 32]             128\n",
      "           Conv2d-23          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 32, 32]             512\n",
      "       Bottleneck-25          [-1, 256, 32, 32]               0\n",
      "           Conv2d-26          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-27          [-1, 128, 32, 32]             256\n",
      "           Conv2d-28          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 16, 16]             256\n",
      "           Conv2d-30          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-31          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-32          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-34          [-1, 512, 16, 16]               0\n",
      "           Conv2d-35          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-36          [-1, 128, 16, 16]             256\n",
      "           Conv2d-37          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-38          [-1, 128, 16, 16]             256\n",
      "           Conv2d-39          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-40          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-41          [-1, 512, 16, 16]               0\n",
      "           Conv2d-42          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-43          [-1, 128, 16, 16]             256\n",
      "           Conv2d-44          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-45          [-1, 128, 16, 16]             256\n",
      "           Conv2d-46          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-47          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-48          [-1, 512, 16, 16]               0\n",
      "           Conv2d-49          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 16, 16]             256\n",
      "           Conv2d-51          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-52          [-1, 128, 16, 16]             256\n",
      "           Conv2d-53          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-54          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-55          [-1, 512, 16, 16]               0\n",
      "           Conv2d-56          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-57          [-1, 256, 16, 16]             512\n",
      "           Conv2d-58            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-59            [-1, 256, 8, 8]             512\n",
      "           Conv2d-60           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-61           [-1, 1024, 8, 8]           2,048\n",
      "           Conv2d-62           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-63           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-64           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-65            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-66            [-1, 256, 8, 8]             512\n",
      "           Conv2d-67            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-68            [-1, 256, 8, 8]             512\n",
      "           Conv2d-69           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-70           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-71           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-72            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-73            [-1, 256, 8, 8]             512\n",
      "           Conv2d-74            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-75            [-1, 256, 8, 8]             512\n",
      "           Conv2d-76           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-77           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-78           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-79            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-80            [-1, 256, 8, 8]             512\n",
      "           Conv2d-81            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-82            [-1, 256, 8, 8]             512\n",
      "           Conv2d-83           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-84           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-85           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-86            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-87            [-1, 256, 8, 8]             512\n",
      "           Conv2d-88            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-89            [-1, 256, 8, 8]             512\n",
      "           Conv2d-90           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-91           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-92           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-93            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-94            [-1, 256, 8, 8]             512\n",
      "           Conv2d-95            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-96            [-1, 256, 8, 8]             512\n",
      "           Conv2d-97           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-99           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-100            [-1, 512, 8, 8]         524,288\n",
      "     BatchNorm2d-101            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-102            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-103            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-104           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-105           [-1, 2048, 4, 4]           4,096\n",
      "          Conv2d-106           [-1, 2048, 4, 4]       2,097,152\n",
      "     BatchNorm2d-107           [-1, 2048, 4, 4]           4,096\n",
      "      Bottleneck-108           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-109            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-110            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-111            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-112            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-113           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-114           [-1, 2048, 4, 4]           4,096\n",
      "      Bottleneck-115           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-116            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-117            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-118            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-119            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-120           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-121           [-1, 2048, 4, 4]           4,096\n",
      "      Bottleneck-122           [-1, 2048, 4, 4]               0\n",
      "          Linear-123                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 23,520,842\n",
      "Trainable params: 23,520,842\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 66.13\n",
      "Params size (MB): 89.72\n",
      "Estimated Total Size (MB): 155.86\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# total number of params 591,322\n",
    "summary(model, (3, 32, 32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# copy weights\n",
    "global_weights = model.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\OneDrive - Politecnico di Torino\\PoliTO\\MASTER\\MACHINE LEARNING AND DEEP LEARNING\\MLDL Federated Learning\\update.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image), torch.tensor(label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Global Round : 1 | Local Epoch : 1 | Train Loss: 2.4673 | Train Accuracy: 0.11\n",
      "| Global Round : 1 | Average Train Loss: 2.4673 \n",
      "| Client : 43 | Average Loss: 2.4673 \n",
      "| Global Round : 1 | Local Epoch : 1 | Train Loss: 2.4488 | Train Accuracy: 0.11\n",
      "| Global Round : 1 | Average Train Loss: 2.4488 \n",
      "| Client : 36 | Average Loss: 2.4488 \n",
      "| Global Round : 1 | Local Epoch : 1 | Train Loss: 2.4625 | Train Accuracy: 0.11\n",
      "| Global Round : 1 | Average Train Loss: 2.4625 \n",
      "| Client : 92 | Average Loss: 2.4625 \n",
      "| Global Round : 1 | Local Epoch : 1 | Train Loss: 2.4533 | Train Accuracy: 0.10\n",
      "| Global Round : 1 | Average Train Loss: 2.4533 \n",
      "| Client : 80 | Average Loss: 2.4533 \n",
      "| Global Round : 1 | Local Epoch : 1 | Train Loss: 2.4343 | Train Accuracy: 0.10\n",
      "| Global Round : 1 | Average Train Loss: 2.4343 \n",
      "| Client : 15 | Average Loss: 2.4343 \n",
      "| Global Round : 1 | Local Epoch : 1 | Train Loss: 2.4625 | Train Accuracy: 0.11\n",
      "| Global Round : 1 | Average Train Loss: 2.4625 \n",
      "| Client : 26 | Average Loss: 2.4625 \n",
      "| Global Round : 1 | Local Epoch : 1 | Train Loss: 2.4779 | Train Accuracy: 0.09\n",
      "| Global Round : 1 | Average Train Loss: 2.4779 \n",
      "| Client : 63 | Average Loss: 2.4779 \n",
      "| Global Round : 1 | Local Epoch : 1 | Train Loss: 2.4545 | Train Accuracy: 0.11\n",
      "| Global Round : 1 | Average Train Loss: 2.4545 \n",
      "| Client : 52 | Average Loss: 2.4545 \n",
      "| Global Round : 1 | Local Epoch : 1 | Train Loss: 2.4741 | Train Accuracy: 0.10\n",
      "| Global Round : 1 | Average Train Loss: 2.4741 \n",
      "| Client : 48 | Average Loss: 2.4741 \n",
      "| Global Round : 1 | Local Epoch : 1 | Train Loss: 2.4540 | Train Accuracy: 0.10\n",
      "| Global Round : 1 | Average Train Loss: 2.4540 \n",
      "| Client : 93 | Average Loss: 2.4540 \n",
      "\n",
      "Average training statistics (global epoch : 1\n",
      "|---- Trainig Loss : 2.458916056421068\n",
      "|---- Training Accuracy: 9.48% \n",
      "\n",
      "Epoch: 2 \n",
      "\n",
      "| Global Round : 2 | Local Epoch : 1 | Train Loss: 2.3372 | Train Accuracy: 0.10\n",
      "| Global Round : 2 | Average Train Loss: 2.3372 \n",
      "| Client : 24 | Average Loss: 2.3372 \n",
      "| Global Round : 2 | Local Epoch : 1 | Train Loss: 2.3108 | Train Accuracy: 0.10\n",
      "| Global Round : 2 | Average Train Loss: 2.3108 \n",
      "| Client : 19 | Average Loss: 2.3108 \n",
      "| Global Round : 2 | Local Epoch : 1 | Train Loss: 2.3399 | Train Accuracy: 0.10\n",
      "| Global Round : 2 | Average Train Loss: 2.3399 \n",
      "| Client : 18 | Average Loss: 2.3399 \n",
      "| Global Round : 2 | Local Epoch : 1 | Train Loss: 2.3194 | Train Accuracy: 0.10\n",
      "| Global Round : 2 | Average Train Loss: 2.3194 \n",
      "| Client : 4 | Average Loss: 2.3194 \n",
      "| Global Round : 2 | Local Epoch : 1 | Train Loss: 2.3371 | Train Accuracy: 0.11\n",
      "| Global Round : 2 | Average Train Loss: 2.3371 \n",
      "| Client : 6 | Average Loss: 2.3371 \n",
      "| Global Round : 2 | Local Epoch : 1 | Train Loss: 2.3111 | Train Accuracy: 0.10\n",
      "| Global Round : 2 | Average Train Loss: 2.3111 \n",
      "| Client : 40 | Average Loss: 2.3111 \n",
      "| Global Round : 2 | Local Epoch : 1 | Train Loss: 2.3392 | Train Accuracy: 0.10\n",
      "| Global Round : 2 | Average Train Loss: 2.3392 \n",
      "| Client : 53 | Average Loss: 2.3392 \n",
      "| Global Round : 2 | Local Epoch : 1 | Train Loss: 2.3241 | Train Accuracy: 0.08\n",
      "| Global Round : 2 | Average Train Loss: 2.3241 \n",
      "| Client : 32 | Average Loss: 2.3241 \n",
      "| Global Round : 2 | Local Epoch : 1 | Train Loss: 2.3125 | Train Accuracy: 0.12\n",
      "| Global Round : 2 | Average Train Loss: 2.3125 \n",
      "| Client : 79 | Average Loss: 2.3125 \n",
      "| Global Round : 2 | Local Epoch : 1 | Train Loss: 2.3397 | Train Accuracy: 0.13\n",
      "| Global Round : 2 | Average Train Loss: 2.3397 \n",
      "| Client : 26 | Average Loss: 2.3397 \n",
      "\n",
      "Average training statistics (global epoch : 2\n",
      "|---- Trainig Loss : 2.3929956630600824\n",
      "|---- Training Accuracy: 10.44% \n",
      "\n",
      "Epoch: 3 \n",
      "\n",
      "| Global Round : 3 | Local Epoch : 1 | Train Loss: 2.3045 | Train Accuracy: 0.09\n",
      "| Global Round : 3 | Average Train Loss: 2.3045 \n",
      "| Client : 81 | Average Loss: 2.3045 \n",
      "| Global Round : 3 | Local Epoch : 1 | Train Loss: 2.2978 | Train Accuracy: 0.12\n",
      "| Global Round : 3 | Average Train Loss: 2.2978 \n",
      "| Client : 68 | Average Loss: 2.2978 \n",
      "| Global Round : 3 | Local Epoch : 1 | Train Loss: 2.2946 | Train Accuracy: 0.12\n",
      "| Global Round : 3 | Average Train Loss: 2.2946 \n",
      "| Client : 29 | Average Loss: 2.2946 \n",
      "| Global Round : 3 | Local Epoch : 1 | Train Loss: 2.2937 | Train Accuracy: 0.10\n",
      "| Global Round : 3 | Average Train Loss: 2.2937 \n",
      "| Client : 86 | Average Loss: 2.2937 \n",
      "| Global Round : 3 | Local Epoch : 1 | Train Loss: 2.2981 | Train Accuracy: 0.13\n",
      "| Global Round : 3 | Average Train Loss: 2.2981 \n",
      "| Client : 58 | Average Loss: 2.2981 \n",
      "| Global Round : 3 | Local Epoch : 1 | Train Loss: 2.3147 | Train Accuracy: 0.08\n",
      "| Global Round : 3 | Average Train Loss: 2.3147 \n",
      "| Client : 79 | Average Loss: 2.3147 \n",
      "| Global Round : 3 | Local Epoch : 1 | Train Loss: 2.2938 | Train Accuracy: 0.11\n",
      "| Global Round : 3 | Average Train Loss: 2.2938 \n",
      "| Client : 15 | Average Loss: 2.2938 \n",
      "| Global Round : 3 | Local Epoch : 1 | Train Loss: 2.2785 | Train Accuracy: 0.11\n",
      "| Global Round : 3 | Average Train Loss: 2.2785 \n",
      "| Client : 76 | Average Loss: 2.2785 \n",
      "| Global Round : 3 | Local Epoch : 1 | Train Loss: 2.3064 | Train Accuracy: 0.11\n",
      "| Global Round : 3 | Average Train Loss: 2.3064 \n",
      "| Client : 24 | Average Loss: 2.3064 \n",
      "| Global Round : 3 | Local Epoch : 1 | Train Loss: 2.2880 | Train Accuracy: 0.13\n",
      "| Global Round : 3 | Average Train Loss: 2.2880 \n",
      "| Client : 45 | Average Loss: 2.2880 \n",
      "\n",
      "Average training statistics (global epoch : 3\n",
      "|---- Trainig Loss : 2.3609968715950296\n",
      "|---- Training Accuracy: 10.44% \n",
      "\n",
      "Epoch: 4 \n",
      "\n",
      "| Global Round : 4 | Local Epoch : 1 | Train Loss: 2.2993 | Train Accuracy: 0.11\n",
      "| Global Round : 4 | Average Train Loss: 2.2993 \n",
      "| Client : 96 | Average Loss: 2.2993 \n",
      "| Global Round : 4 | Local Epoch : 1 | Train Loss: 2.2811 | Train Accuracy: 0.11\n",
      "| Global Round : 4 | Average Train Loss: 2.2811 \n",
      "| Client : 84 | Average Loss: 2.2811 \n",
      "| Global Round : 4 | Local Epoch : 1 | Train Loss: 2.2593 | Train Accuracy: 0.14\n",
      "| Global Round : 4 | Average Train Loss: 2.2593 \n",
      "| Client : 38 | Average Loss: 2.2593 \n",
      "| Global Round : 4 | Local Epoch : 1 | Train Loss: 2.2813 | Train Accuracy: 0.14\n",
      "| Global Round : 4 | Average Train Loss: 2.2813 \n",
      "| Client : 56 | Average Loss: 2.2813 \n",
      "| Global Round : 4 | Local Epoch : 1 | Train Loss: 2.2774 | Train Accuracy: 0.12\n",
      "| Global Round : 4 | Average Train Loss: 2.2774 \n",
      "| Client : 23 | Average Loss: 2.2774 \n",
      "| Global Round : 4 | Local Epoch : 1 | Train Loss: 2.2945 | Train Accuracy: 0.09\n",
      "| Global Round : 4 | Average Train Loss: 2.2945 \n",
      "| Client : 55 | Average Loss: 2.2945 \n",
      "| Global Round : 4 | Local Epoch : 1 | Train Loss: 2.2816 | Train Accuracy: 0.14\n",
      "| Global Round : 4 | Average Train Loss: 2.2816 \n",
      "| Client : 10 | Average Loss: 2.2816 \n",
      "| Global Round : 4 | Local Epoch : 1 | Train Loss: 2.2794 | Train Accuracy: 0.12\n",
      "| Global Round : 4 | Average Train Loss: 2.2794 \n",
      "| Client : 71 | Average Loss: 2.2794 \n",
      "| Global Round : 4 | Local Epoch : 1 | Train Loss: 2.2957 | Train Accuracy: 0.14\n",
      "| Global Round : 4 | Average Train Loss: 2.2957 \n",
      "| Client : 3 | Average Loss: 2.2957 \n",
      "| Global Round : 4 | Local Epoch : 1 | Train Loss: 2.2740 | Train Accuracy: 0.14\n",
      "| Global Round : 4 | Average Train Loss: 2.2740 \n",
      "| Client : 2 | Average Loss: 2.2740 \n",
      "\n",
      "Average training statistics (global epoch : 4\n",
      "|---- Trainig Loss : 2.3413377040492165\n",
      "|---- Training Accuracy: 10.44% \n",
      "\n",
      "Epoch: 5 \n",
      "\n",
      "| Global Round : 5 | Local Epoch : 1 | Train Loss: 2.2895 | Train Accuracy: 0.12\n",
      "| Global Round : 5 | Average Train Loss: 2.2895 \n",
      "| Client : 34 | Average Loss: 2.2895 \n",
      "| Global Round : 5 | Local Epoch : 1 | Train Loss: 2.3031 | Train Accuracy: 0.09\n",
      "| Global Round : 5 | Average Train Loss: 2.3031 \n",
      "| Client : 25 | Average Loss: 2.3031 \n",
      "| Global Round : 5 | Local Epoch : 1 | Train Loss: 2.2715 | Train Accuracy: 0.12\n",
      "| Global Round : 5 | Average Train Loss: 2.2715 \n",
      "| Client : 77 | Average Loss: 2.2715 \n",
      "| Global Round : 5 | Local Epoch : 1 | Train Loss: 2.2780 | Train Accuracy: 0.15\n",
      "| Global Round : 5 | Average Train Loss: 2.2780 \n",
      "| Client : 54 | Average Loss: 2.2780 \n",
      "| Global Round : 5 | Local Epoch : 1 | Train Loss: 2.2854 | Train Accuracy: 0.10\n",
      "| Global Round : 5 | Average Train Loss: 2.2854 \n",
      "| Client : 53 | Average Loss: 2.2854 \n",
      "| Global Round : 5 | Local Epoch : 1 | Train Loss: 2.2832 | Train Accuracy: 0.13\n",
      "| Global Round : 5 | Average Train Loss: 2.2832 \n",
      "| Client : 84 | Average Loss: 2.2832 \n",
      "| Global Round : 5 | Local Epoch : 1 | Train Loss: 2.2790 | Train Accuracy: 0.12\n",
      "| Global Round : 5 | Average Train Loss: 2.2790 \n",
      "| Client : 69 | Average Loss: 2.2790 \n",
      "| Global Round : 5 | Local Epoch : 1 | Train Loss: 2.2800 | Train Accuracy: 0.14\n",
      "| Global Round : 5 | Average Train Loss: 2.2800 \n",
      "| Client : 33 | Average Loss: 2.2800 \n",
      "| Global Round : 5 | Local Epoch : 1 | Train Loss: 2.2699 | Train Accuracy: 0.12\n",
      "| Global Round : 5 | Average Train Loss: 2.2699 \n",
      "| Client : 1 | Average Loss: 2.2699 \n",
      "| Global Round : 5 | Local Epoch : 1 | Train Loss: 2.2901 | Train Accuracy: 0.12\n",
      "| Global Round : 5 | Average Train Loss: 2.2901 \n",
      "| Client : 24 | Average Loss: 2.2901 \n",
      "\n",
      "Average training statistics (global epoch : 5\n",
      "|---- Trainig Loss : 2.3296649528927276\n",
      "|---- Training Accuracy: 10.44% \n",
      "\n",
      "Epoch: 6 \n",
      "\n",
      "| Global Round : 6 | Local Epoch : 1 | Train Loss: 2.2800 | Train Accuracy: 0.12\n",
      "| Global Round : 6 | Average Train Loss: 2.2800 \n",
      "| Client : 78 | Average Loss: 2.2800 \n",
      "| Global Round : 6 | Local Epoch : 1 | Train Loss: 2.2871 | Train Accuracy: 0.10\n",
      "| Global Round : 6 | Average Train Loss: 2.2871 \n",
      "| Client : 92 | Average Loss: 2.2871 \n",
      "| Global Round : 6 | Local Epoch : 1 | Train Loss: 2.2839 | Train Accuracy: 0.15\n",
      "| Global Round : 6 | Average Train Loss: 2.2839 \n",
      "| Client : 0 | Average Loss: 2.2839 \n",
      "| Global Round : 6 | Local Epoch : 1 | Train Loss: 2.2636 | Train Accuracy: 0.14\n",
      "| Global Round : 6 | Average Train Loss: 2.2636 \n",
      "| Client : 75 | Average Loss: 2.2636 \n",
      "| Global Round : 6 | Local Epoch : 1 | Train Loss: 2.2985 | Train Accuracy: 0.14\n",
      "| Global Round : 6 | Average Train Loss: 2.2985 \n",
      "| Client : 20 | Average Loss: 2.2985 \n",
      "| Global Round : 6 | Local Epoch : 1 | Train Loss: 2.2941 | Train Accuracy: 0.10\n",
      "| Global Round : 6 | Average Train Loss: 2.2941 \n",
      "| Client : 13 | Average Loss: 2.2941 \n",
      "| Global Round : 6 | Local Epoch : 1 | Train Loss: 2.2800 | Train Accuracy: 0.11\n",
      "| Global Round : 6 | Average Train Loss: 2.2800 \n",
      "| Client : 43 | Average Loss: 2.2800 \n",
      "| Global Round : 6 | Local Epoch : 1 | Train Loss: 2.2711 | Train Accuracy: 0.13\n",
      "| Global Round : 6 | Average Train Loss: 2.2711 \n",
      "| Client : 51 | Average Loss: 2.2711 \n",
      "| Global Round : 6 | Local Epoch : 1 | Train Loss: 2.2664 | Train Accuracy: 0.14\n",
      "| Global Round : 6 | Average Train Loss: 2.2664 \n",
      "| Client : 65 | Average Loss: 2.2664 \n",
      "| Global Round : 6 | Local Epoch : 1 | Train Loss: 2.2852 | Train Accuracy: 0.14\n",
      "| Global Round : 6 | Average Train Loss: 2.2852 \n",
      "| Client : 9 | Average Loss: 2.2852 \n",
      "\n",
      "Average training statistics (global epoch : 6\n",
      "|---- Trainig Loss : 2.3215544383967366\n",
      "|---- Training Accuracy: 10.44% \n",
      "\n",
      "Epoch: 7 \n",
      "\n",
      "| Global Round : 7 | Local Epoch : 1 | Train Loss: 2.2710 | Train Accuracy: 0.14\n",
      "| Global Round : 7 | Average Train Loss: 2.2710 \n",
      "| Client : 81 | Average Loss: 2.2710 \n",
      "| Global Round : 7 | Local Epoch : 1 | Train Loss: 2.2566 | Train Accuracy: 0.17\n",
      "| Global Round : 7 | Average Train Loss: 2.2566 \n",
      "| Client : 35 | Average Loss: 2.2566 \n",
      "| Global Round : 7 | Local Epoch : 1 | Train Loss: 2.2645 | Train Accuracy: 0.14\n",
      "| Global Round : 7 | Average Train Loss: 2.2645 \n",
      "| Client : 57 | Average Loss: 2.2645 \n",
      "| Global Round : 7 | Local Epoch : 1 | Train Loss: 2.2790 | Train Accuracy: 0.12\n",
      "| Global Round : 7 | Average Train Loss: 2.2790 \n",
      "| Client : 6 | Average Loss: 2.2790 \n",
      "| Global Round : 7 | Local Epoch : 1 | Train Loss: 2.2727 | Train Accuracy: 0.14\n",
      "| Global Round : 7 | Average Train Loss: 2.2727 \n",
      "| Client : 88 | Average Loss: 2.2727 \n",
      "| Global Round : 7 | Local Epoch : 1 | Train Loss: 2.2872 | Train Accuracy: 0.14\n",
      "| Global Round : 7 | Average Train Loss: 2.2872 \n",
      "| Client : 66 | Average Loss: 2.2872 \n",
      "| Global Round : 7 | Local Epoch : 1 | Train Loss: 2.2903 | Train Accuracy: 0.13\n",
      "| Global Round : 7 | Average Train Loss: 2.2903 \n",
      "| Client : 78 | Average Loss: 2.2903 \n",
      "| Global Round : 7 | Local Epoch : 1 | Train Loss: 2.2718 | Train Accuracy: 0.13\n",
      "| Global Round : 7 | Average Train Loss: 2.2718 \n",
      "| Client : 2 | Average Loss: 2.2718 \n",
      "| Global Round : 7 | Local Epoch : 1 | Train Loss: 2.2782 | Train Accuracy: 0.13\n",
      "| Global Round : 7 | Average Train Loss: 2.2782 \n",
      "| Client : 91 | Average Loss: 2.2782 \n",
      "| Global Round : 7 | Local Epoch : 1 | Train Loss: 2.2609 | Train Accuracy: 0.12\n",
      "| Global Round : 7 | Average Train Loss: 2.2609 \n",
      "| Client : 12 | Average Loss: 2.2609 \n",
      "\n",
      "Average training statistics (global epoch : 7\n",
      "|---- Trainig Loss : 2.314649780303713\n",
      "|---- Training Accuracy: 10.54% \n",
      "\n",
      "Epoch: 8 \n",
      "\n",
      "| Global Round : 8 | Local Epoch : 1 | Train Loss: 2.2685 | Train Accuracy: 0.14\n",
      "| Global Round : 8 | Average Train Loss: 2.2685 \n",
      "| Client : 98 | Average Loss: 2.2685 \n",
      "| Global Round : 8 | Local Epoch : 1 | Train Loss: 2.2507 | Train Accuracy: 0.14\n",
      "| Global Round : 8 | Average Train Loss: 2.2507 \n",
      "| Client : 18 | Average Loss: 2.2507 \n",
      "| Global Round : 8 | Local Epoch : 1 | Train Loss: 2.2698 | Train Accuracy: 0.14\n",
      "| Global Round : 8 | Average Train Loss: 2.2698 \n",
      "| Client : 22 | Average Loss: 2.2698 \n",
      "| Global Round : 8 | Local Epoch : 1 | Train Loss: 2.2754 | Train Accuracy: 0.13\n",
      "| Global Round : 8 | Average Train Loss: 2.2754 \n",
      "| Client : 47 | Average Loss: 2.2754 \n",
      "| Global Round : 8 | Local Epoch : 1 | Train Loss: 2.2575 | Train Accuracy: 0.14\n",
      "| Global Round : 8 | Average Train Loss: 2.2575 \n",
      "| Client : 23 | Average Loss: 2.2575 \n",
      "| Global Round : 8 | Local Epoch : 1 | Train Loss: 2.2678 | Train Accuracy: 0.12\n",
      "| Global Round : 8 | Average Train Loss: 2.2678 \n",
      "| Client : 17 | Average Loss: 2.2678 \n",
      "| Global Round : 8 | Local Epoch : 1 | Train Loss: 2.2773 | Train Accuracy: 0.15\n",
      "| Global Round : 8 | Average Train Loss: 2.2773 \n",
      "| Client : 44 | Average Loss: 2.2773 \n",
      "| Global Round : 8 | Local Epoch : 1 | Train Loss: 2.2866 | Train Accuracy: 0.14\n",
      "| Global Round : 8 | Average Train Loss: 2.2866 \n",
      "| Client : 88 | Average Loss: 2.2866 \n",
      "| Global Round : 8 | Local Epoch : 1 | Train Loss: 2.2636 | Train Accuracy: 0.13\n",
      "| Global Round : 8 | Average Train Loss: 2.2636 \n",
      "| Client : 94 | Average Loss: 2.2636 \n",
      "| Global Round : 8 | Local Epoch : 1 | Train Loss: 2.3014 | Train Accuracy: 0.12\n",
      "| Global Round : 8 | Average Train Loss: 2.3014 \n",
      "| Client : 3 | Average Loss: 2.3014 \n",
      "\n",
      "Average training statistics (global epoch : 8\n",
      "|---- Trainig Loss : 2.309301425245073\n",
      "|---- Training Accuracy: 10.28% \n",
      "\n",
      "Epoch: 9 \n",
      "\n",
      "| Global Round : 9 | Local Epoch : 1 | Train Loss: 2.2658 | Train Accuracy: 0.16\n",
      "| Global Round : 9 | Average Train Loss: 2.2658 \n",
      "| Client : 85 | Average Loss: 2.2658 \n",
      "| Global Round : 9 | Local Epoch : 1 | Train Loss: 2.2718 | Train Accuracy: 0.12\n",
      "| Global Round : 9 | Average Train Loss: 2.2718 \n",
      "| Client : 95 | Average Loss: 2.2718 \n",
      "| Global Round : 9 | Local Epoch : 1 | Train Loss: 2.2551 | Train Accuracy: 0.16\n",
      "| Global Round : 9 | Average Train Loss: 2.2551 \n",
      "| Client : 54 | Average Loss: 2.2551 \n",
      "| Global Round : 9 | Local Epoch : 1 | Train Loss: 2.2546 | Train Accuracy: 0.19\n",
      "| Global Round : 9 | Average Train Loss: 2.2546 \n",
      "| Client : 1 | Average Loss: 2.2546 \n",
      "| Global Round : 9 | Local Epoch : 1 | Train Loss: 2.2772 | Train Accuracy: 0.11\n",
      "| Global Round : 9 | Average Train Loss: 2.2772 \n",
      "| Client : 72 | Average Loss: 2.2772 \n",
      "| Global Round : 9 | Local Epoch : 1 | Train Loss: 2.2601 | Train Accuracy: 0.15\n",
      "| Global Round : 9 | Average Train Loss: 2.2601 \n",
      "| Client : 6 | Average Loss: 2.2601 \n",
      "| Global Round : 9 | Local Epoch : 1 | Train Loss: 2.2615 | Train Accuracy: 0.13\n",
      "| Global Round : 9 | Average Train Loss: 2.2615 \n",
      "| Client : 0 | Average Loss: 2.2615 \n",
      "| Global Round : 9 | Local Epoch : 1 | Train Loss: 2.2481 | Train Accuracy: 0.15\n",
      "| Global Round : 9 | Average Train Loss: 2.2481 \n",
      "| Client : 42 | Average Loss: 2.2481 \n",
      "| Global Round : 9 | Local Epoch : 1 | Train Loss: 2.2626 | Train Accuracy: 0.14\n",
      "| Global Round : 9 | Average Train Loss: 2.2626 \n",
      "| Client : 15 | Average Loss: 2.2626 \n",
      "| Global Round : 9 | Local Epoch : 1 | Train Loss: 2.2598 | Train Accuracy: 0.14\n",
      "| Global Round : 9 | Average Train Loss: 2.2598 \n",
      "| Client : 51 | Average Loss: 2.2598 \n",
      "\n",
      "Average training statistics (global epoch : 9\n",
      "|---- Trainig Loss : 2.3040097771515082\n",
      "|---- Training Accuracy: 10.10% \n",
      "\n",
      "Epoch: 10 \n",
      "\n",
      "| Global Round : 10 | Local Epoch : 1 | Train Loss: 2.2738 | Train Accuracy: 0.13\n",
      "| Global Round : 10 | Average Train Loss: 2.2738 \n",
      "| Client : 9 | Average Loss: 2.2738 \n",
      "| Global Round : 10 | Local Epoch : 1 | Train Loss: 2.2574 | Train Accuracy: 0.15\n",
      "| Global Round : 10 | Average Train Loss: 2.2574 \n",
      "| Client : 55 | Average Loss: 2.2574 \n",
      "| Global Round : 10 | Local Epoch : 1 | Train Loss: 2.2789 | Train Accuracy: 0.15\n",
      "| Global Round : 10 | Average Train Loss: 2.2789 \n",
      "| Client : 74 | Average Loss: 2.2789 \n",
      "| Global Round : 10 | Local Epoch : 1 | Train Loss: 2.2598 | Train Accuracy: 0.15\n",
      "| Global Round : 10 | Average Train Loss: 2.2598 \n",
      "| Client : 46 | Average Loss: 2.2598 \n",
      "| Global Round : 10 | Local Epoch : 1 | Train Loss: 2.2575 | Train Accuracy: 0.15\n",
      "| Global Round : 10 | Average Train Loss: 2.2575 \n",
      "| Client : 58 | Average Loss: 2.2575 \n",
      "| Global Round : 10 | Local Epoch : 1 | Train Loss: 2.2639 | Train Accuracy: 0.13\n",
      "| Global Round : 10 | Average Train Loss: 2.2639 \n",
      "| Client : 18 | Average Loss: 2.2639 \n",
      "| Global Round : 10 | Local Epoch : 1 | Train Loss: 2.2425 | Train Accuracy: 0.15\n",
      "| Global Round : 10 | Average Train Loss: 2.2425 \n",
      "| Client : 38 | Average Loss: 2.2425 \n",
      "| Global Round : 10 | Local Epoch : 1 | Train Loss: 2.2329 | Train Accuracy: 0.17\n",
      "| Global Round : 10 | Average Train Loss: 2.2329 \n",
      "| Client : 19 | Average Loss: 2.2329 \n",
      "| Global Round : 10 | Local Epoch : 1 | Train Loss: 2.2751 | Train Accuracy: 0.13\n",
      "| Global Round : 10 | Average Train Loss: 2.2751 \n",
      "| Client : 5 | Average Loss: 2.2751 \n",
      "| Global Round : 10 | Local Epoch : 1 | Train Loss: 2.2829 | Train Accuracy: 0.13\n",
      "| Global Round : 10 | Average Train Loss: 2.2829 \n",
      "| Client : 37 | Average Loss: 2.2829 \n",
      "\n",
      "Average training statistics (global epoch : 10\n",
      "|---- Trainig Loss : 2.2998543180995514\n",
      "|---- Training Accuracy: 11.16% \n",
      "\n",
      "Epoch: 11 \n",
      "\n",
      "| Global Round : 11 | Local Epoch : 1 | Train Loss: 2.2632 | Train Accuracy: 0.12\n",
      "| Global Round : 11 | Average Train Loss: 2.2632 \n",
      "| Client : 68 | Average Loss: 2.2632 \n",
      "| Global Round : 11 | Local Epoch : 1 | Train Loss: 2.2357 | Train Accuracy: 0.16\n",
      "| Global Round : 11 | Average Train Loss: 2.2357 \n",
      "| Client : 16 | Average Loss: 2.2357 \n",
      "| Global Round : 11 | Local Epoch : 1 | Train Loss: 2.2662 | Train Accuracy: 0.15\n",
      "| Global Round : 11 | Average Train Loss: 2.2662 \n",
      "| Client : 18 | Average Loss: 2.2662 \n",
      "| Global Round : 11 | Local Epoch : 1 | Train Loss: 2.2600 | Train Accuracy: 0.13\n",
      "| Global Round : 11 | Average Train Loss: 2.2600 \n",
      "| Client : 48 | Average Loss: 2.2600 \n",
      "| Global Round : 11 | Local Epoch : 1 | Train Loss: 2.2566 | Train Accuracy: 0.17\n",
      "| Global Round : 11 | Average Train Loss: 2.2566 \n",
      "| Client : 98 | Average Loss: 2.2566 \n",
      "| Global Round : 11 | Local Epoch : 1 | Train Loss: 2.2569 | Train Accuracy: 0.15\n",
      "| Global Round : 11 | Average Train Loss: 2.2569 \n",
      "| Client : 47 | Average Loss: 2.2569 \n",
      "| Global Round : 11 | Local Epoch : 1 | Train Loss: 2.2562 | Train Accuracy: 0.14\n",
      "| Global Round : 11 | Average Train Loss: 2.2562 \n",
      "| Client : 39 | Average Loss: 2.2562 \n",
      "| Global Round : 11 | Local Epoch : 1 | Train Loss: 2.2603 | Train Accuracy: 0.13\n",
      "| Global Round : 11 | Average Train Loss: 2.2603 \n",
      "| Client : 67 | Average Loss: 2.2603 \n",
      "| Global Round : 11 | Local Epoch : 1 | Train Loss: 2.2827 | Train Accuracy: 0.12\n",
      "| Global Round : 11 | Average Train Loss: 2.2827 \n",
      "| Client : 37 | Average Loss: 2.2827 \n",
      "| Global Round : 11 | Local Epoch : 1 | Train Loss: 2.2513 | Train Accuracy: 0.16\n",
      "| Global Round : 11 | Average Train Loss: 2.2513 \n",
      "| Client : 69 | Average Loss: 2.2513 \n",
      "\n",
      "Average training statistics (global epoch : 11\n",
      "|---- Trainig Loss : 2.2961320110956827\n",
      "|---- Training Accuracy: 11.34% \n",
      "\n",
      "Epoch: 12 \n",
      "\n",
      "| Global Round : 12 | Local Epoch : 1 | Train Loss: 2.2355 | Train Accuracy: 0.15\n",
      "| Global Round : 12 | Average Train Loss: 2.2355 \n",
      "| Client : 11 | Average Loss: 2.2355 \n",
      "| Global Round : 12 | Local Epoch : 1 | Train Loss: 2.2665 | Train Accuracy: 0.16\n",
      "| Global Round : 12 | Average Train Loss: 2.2665 \n",
      "| Client : 26 | Average Loss: 2.2665 \n",
      "| Global Round : 12 | Local Epoch : 1 | Train Loss: 2.2307 | Train Accuracy: 0.17\n",
      "| Global Round : 12 | Average Train Loss: 2.2307 \n",
      "| Client : 29 | Average Loss: 2.2307 \n",
      "| Global Round : 12 | Local Epoch : 1 | Train Loss: 2.2656 | Train Accuracy: 0.15\n",
      "| Global Round : 12 | Average Train Loss: 2.2656 \n",
      "| Client : 34 | Average Loss: 2.2656 \n",
      "| Global Round : 12 | Local Epoch : 1 | Train Loss: 2.2507 | Train Accuracy: 0.14\n",
      "| Global Round : 12 | Average Train Loss: 2.2507 \n",
      "| Client : 10 | Average Loss: 2.2507 \n",
      "| Global Round : 12 | Local Epoch : 1 | Train Loss: 2.2648 | Train Accuracy: 0.12\n",
      "| Global Round : 12 | Average Train Loss: 2.2648 \n",
      "| Client : 79 | Average Loss: 2.2648 \n",
      "| Global Round : 12 | Local Epoch : 1 | Train Loss: 2.2567 | Train Accuracy: 0.16\n",
      "| Global Round : 12 | Average Train Loss: 2.2567 \n",
      "| Client : 0 | Average Loss: 2.2567 \n",
      "| Global Round : 12 | Local Epoch : 1 | Train Loss: 2.2361 | Train Accuracy: 0.15\n",
      "| Global Round : 12 | Average Train Loss: 2.2361 \n",
      "| Client : 8 | Average Loss: 2.2361 \n",
      "| Global Round : 12 | Local Epoch : 1 | Train Loss: 2.2196 | Train Accuracy: 0.16\n",
      "| Global Round : 12 | Average Train Loss: 2.2196 \n",
      "| Client : 76 | Average Loss: 2.2196 \n",
      "| Global Round : 12 | Local Epoch : 1 | Train Loss: 2.2399 | Train Accuracy: 0.15\n",
      "| Global Round : 12 | Average Train Loss: 2.2399 \n",
      "| Client : 33 | Average Loss: 2.2399 \n",
      "\n",
      "Average training statistics (global epoch : 12\n",
      "|---- Trainig Loss : 2.2920051028993393\n",
      "|---- Training Accuracy: 12.90% \n",
      "\n",
      "Epoch: 13 \n",
      "\n",
      "| Global Round : 13 | Local Epoch : 1 | Train Loss: 2.2580 | Train Accuracy: 0.14\n",
      "| Global Round : 13 | Average Train Loss: 2.2580 \n",
      "| Client : 74 | Average Loss: 2.2580 \n",
      "| Global Round : 13 | Local Epoch : 1 | Train Loss: 2.2416 | Train Accuracy: 0.15\n",
      "| Global Round : 13 | Average Train Loss: 2.2416 \n",
      "| Client : 1 | Average Loss: 2.2416 \n",
      "| Global Round : 13 | Local Epoch : 1 | Train Loss: 2.2638 | Train Accuracy: 0.16\n",
      "| Global Round : 13 | Average Train Loss: 2.2638 \n",
      "| Client : 69 | Average Loss: 2.2638 \n",
      "| Global Round : 13 | Local Epoch : 1 | Train Loss: 2.2191 | Train Accuracy: 0.16\n",
      "| Global Round : 13 | Average Train Loss: 2.2191 \n",
      "| Client : 35 | Average Loss: 2.2191 \n",
      "| Global Round : 13 | Local Epoch : 1 | Train Loss: 2.2110 | Train Accuracy: 0.18\n",
      "| Global Round : 13 | Average Train Loss: 2.2110 \n",
      "| Client : 85 | Average Loss: 2.2110 \n",
      "| Global Round : 13 | Local Epoch : 1 | Train Loss: 2.2410 | Train Accuracy: 0.15\n",
      "| Global Round : 13 | Average Train Loss: 2.2410 \n",
      "| Client : 33 | Average Loss: 2.2410 \n",
      "| Global Round : 13 | Local Epoch : 1 | Train Loss: 2.2400 | Train Accuracy: 0.13\n",
      "| Global Round : 13 | Average Train Loss: 2.2400 \n",
      "| Client : 22 | Average Loss: 2.2400 \n",
      "| Global Round : 13 | Local Epoch : 1 | Train Loss: 2.2358 | Train Accuracy: 0.13\n",
      "| Global Round : 13 | Average Train Loss: 2.2358 \n",
      "| Client : 55 | Average Loss: 2.2358 \n",
      "| Global Round : 13 | Local Epoch : 1 | Train Loss: 2.2672 | Train Accuracy: 0.14\n",
      "| Global Round : 13 | Average Train Loss: 2.2672 \n",
      "| Client : 84 | Average Loss: 2.2672 \n",
      "| Global Round : 13 | Local Epoch : 1 | Train Loss: 2.2390 | Train Accuracy: 0.16\n",
      "| Global Round : 13 | Average Train Loss: 2.2390 \n",
      "| Client : 18 | Average Loss: 2.2390 \n",
      "\n",
      "Average training statistics (global epoch : 13\n",
      "|---- Trainig Loss : 2.2881302776744223\n",
      "|---- Training Accuracy: 14.80% \n",
      "\n",
      "Epoch: 14 \n",
      "\n",
      "| Global Round : 14 | Local Epoch : 1 | Train Loss: 2.2072 | Train Accuracy: 0.15\n",
      "| Global Round : 14 | Average Train Loss: 2.2072 \n",
      "| Client : 38 | Average Loss: 2.2072 \n",
      "| Global Round : 14 | Local Epoch : 1 | Train Loss: 2.2358 | Train Accuracy: 0.15\n",
      "| Global Round : 14 | Average Train Loss: 2.2358 \n",
      "| Client : 56 | Average Loss: 2.2358 \n",
      "| Global Round : 14 | Local Epoch : 1 | Train Loss: 2.2608 | Train Accuracy: 0.13\n",
      "| Global Round : 14 | Average Train Loss: 2.2608 \n",
      "| Client : 28 | Average Loss: 2.2608 \n",
      "| Global Round : 14 | Local Epoch : 1 | Train Loss: 2.2570 | Train Accuracy: 0.15\n",
      "| Global Round : 14 | Average Train Loss: 2.2570 \n",
      "| Client : 2 | Average Loss: 2.2570 \n",
      "| Global Round : 14 | Local Epoch : 1 | Train Loss: 2.2431 | Train Accuracy: 0.12\n",
      "| Global Round : 14 | Average Train Loss: 2.2431 \n",
      "| Client : 92 | Average Loss: 2.2431 \n",
      "| Global Round : 14 | Local Epoch : 1 | Train Loss: 2.2355 | Train Accuracy: 0.14\n",
      "| Global Round : 14 | Average Train Loss: 2.2355 \n",
      "| Client : 73 | Average Loss: 2.2355 \n",
      "| Global Round : 14 | Local Epoch : 1 | Train Loss: 2.2346 | Train Accuracy: 0.18\n",
      "| Global Round : 14 | Average Train Loss: 2.2346 \n",
      "| Client : 0 | Average Loss: 2.2346 \n",
      "| Global Round : 14 | Local Epoch : 1 | Train Loss: 2.2472 | Train Accuracy: 0.15\n",
      "| Global Round : 14 | Average Train Loss: 2.2472 \n",
      "| Client : 69 | Average Loss: 2.2472 \n",
      "| Global Round : 14 | Local Epoch : 1 | Train Loss: 2.2525 | Train Accuracy: 0.15\n",
      "| Global Round : 14 | Average Train Loss: 2.2525 \n",
      "| Client : 13 | Average Loss: 2.2525 \n",
      "| Global Round : 14 | Local Epoch : 1 | Train Loss: 2.2527 | Train Accuracy: 0.16\n",
      "| Global Round : 14 | Average Train Loss: 2.2527 \n",
      "| Client : 91 | Average Loss: 2.2527 \n",
      "\n",
      "Average training statistics (global epoch : 14\n",
      "|---- Trainig Loss : 2.284881032179272\n",
      "|---- Training Accuracy: 15.74% \n",
      "\n",
      "Epoch: 15 \n",
      "\n",
      "| Global Round : 15 | Local Epoch : 1 | Train Loss: 2.2332 | Train Accuracy: 0.14\n",
      "| Global Round : 15 | Average Train Loss: 2.2332 \n",
      "| Client : 11 | Average Loss: 2.2332 \n",
      "| Global Round : 15 | Local Epoch : 1 | Train Loss: 2.2375 | Train Accuracy: 0.15\n",
      "| Global Round : 15 | Average Train Loss: 2.2375 \n",
      "| Client : 54 | Average Loss: 2.2375 \n",
      "| Global Round : 15 | Local Epoch : 1 | Train Loss: 2.2613 | Train Accuracy: 0.14\n",
      "| Global Round : 15 | Average Train Loss: 2.2613 \n",
      "| Client : 31 | Average Loss: 2.2613 \n",
      "| Global Round : 15 | Local Epoch : 1 | Train Loss: 2.2320 | Train Accuracy: 0.16\n",
      "| Global Round : 15 | Average Train Loss: 2.2320 \n",
      "| Client : 13 | Average Loss: 2.2320 \n",
      "| Global Round : 15 | Local Epoch : 1 | Train Loss: 2.2303 | Train Accuracy: 0.17\n",
      "| Global Round : 15 | Average Train Loss: 2.2303 \n",
      "| Client : 20 | Average Loss: 2.2303 \n",
      "| Global Round : 15 | Local Epoch : 1 | Train Loss: 2.2247 | Train Accuracy: 0.16\n",
      "| Global Round : 15 | Average Train Loss: 2.2247 \n",
      "| Client : 97 | Average Loss: 2.2247 \n",
      "| Global Round : 15 | Local Epoch : 1 | Train Loss: 2.2541 | Train Accuracy: 0.16\n",
      "| Global Round : 15 | Average Train Loss: 2.2541 \n",
      "| Client : 3 | Average Loss: 2.2541 \n",
      "| Global Round : 15 | Local Epoch : 1 | Train Loss: 2.2361 | Train Accuracy: 0.17\n",
      "| Global Round : 15 | Average Train Loss: 2.2361 \n",
      "| Client : 87 | Average Loss: 2.2361 \n",
      "| Global Round : 15 | Local Epoch : 1 | Train Loss: 2.2421 | Train Accuracy: 0.16\n",
      "| Global Round : 15 | Average Train Loss: 2.2421 \n",
      "| Client : 80 | Average Loss: 2.2421 \n",
      "| Global Round : 15 | Local Epoch : 1 | Train Loss: 2.2383 | Train Accuracy: 0.17\n",
      "| Global Round : 15 | Average Train Loss: 2.2383 \n",
      "| Client : 79 | Average Loss: 2.2383 \n",
      "\n",
      "Average training statistics (global epoch : 15\n",
      "|---- Trainig Loss : 2.281818870332506\n",
      "|---- Training Accuracy: 15.96% \n",
      "\n",
      "Epoch: 16 \n",
      "\n",
      "| Global Round : 16 | Local Epoch : 1 | Train Loss: 2.1925 | Train Accuracy: 0.16\n",
      "| Global Round : 16 | Average Train Loss: 2.1925 \n",
      "| Client : 83 | Average Loss: 2.1925 \n",
      "| Global Round : 16 | Local Epoch : 1 | Train Loss: 2.2400 | Train Accuracy: 0.15\n",
      "| Global Round : 16 | Average Train Loss: 2.2400 \n",
      "| Client : 25 | Average Loss: 2.2400 \n",
      "| Global Round : 16 | Local Epoch : 1 | Train Loss: 2.2359 | Train Accuracy: 0.12\n",
      "| Global Round : 16 | Average Train Loss: 2.2359 \n",
      "| Client : 10 | Average Loss: 2.2359 \n",
      "| Global Round : 16 | Local Epoch : 1 | Train Loss: 2.2277 | Train Accuracy: 0.12\n",
      "| Global Round : 16 | Average Train Loss: 2.2277 \n",
      "| Client : 1 | Average Loss: 2.2277 \n",
      "| Global Round : 16 | Local Epoch : 1 | Train Loss: 2.2153 | Train Accuracy: 0.18\n",
      "| Global Round : 16 | Average Train Loss: 2.2153 \n",
      "| Client : 22 | Average Loss: 2.2153 \n",
      "| Global Round : 16 | Local Epoch : 1 | Train Loss: 2.2466 | Train Accuracy: 0.15\n",
      "| Global Round : 16 | Average Train Loss: 2.2466 \n",
      "| Client : 84 | Average Loss: 2.2466 \n",
      "| Global Round : 16 | Local Epoch : 1 | Train Loss: 2.2307 | Train Accuracy: 0.13\n",
      "| Global Round : 16 | Average Train Loss: 2.2307 \n",
      "| Client : 37 | Average Loss: 2.2307 \n",
      "| Global Round : 16 | Local Epoch : 1 | Train Loss: 2.2243 | Train Accuracy: 0.16\n",
      "| Global Round : 16 | Average Train Loss: 2.2243 \n",
      "| Client : 16 | Average Loss: 2.2243 \n",
      "| Global Round : 16 | Local Epoch : 1 | Train Loss: 2.2267 | Train Accuracy: 0.17\n",
      "| Global Round : 16 | Average Train Loss: 2.2267 \n",
      "| Client : 60 | Average Loss: 2.2267 \n",
      "| Global Round : 16 | Local Epoch : 1 | Train Loss: 2.2460 | Train Accuracy: 0.13\n",
      "| Global Round : 16 | Average Train Loss: 2.2460 \n",
      "| Client : 46 | Average Loss: 2.2460 \n",
      "\n",
      "Average training statistics (global epoch : 16\n",
      "|---- Trainig Loss : 2.2784908057782385\n",
      "|---- Training Accuracy: 16.22% \n",
      "\n",
      "Epoch: 17 \n",
      "\n",
      "| Global Round : 17 | Local Epoch : 1 | Train Loss: 2.2571 | Train Accuracy: 0.14\n",
      "| Global Round : 17 | Average Train Loss: 2.2571 \n",
      "| Client : 66 | Average Loss: 2.2571 \n",
      "| Global Round : 17 | Local Epoch : 1 | Train Loss: 2.2348 | Train Accuracy: 0.13\n",
      "| Global Round : 17 | Average Train Loss: 2.2348 \n",
      "| Client : 71 | Average Loss: 2.2348 \n",
      "| Global Round : 17 | Local Epoch : 1 | Train Loss: 2.2264 | Train Accuracy: 0.15\n",
      "| Global Round : 17 | Average Train Loss: 2.2264 \n",
      "| Client : 37 | Average Loss: 2.2264 \n",
      "| Global Round : 17 | Local Epoch : 1 | Train Loss: 2.2549 | Train Accuracy: 0.15\n",
      "| Global Round : 17 | Average Train Loss: 2.2549 \n",
      "| Client : 77 | Average Loss: 2.2549 \n",
      "| Global Round : 17 | Local Epoch : 1 | Train Loss: 2.2098 | Train Accuracy: 0.19\n",
      "| Global Round : 17 | Average Train Loss: 2.2098 \n",
      "| Client : 63 | Average Loss: 2.2098 \n",
      "| Global Round : 17 | Local Epoch : 1 | Train Loss: 2.2249 | Train Accuracy: 0.19\n",
      "| Global Round : 17 | Average Train Loss: 2.2249 \n",
      "| Client : 79 | Average Loss: 2.2249 \n",
      "| Global Round : 17 | Local Epoch : 1 | Train Loss: 2.2394 | Train Accuracy: 0.18\n",
      "| Global Round : 17 | Average Train Loss: 2.2394 \n",
      "| Client : 87 | Average Loss: 2.2394 \n",
      "| Global Round : 17 | Local Epoch : 1 | Train Loss: 2.2381 | Train Accuracy: 0.17\n",
      "| Global Round : 17 | Average Train Loss: 2.2381 \n",
      "| Client : 7 | Average Loss: 2.2381 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     32\u001B[0m local_model \u001B[38;5;241m=\u001B[39m LocalUpdate(dataset\u001B[38;5;241m=\u001B[39mtrain_dataset, idxs\u001B[38;5;241m=\u001B[39muser_groups[idx],\n\u001B[0;32m     33\u001B[0m                           gpu\u001B[38;5;241m=\u001B[39mgpu, optimizer\u001B[38;5;241m=\u001B[39moptimizer,\n\u001B[0;32m     34\u001B[0m                           local_batch_size\u001B[38;5;241m=\u001B[39mlocal_batch_size, lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[0;32m     35\u001B[0m                           local_epochs\u001B[38;5;241m=\u001B[39mlocal_epochs, loss_function\u001B[38;5;241m=\u001B[39mloss_function)\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# get updated weight and loss from local model\u001B[39;00m\n\u001B[1;32m---> 38\u001B[0m w, loss \u001B[38;5;241m=\u001B[39m \u001B[43mlocal_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeepcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# pass the global model to the clients\u001B[39;49;00m\n\u001B[0;32m     39\u001B[0m \u001B[43m                                     \u001B[49m\u001B[43mglobal_round\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m| Client : \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m | Average Loss: \u001B[39m\u001B[38;5;132;01m{:.4f}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m     42\u001B[0m     idx, loss))\n\u001B[0;32m     44\u001B[0m local_weights\u001B[38;5;241m.\u001B[39mappend(copy\u001B[38;5;241m.\u001B[39mdeepcopy(w))\n",
      "File \u001B[1;32m~\\OneDrive - Politecnico di Torino\\PoliTO\\MASTER\\MACHINE LEARNING AND DEEP LEARNING\\MLDL Federated Learning\\update.py:117\u001B[0m, in \u001B[0;36mLocalUpdate.update_weights\u001B[1;34m(self, model, global_round)\u001B[0m\n\u001B[0;32m    114\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    116\u001B[0m \u001B[38;5;66;03m# update training loss\u001B[39;00m\n\u001B[1;32m--> 117\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m images\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m    118\u001B[0m \u001B[38;5;66;03m# print('outputs on which to apply torch.max ', prediction)\u001B[39;00m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;66;03m# find the maximum along the rows, use dim=1 to torch.max()\u001B[39;00m\n\u001B[0;32m    120\u001B[0m _, predicted_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(output\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "train_loss, train_accuracy = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "cv_loss, cv_acc = [], []\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    local_weights = []\n",
    "    local_losses = []\n",
    "\n",
    "    print(f'Epoch: {epoch} \\n')\n",
    "\n",
    "    # # increase lr to improve groupNorm performance\n",
    "    # if normalization_type == \"GroupNorm\":\n",
    "    #     if epoch < 20:\n",
    "    #         lr = 0.001\n",
    "    #     if 20 <= epoch < 80:\n",
    "    #         lr = 0.01\n",
    "    #     if epoch >= 80:\n",
    "    #         lr = 0.05\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "\n",
    "    # different clients at each epoch\n",
    "    m = max(int(frac * num_users), 1) # number of users to be used for federated updates, at least 1\n",
    "    idxs_users = np.random.choice(range(num_users), m, replace=False) # choose randomly m users\n",
    "\n",
    "    for idx in idxs_users:  # for each client\n",
    "        # get local model\n",
    "        local_model = LocalUpdate(dataset=train_dataset, idxs=user_groups[idx],\n",
    "                                  gpu=gpu, optimizer=optimizer,\n",
    "                                  local_batch_size=local_batch_size, lr=lr,\n",
    "                                  local_epochs=local_epochs, loss_function=loss_function)\n",
    "\n",
    "        # get updated weight and loss from local model\n",
    "        w, loss = local_model.update_weights(model=copy.deepcopy(model), # pass the global model to the clients\n",
    "                                             global_round=epoch)\n",
    "\n",
    "        print('| Client : {} | Average Loss: {:.4f} '.format(\n",
    "            idx, loss))\n",
    "\n",
    "        local_weights.append(copy.deepcopy(w))\n",
    "        local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "    # compute global weights (average of local weights)\n",
    "    global_weights = average_weights(local_weights)\n",
    "    # update weights of the global model\n",
    "    model.load_state_dict(global_weights)\n",
    "\n",
    "    # compute average loss\n",
    "    loss_avg = sum(local_losses) / len(local_losses)\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    ######################\n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    # calculate avg training accuracy over all users at every epoch\n",
    "    list_acc, list_loss = [], []\n",
    "    for client in range(num_users): # for each client\n",
    "        # get local model\n",
    "        local_model = LocalUpdate(dataset=train_dataset, idxs=user_groups[client],\n",
    "                                  gpu=gpu, optimizer=optimizer,\n",
    "                                  local_batch_size=local_batch_size, lr=lr,\n",
    "                                  local_epochs=local_epochs, loss_function=loss_function)\n",
    "\n",
    "        # get accuracy and loss of local model\n",
    "        acc, loss = local_model.inference(model=model)\n",
    "        list_acc.append(acc)\n",
    "        list_loss.append(loss)\n",
    "\n",
    "    # compute average accuracy\n",
    "    train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "\n",
    "    # print stats\n",
    "    print(f'\\nAverage training statistics (global epoch : {epoch}')\n",
    "    print(f'|---- Trainig Loss : {np.mean(np.array(train_loss))}')\n",
    "    print('|---- Training Accuracy: {:.2f}% \\n'.format(100*train_accuracy[-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save train loss and accuracy\n",
    "import pandas as pd\n",
    "\n",
    "filename_csv = 'fedAVG_results/{}_{}_{}_lr_[{}]_C[{}]_iid[{}]_unbalanced[{}]_E[{}]_B[{}]_{}_numGroups[{}].csv'\\\n",
    "    .format(\"ResNet50\", n_epochs, optimizer, lr, frac, iid, unbalanced,\n",
    "           local_epochs, local_batch_size, normalization_type, num_groups)\n",
    "\n",
    "data = list(zip(train_loss, train_accuracy))\n",
    "pd.DataFrame(data, columns=['train_loss','train_accuracy']).to_csv(filename_csv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "\n",
    "filename_pt = 'fedAVG_results/{}_{}_{}_lr_[{}]_C[{}]_iid[{}]_unbalanced[{}]_E[{}]_B[{}]_{}_numGroups[{}].pt'\\\n",
    "    .format(\"ResNet50\", n_epochs, optimizer, lr, frac, iid, unbalanced,\n",
    "            local_epochs, local_batch_size, normalization_type, num_groups)\n",
    "torch.save(model.state_dict(), filename_pt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test the trained model\n",
    "\n",
    "test_acc, test_loss = test_inference(model=model, test_dataset=test_dataset, gpu=gpu,\n",
    "                                     loss_function=loss_function)\n",
    "\n",
    "print(f'\\nResults after {n_epochs} global rounds of training:')\n",
    "print(\"|---- Avgerage Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}